{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ace Attorney Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib; sys.path.append(str(pathlib.Path().resolve().parent))\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from tools.serving.api_providers import (\n",
    "    openai_text_reasoning_completion,\n",
    "    anthropic_text_completion,\n",
    "    gemini_text_completion,\n",
    "    together_ai_completion,\n",
    "    xai_grok_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model descriptions...\n",
      "Generating for o3-2025-04-16 - The First Turnabout\n",
      "Generating for o3-2025-04-16 - Turnabout Sisters - (Part 1 to Part 4)\n",
      "Generating for gemini-2.5-pro-exp-03-25 - The First Turnabout\n",
      "Generating for gemini-2.5-pro-exp-03-25 - Turnabout Sisters - (Part 1 to Part 4)\n",
      "Generating for claude-3-7-sonnet-20250219(thinking) - The First Turnabout\n",
      "Generating for claude-3-7-sonnet-20250219(thinking) - Turnabout Sisters - (Part 1 to Part 4)\n",
      "Generating for claude-3-5-sonnet-20241022 - The First Turnabout\n",
      "Generating for claude-3-5-sonnet-20241022 - Turnabout Sisters - (Part 1 to Part 4)\n",
      "Generating for o4-mini-2025-04-16 - The First Turnabout\n",
      "Generating for o4-mini-2025-04-16 - Turnabout Sisters - (Part 1 to Part 4)\n",
      "Generating for Llama-4-Maverick-17B-128E-Instruct-FP8 - The First Turnabout\n",
      "Generating for Llama-4-Maverick-17B-128E-Instruct-FP8 - Turnabout Sisters - (Part 1 to Part 4)\n",
      "Computing semantic similarities...\n",
      "Processing model: o3-2025-04-16\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8218\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8381\n",
      "Processing model: gemini-2.5-pro-exp-03-25\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8414\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8317\n",
      "Processing model: claude-3-7-sonnet-20250219(thinking)\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8293\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8635\n",
      "Processing model: claude-3-5-sonnet-20241022\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.7554\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8606\n",
      "Processing model: o4-mini-2025-04-16\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.6560\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.7580\n",
      "Processing model: Llama-4-Maverick-17B-128E-Instruct-FP8\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.6384\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.4718\n",
      "Analyzing correlations...\n",
      "\n",
      "Correlation Results:\n",
      "Correlation with rank: -0.7714 (p-value = 0.0724)\n",
      "Correlation with game score: 0.7714 (p-value = 0.0724)\n",
      "\n",
      "Detailed Results:\n",
      "                                    Model  First Turnabout Similarity  \\\n",
      "2    claude-3-7-sonnet-20250219(thinking)                    0.829252   \n",
      "1                gemini-2.5-pro-exp-03-25                    0.841352   \n",
      "0                           o3-2025-04-16                    0.821797   \n",
      "3              claude-3-5-sonnet-20241022                    0.755424   \n",
      "4                      o4-mini-2025-04-16                    0.656018   \n",
      "5  Llama-4-Maverick-17B-128E-Instruct-FP8                    0.638361   \n",
      "\n",
      "   Sister Turnabout Similarity  Total Similarity  Performance Rank  Game Score  \n",
      "2                     0.863538          1.692790                 4           8  \n",
      "1                     0.831678          1.673030                 3          20  \n",
      "0                     0.838109          1.659906                 2          23  \n",
      "3                     0.860616          1.616040                 6           6  \n",
      "4                     0.758044          1.414062                11           1  \n",
      "5                     0.471835          1.110196                13           0  \n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template with {level name} as the placeholder\n",
    "prompt_template = \"\"\"\n",
    "You are an expert on *Phoenix Wright: Ace Attorney*.\n",
    "\n",
    "LEVEL = “{level name}”\n",
    "\n",
    "Return **exactly** the three sections below, in the order shown, with *nothing* else before or after them.\n",
    "\n",
    "1) Detailed Narrative Description:\n",
    "<Write a tightly written, chronological summary using exact dates/times from the case timeline. Include:\n",
    "  • crime details (when, where, with what),\n",
    "  • key character actions and motives,\n",
    "  • off-screen investigations by Phoenix or allies (e.g., record gathering, witness discovery),\n",
    "  • courtroom sequence: who testifies, what claims are made, and how Phoenix counters them.\n",
    "Keep the paragraph clear, factual, and dense with relevant case details.>\n",
    "\n",
    "2) Complete Evidence List:\n",
    "- \"Exact In-Game Item Name\" (Category) Relevance: One-sentence explanation of how it supports Phoenix’s case or undermines the prosecution.\n",
    "- (repeat bullet for each item used in court or discovered during investigation)\n",
    "\n",
    "3) Cross-Examination Breakdown:\n",
    "- **<Witness Name>, Round X**\n",
    "  - Statement: \"<Quoted line or paraphrased claim from testimony>\"\n",
    "  - Present: \"<Exact Evidence Name>\"  (or “Press” if only pressing)\n",
    "  - Contradiction Exposed: <Brief sentence showing the factual/logical error>\n",
    "  - Impact: <How this weakens the witness or strengthens Phoenix’s case>\n",
    "- (repeat bullet for each cross-exam round across all witnesses)\n",
    "\n",
    "**Formatting rules (must follow strictly):**\n",
    "- Use the section headings *exactly* as shown (including numbers, parentheses, and colons).\n",
    "- Leave exactly one blank line between sections.\n",
    "- Section 1 is a paragraph; Sections 2 and 3 are bullet lists.\n",
    "- No markdown, no extra headings, no commentary outside the 3 sections.\n",
    "- Do not make up filler; if a detail isn’t known in-game, omit it entirely.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define the level names\n",
    "level_names = [\"The First Turnabout\", \"Turnabout Sisters - (Part 1 to Part 4)\"]\n",
    "\n",
    "# Create the actual prompts\n",
    "prompts = [prompt_template.format(**{\"level name\": level_name}) for level_name in level_names]\n",
    "\n",
    "# Step 1: Define prompts and load performance data\n",
    "def setup_study_data():\n",
    "    # Load the performance data\n",
    "    with open(\"./rank_data_03_25_2025.json\", \"r\") as f:\n",
    "        performance_data = json.load(f)\n",
    "    \n",
    "    # Extract Ace Attorney ranks\n",
    "    ace_attorney_data = performance_data[\"Ace Attorney\"][\"results\"]\n",
    "    \n",
    "    # Create a dictionary mapping model names to ranks and scores\n",
    "    model_performance = {}\n",
    "    for result in ace_attorney_data:\n",
    "        model_name = result[\"model\"]\n",
    "        model_performance[model_name] = {\n",
    "            \"rank\": result[\"rank\"],\n",
    "            \"levels_cracked\": result[\"levels_cracked\"],\n",
    "            \"score\": result[\"score\"]\n",
    "        }\n",
    "    \n",
    "    # Load ground truth transcripts\n",
    "    with open(\"./ace_attorney_ground_truth.json\", \"r\") as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    # Extract transcripts in the same order as level_names\n",
    "    transcripts = [ground_truth[level_name] for level_name in level_names]\n",
    "    \n",
    "    return prompts, level_names, model_performance, transcripts\n",
    "\n",
    "# Step 2: Generate Text Descriptions\n",
    "def generate_descriptions(models, prompts, level_names):\n",
    "    # First try to load existing results\n",
    "    try:\n",
    "        with open(\"generated_texts.json\", \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        # Skip if model results already exist\n",
    "        if model_name in results:\n",
    "            print(f\"Skipping {model_name} - already exists in results\")\n",
    "            continue\n",
    "            \n",
    "        results[model_name] = {}\n",
    "        \n",
    "        for prompt, level_name in zip(prompts, level_names):\n",
    "            system_prompt = \"You are an Ace Attorney expert.\"\n",
    "            print(f\"Generating for {model_name} - {level_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate based on model provider\n",
    "                if \"o1-\" in model_name or \"o3-\" in model_name or \"o4-\" in model_name or \"gpt-\" in model_name:\n",
    "                    generated_text = openai_text_reasoning_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "\n",
    "                elif \"claude-3-7-sonnet-20250219(thinking)\" in model_name:\n",
    "                    generated_text = anthropic_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=\"claude-3-7-sonnet-20250219\",\n",
    "                        prompt=prompt,\n",
    "                        thinking=True\n",
    "                    )\n",
    "                    \n",
    "                elif \"claude\" in model_name:\n",
    "                    generated_text = anthropic_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        thinking=False\n",
    "                    )\n",
    "                    \n",
    "                elif \"gemini\" in model_name:\n",
    "                    generated_text = gemini_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt\n",
    "                    )\n",
    "                    \n",
    "                elif \"llama-4-maverick\" in model_name.lower():\n",
    "                    generated_text = together_ai_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                elif \"grok-3\" in model_name:\n",
    "                    generated_text = xai_grok_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                \n",
    "                results[model_name][level_name] = generated_text\n",
    "                \n",
    "                # Save after each successful generation\n",
    "                with open(\"generated_texts.json\", \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating for {model_name} - {level_name}: {str(e)}\")\n",
    "                # Save partial results even if there's an error\n",
    "                with open(\"generated_texts.json\", \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def compute_similarity(generated_texts, transcripts):\n",
    "    # Load SBERT model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    similarity_scores = {}\n",
    "    \n",
    "    for model_name, texts in generated_texts.items():\n",
    "        similarity_scores[model_name] = []\n",
    "        \n",
    "        # Debug print\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "        print(f\"Available texts: {list(texts.keys())}\")\n",
    "        \n",
    "        try:\n",
    "            for i, level_name in enumerate(level_names):\n",
    "                if level_name in texts:\n",
    "                    # Get embeddings\n",
    "                    emb_gen = model.encode(texts[level_name], show_progress_bar=False)\n",
    "                    emb_trans = model.encode(transcripts[i], show_progress_bar=False)\n",
    "                    \n",
    "                    # Calculate cosine similarity\n",
    "                    sim_score = cosine_similarity([emb_gen], [emb_trans])[0][0]\n",
    "                    similarity_scores[model_name].append(sim_score)\n",
    "                    \n",
    "                    print(f\"Computed similarity for {level_name}: {sim_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Missing text for {level_name} in {model_name}\")\n",
    "                    similarity_scores[model_name].append(0.0)  # or np.nan if you prefer\n",
    "                \n",
    "        except KeyError as e:\n",
    "            print(f\"Error processing {model_name}: Missing key {e}\")\n",
    "            print(f\"Available keys: {list(texts.keys())}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# Step 4: Calculate total similarity and correlation with performance\n",
    "def analyze_correlation(similarity_scores, model_performance):\n",
    "    # Calculate total similarity score for each model\n",
    "    total_similarity = {}\n",
    "    for model_name, scores in similarity_scores.items():\n",
    "        total_similarity[model_name] = sum(scores)\n",
    "    \n",
    "    # Prepare data for correlation\n",
    "    models = []\n",
    "    sim_scores = []\n",
    "    ranks = []\n",
    "    game_scores = []\n",
    "    \n",
    "    for model_name, total_sim in total_similarity.items():\n",
    "        if model_name in model_performance:\n",
    "            models.append(model_name)\n",
    "            sim_scores.append(total_sim)\n",
    "            ranks.append(model_performance[model_name][\"rank\"])\n",
    "            game_scores.append(model_performance[model_name][\"score\"])\n",
    "    \n",
    "    # Calculate Spearman's rank correlation with rank\n",
    "    rank_rho, rank_p_val = spearmanr(sim_scores, ranks)\n",
    "    \n",
    "    # Calculate Spearman's rank correlation with game score\n",
    "    score_rho, score_p_val = spearmanr(sim_scores, game_scores)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Model\": models,\n",
    "        \"Total Similarity\": sim_scores,\n",
    "        \"Performance Rank\": ranks,\n",
    "        \"Game Score\": game_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by total similarity\n",
    "    results_df = results_df.sort_values(\"Total Similarity\", ascending=False)\n",
    "    \n",
    "    correlation_results = {\n",
    "        \"rank_correlation\": rank_rho,\n",
    "        \"rank_p_value\": rank_p_val,\n",
    "        \"score_correlation\": score_rho,\n",
    "        \"score_p_value\": score_p_val\n",
    "    }\n",
    "    \n",
    "    return results_df, correlation_results, total_similarity\n",
    "\n",
    "# Step 5: Main Function\n",
    "def run_ablation_study():\n",
    "    # Setup data\n",
    "    prompts, level_names, model_performance, transcripts = setup_study_data()\n",
    "    \n",
    "    # Define models to test as specified by the user\n",
    "    models = [\n",
    "        \"o3-2025-04-16\",\n",
    "        \"gemini-2.5-pro-exp-03-25\",\n",
    "        \"claude-3-7-sonnet-20250219(thinking)\",\n",
    "        \"claude-3-5-sonnet-20241022\",\n",
    "        \"o4-mini-2025-04-16\",\n",
    "        \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "    ]\n",
    "    \n",
    "    # Generate descriptions\n",
    "    print(\"Generating model descriptions...\")\n",
    "    generated_texts = generate_descriptions(models, prompts, level_names)\n",
    "    \n",
    "    # Compute similarities\n",
    "    print(\"Computing semantic similarities...\")\n",
    "    similarity_scores = compute_similarity(generated_texts, transcripts)\n",
    "    \n",
    "    # Analyze correlations\n",
    "    print(\"Analyzing correlations...\")\n",
    "    results_df, correlation_results, total_similarity = analyze_correlation(similarity_scores, model_performance)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nCorrelation Results:\")\n",
    "    print(f\"Correlation with rank: {correlation_results['rank_correlation']:.4f} (p-value = {correlation_results['rank_p_value']:.4f})\")\n",
    "    print(f\"Correlation with game score: {correlation_results['score_correlation']:.4f} (p-value = {correlation_results['score_p_value']:.4f})\")\n",
    "    \n",
    "    # Create a detailed DataFrame showing similarity scores for each prompt\n",
    "    detailed_df = pd.DataFrame({\n",
    "        \"Model\": list(similarity_scores.keys()),\n",
    "        \"First Turnabout Similarity\": [scores[0] for scores in similarity_scores.values()],\n",
    "        \"Sister Turnabout Similarity\": [scores[1] for scores in similarity_scores.values()],\n",
    "        \"Total Similarity\": [total_similarity[model] for model in similarity_scores.keys()]\n",
    "    })\n",
    "    \n",
    "    # Add performance data where available\n",
    "    performance_rank = []\n",
    "    performance_score = []\n",
    "    \n",
    "    for model in detailed_df[\"Model\"]:\n",
    "        if model in model_performance:\n",
    "            performance_rank.append(model_performance[model][\"rank\"])\n",
    "            performance_score.append(model_performance[model][\"score\"])\n",
    "        else:\n",
    "            performance_rank.append(np.nan)\n",
    "            performance_score.append(np.nan)\n",
    "    \n",
    "    detailed_df[\"Performance Rank\"] = performance_rank\n",
    "    detailed_df[\"Game Score\"] = performance_score\n",
    "    \n",
    "    # Sort by total similarity\n",
    "    detailed_df = detailed_df.sort_values(\"Total Similarity\", ascending=False)\n",
    "    \n",
    "    return results_df, correlation_results, detailed_df, generated_texts, similarity_scores\n",
    "\n",
    "# Run the study if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, correlation_results, detailed_df, generated_texts, similarity_scores = run_ablation_study()\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(detailed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average embedding cosine similarity between cases: 0.6035\n",
      "\n",
      "Updated Results with Embedding Cross Correlation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>First Turnabout Similarity</th>\n",
       "      <th>Sister Turnabout Similarity</th>\n",
       "      <th>Total Similarity</th>\n",
       "      <th>Performance Rank</th>\n",
       "      <th>Game Score</th>\n",
       "      <th>Cross Similarity Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-3-7-sonnet-20250219(thinking)</td>\n",
       "      <td>0.829252</td>\n",
       "      <td>0.863538</td>\n",
       "      <td>1.692790</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.688704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>0.841352</td>\n",
       "      <td>0.831678</td>\n",
       "      <td>1.673030</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.656701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>0.821797</td>\n",
       "      <td>0.838109</td>\n",
       "      <td>1.659906</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.594483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>0.755424</td>\n",
       "      <td>0.860616</td>\n",
       "      <td>1.616040</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.526616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o4-mini-2025-04-16</td>\n",
       "      <td>0.656018</td>\n",
       "      <td>0.758044</td>\n",
       "      <td>1.414062</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama-4-Maverick-17B-128E-Instruct-FP8</td>\n",
       "      <td>0.638361</td>\n",
       "      <td>0.471835</td>\n",
       "      <td>1.110196</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.589145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  First Turnabout Similarity  \\\n",
       "2    claude-3-7-sonnet-20250219(thinking)                    0.829252   \n",
       "1                gemini-2.5-pro-exp-03-25                    0.841352   \n",
       "0                           o3-2025-04-16                    0.821797   \n",
       "3              claude-3-5-sonnet-20241022                    0.755424   \n",
       "4                      o4-mini-2025-04-16                    0.656018   \n",
       "5  Llama-4-Maverick-17B-128E-Instruct-FP8                    0.638361   \n",
       "\n",
       "   Sister Turnabout Similarity  Total Similarity  Performance Rank  \\\n",
       "2                     0.863538          1.692790                 4   \n",
       "1                     0.831678          1.673030                 3   \n",
       "0                     0.838109          1.659906                 2   \n",
       "3                     0.860616          1.616040                 6   \n",
       "4                     0.758044          1.414062                11   \n",
       "5                     0.471835          1.110196                13   \n",
       "\n",
       "   Game Score  Cross Similarity Scores  \n",
       "2           8                 0.688704  \n",
       "1          20                 0.656701  \n",
       "0          23                 0.594483  \n",
       "3           6                 0.526616  \n",
       "4           1                 0.565571  \n",
       "5           0                 0.589145  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def add_embedding_cross_correlation(detailed_df, generated_texts):\n",
    "    # Initialize SBERT model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Calculate cross-correlation using embeddings for each model\n",
    "    cross_correlations = []\n",
    "    \n",
    "    for model_name in detailed_df['Model']:\n",
    "        if model_name in generated_texts:\n",
    "            # Check if both required texts exist for this model\n",
    "            if \"The First Turnabout\" in generated_texts[model_name] and \"Turnabout Sisters - (Part 1 to Part 4)\" in generated_texts[model_name]:\n",
    "                # Get the generated texts for both cases\n",
    "                first_turnabout_text = generated_texts[model_name][\"The First Turnabout\"]\n",
    "                sister_turnabout_text = generated_texts[model_name][\"Turnabout Sisters - (Part 1 to Part 4)\"]\n",
    "                \n",
    "                # Get embeddings for both texts\n",
    "                first_turnabout_embedding = model.encode(first_turnabout_text, show_progress_bar=False)\n",
    "                sister_turnabout_embedding = model.encode(sister_turnabout_text, show_progress_bar=False)\n",
    "                \n",
    "                # Calculate cosine similarity between embedding vectors\n",
    "                # Reshape embeddings to 2D arrays for cosine_similarity\n",
    "                sim_score = cosine_similarity([first_turnabout_embedding], [sister_turnabout_embedding])[0][0]\n",
    "                cross_correlations.append(sim_score)\n",
    "            else:\n",
    "                # If either text is missing, append NaN\n",
    "                cross_correlations.append(np.nan)\n",
    "        else:\n",
    "            cross_correlations.append(np.nan)\n",
    "    \n",
    "    # Add the embedding-based cross-correlation scores as a new column\n",
    "    detailed_df['Cross Similarity Scores'] = cross_correlations\n",
    "    \n",
    "    # Calculate average embedding correlation across all models\n",
    "    valid_correlations = [c for c in cross_correlations if not np.isnan(c)]\n",
    "    avg_correlation = np.mean(valid_correlations)\n",
    "    print(f\"\\nAverage embedding cosine similarity between cases: {avg_correlation:.4f}\")\n",
    "    \n",
    "    return detailed_df\n",
    "\n",
    "# To use this function, you would add this after getting your detailed_df:\n",
    "detailed_df = add_embedding_cross_correlation(detailed_df, generated_texts)\n",
    "\n",
    "# Display the updated DataFrame with the new column\n",
    "print(\"\\nUpdated Results with Embedding Cross Correlation Scores:\")\n",
    "detailed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground truth cross-correlation: 0.7175\n"
     ]
    }
   ],
   "source": [
    "# Calculate ground truth cross-correlation using SBERT embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "with open(\"./ace_attorney_ground_truth.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "    \n",
    "# Extract transcripts in the same order as level_names\n",
    "transcripts = [ground_truth[level_name] for level_name in level_names]\n",
    "\n",
    "# Get embeddings for both transcripts\n",
    "first_turnabout_embedding = model.encode(transcripts[0], show_progress_bar=False)\n",
    "sister_turnabout_embedding = model.encode(transcripts[1], show_progress_bar=False)\n",
    "\n",
    "# Calculate cosine similarity between embedding vectors\n",
    "sim_score = cosine_similarity([first_turnabout_embedding], [sister_turnabout_embedding])[0][0]\n",
    "print(f\"\\nGround truth cross-correlation: {sim_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation plots have been generated and saved as:\n",
      "1. correlation_analysis.png\n",
      "2. model_performance.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_correlation_analysis(detailed_df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_standardized = detailed_df.copy()\n",
    "    \n",
    "    # Standardize the values\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized[['Total Similarity', 'Game Score', 'Performance Rank']] = scaler.fit_transform(\n",
    "        df_standardized[['Total Similarity', 'Game Score', 'Performance Rank']]\n",
    "    )\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Function to calculate correlation and p-value\n",
    "    def get_correlation_stats(x, y):\n",
    "        corr, p_value = stats.pearsonr(x, y)\n",
    "        return corr, p_value\n",
    "    \n",
    "    # Plot 1: Linear Regression Plot for Game Score\n",
    "    sns.regplot(\n",
    "        data=df_standardized,\n",
    "        x='Total Similarity',\n",
    "        y='Game Score',\n",
    "        ax=ax1,\n",
    "        scatter_kws={'alpha':0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    ax1.set_title('Linear Correlation: Similarity vs Game Score\\n(Standardized Values)')\n",
    "    ax1.set_xlabel('Total Similarity Score (Standardized)')\n",
    "    ax1.set_ylabel('Game Score (Standardized)')\n",
    "    \n",
    "    # Calculate correlation and p-value for Game Score\n",
    "    corr_score, p_value_score = get_correlation_stats(\n",
    "        df_standardized['Total Similarity'], \n",
    "        df_standardized['Game Score']\n",
    "    )\n",
    "    \n",
    "    # Add correlation coefficient and p-value to the plot\n",
    "    significance = \"***\" if p_value_score < 0.001 else \"**\" if p_value_score < 0.01 else \"*\" if p_value_score < 0.05 else \"ns\"\n",
    "    ax1.text(0.05, 0.95, \n",
    "             f'Correlation: {corr_score:.3f} {significance}\\np-value: {p_value_score:.3e}', \n",
    "             transform=ax1.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Plot 2: Linear Regression Plot for Performance Rank\n",
    "    sns.regplot(\n",
    "        data=df_standardized,\n",
    "        x='Total Similarity',\n",
    "        y='Performance Rank',\n",
    "        ax=ax2,\n",
    "        scatter_kws={'alpha':0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    ax2.set_title('Linear Correlation: Similarity vs Performance Rank\\n(Standardized Values)')\n",
    "    ax2.set_xlabel('Total Similarity Score (Standardized)')\n",
    "    ax2.set_ylabel('Performance Rank (Standardized)')\n",
    "    \n",
    "    # Calculate correlation and p-value for Performance Rank\n",
    "    corr_rank, p_value_rank = get_correlation_stats(\n",
    "        df_standardized['Total Similarity'], \n",
    "        df_standardized['Performance Rank']\n",
    "    )\n",
    "    \n",
    "    # Add correlation coefficient and p-value to the plot\n",
    "    significance = \"***\" if p_value_rank < 0.001 else \"**\" if p_value_rank < 0.01 else \"*\" if p_value_rank < 0.05 else \"ns\"\n",
    "    ax2.text(0.05, 0.95, \n",
    "             f'Correlation: {corr_rank:.3f} {significance}\\np-value: {p_value_rank:.3e}', \n",
    "             transform=ax2.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Plot 3: Heatmap\n",
    "    # Standardize all columns for correlation\n",
    "    corr_columns = ['First Turnabout Similarity', \n",
    "                   'Sister Turnabout Similarity',\n",
    "                   'Total Similarity',\n",
    "                   'Game Score',\n",
    "                   'Performance Rank']\n",
    "    \n",
    "    # Calculate correlation matrix and p-values using standardized values\n",
    "    corr_matrix = df_standardized[corr_columns].corr()\n",
    "    p_values = pd.DataFrame(\n",
    "        [[stats.pearsonr(df_standardized[i], df_standardized[j])[1] \n",
    "          for j in corr_columns] for i in corr_columns],\n",
    "        columns=corr_columns,\n",
    "        index=corr_columns\n",
    "    )\n",
    "    \n",
    "    # Create heatmap with significance stars\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        ax=ax3,\n",
    "        fmt='.2f',\n",
    "        square=True\n",
    "    )\n",
    "    ax3.set_title('Correlation Heatmap\\n(Standardized Values)')\n",
    "    \n",
    "    # Add significance stars to heatmap\n",
    "    for i in range(len(corr_columns)):\n",
    "        for j in range(len(corr_columns)):\n",
    "            p_val = p_values.iloc[i, j]\n",
    "            if p_val < 0.001:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"***\", \n",
    "                        ha='center', va='center', color='black')\n",
    "            elif p_val < 0.01:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"**\", \n",
    "                        ha='center', va='center', color='black')\n",
    "            elif p_val < 0.05:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"*\", \n",
    "                        ha='center', va='center', color='black')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_performance(detailed_df):\n",
    "    # Create a copy and standardize values\n",
    "    df_standardized = detailed_df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized[['Total Similarity', 'Game Score']] = scaler.fit_transform(\n",
    "        df_standardized[['Total Similarity', 'Game Score']]\n",
    "    )\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Sort dataframe by Game Score\n",
    "    sorted_df = df_standardized.sort_values('Game Score', ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_df['Model'], sorted_df['Game Score'])\n",
    "    \n",
    "    # Add Total Similarity as scatter points\n",
    "    plt.scatter(range(len(sorted_df)), \n",
    "               sorted_df['Total Similarity'],\n",
    "               color='red',\n",
    "               s=100,\n",
    "               label='Total Similarity')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Model Performance vs Similarity Scores\\n(Standardized Values)')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Standardized Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def generate_correlation_plots(detailed_df):\n",
    "    # Generate both plots\n",
    "    plot_correlation_analysis(detailed_df)\n",
    "    plot_model_performance(detailed_df)\n",
    "    \n",
    "    print(\"Correlation plots have been generated and saved as:\")\n",
    "    print(\"1. correlation_analysis.png\")\n",
    "    print(\"2. model_performance.png\")\n",
    "\n",
    "# Example usage:\n",
    "# After running your ablation study and getting detailed_df:\n",
    "generate_correlation_plots(detailed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ace Attorney without Data Contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1)  Imports\n",
    "###############################################################################\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "###############################################################################\n",
    "# 2)  Load only the Ace-Attorney rows from the new-rank JSON\n",
    "###############################################################################\n",
    "def load_ace_attorney(json_path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with the Ace-Attorney results only:\n",
    "\n",
    "        Model · New Rank · New Score · Evaluated Score\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    ace_rows = []\n",
    "    ace_results = data.get(\"Ace Attorney\", {}).get(\"results\", [])\n",
    "    for res in ace_results:\n",
    "        ace_rows.append({\n",
    "            \"Model\":           res.get(\"model\"),\n",
    "            \"New Rank\":        res.get(\"rank\"),\n",
    "            \"New Score\":       res.get(\"score\"),\n",
    "            \"Evaluated Score\": res.get(\"evaluator result\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(ace_rows)\n",
    "\n",
    "###############################################################################\n",
    "# 3)  Merge with your old similarity table\n",
    "###############################################################################\n",
    "def merge_with_similarity(detailed_df: pd.DataFrame,\n",
    "                          ace_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • detailed_df must already contain  Model  and  Total Similarity.\n",
    "    • Only models that appear in *both* tables survive the merge.\n",
    "    \"\"\"\n",
    "    return pd.merge(\n",
    "        detailed_df[[\"Model\", \"Total Similarity\"]],\n",
    "        ace_df,\n",
    "        on=\"Model\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "###############################################################################\n",
    "# 4)  Four-panel correlation plot (unchanged, but re-posted for completeness)\n",
    "###############################################################################\n",
    "def plot_new_correlations(df,\n",
    "                          similarity_col='Total Similarity',\n",
    "                          new_rank_col='New Rank',\n",
    "                          new_score_col='New Score',\n",
    "                          eval_score_col='Evaluated Score',\n",
    "                          out_file='ace_correlation_analysis.png'):\n",
    "\n",
    "    # ── keep rows that have every column we need ─────────────────────────────\n",
    "    df_use = df[[similarity_col,\n",
    "                 new_rank_col,\n",
    "                 new_score_col,\n",
    "                 eval_score_col]].dropna().copy()\n",
    "\n",
    "    # ── z-score for comparability ────────────────────────────────────────────\n",
    "    scaler = StandardScaler()\n",
    "    df_use[df_use.columns] = scaler.fit_transform(df_use)\n",
    "\n",
    "    # ── canvas & spec ────────────────────────────────────────────────────────\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(26, 6))\n",
    "    specs = [\n",
    "        (similarity_col, new_rank_col,  'Similarity vs New Rank'),\n",
    "        (similarity_col, new_score_col, 'Similarity vs New Score'),\n",
    "        (eval_score_col, new_rank_col,  'Evaluated Score vs New Rank'),\n",
    "        (eval_score_col, new_score_col, 'Evaluated Score vs New Score'),\n",
    "    ]\n",
    "\n",
    "    # helper to annotate r & p\n",
    "    def annotate(ax, x, y):\n",
    "        r, p = stats.pearsonr(df_use[x], df_use[y])\n",
    "        stars = \"***\" if p < 1e-3 else \"**\" if p < 1e-2 else \"*\" if p < .05 else \"ns\"\n",
    "        ax.text(0.05, 0.95,\n",
    "                f\"R = {r:.3f} {stars}\\np = {p:.2e}\",\n",
    "                transform=ax.transAxes,\n",
    "                ha='left', va='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    # draw each panel\n",
    "    for ax, (x, y, title) in zip(axes, specs):\n",
    "        sns.regplot(data=df_use, x=x, y=y,\n",
    "                    scatter_kws={'alpha': .5},\n",
    "                    line_kws={'color': 'red'},\n",
    "                    ax=ax)\n",
    "        ax.set_title(title + '\\n(Standardised)')\n",
    "        ax.set_xlabel(f'{x} (z-score)')\n",
    "        ax.set_ylabel(f'{y} (z-score)')\n",
    "        annotate(ax, x, y)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "###############################################################################\n",
    "# 5)  End-to-end driver\n",
    "###############################################################################\n",
    "def build_and_plot_ace(detailed_df,\n",
    "                       json_path='rank_without_datacontainmation.json',\n",
    "                       out_file='ace_correlation_analysis.png'):\n",
    "\n",
    "    ace_df     = load_ace_attorney(json_path)\n",
    "    merged_df  = merge_with_similarity(detailed_df, ace_df)\n",
    "    plot_new_correlations(merged_df, out_file=out_file)\n",
    "\n",
    "    print(f\"✔ Four-panel Ace-Attorney figure saved as “{out_file}”\")\n",
    "    print(f\"  Models included: {merged_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Four-panel Ace-Attorney figure saved as “New_ace_correlation_analysis.png”\n",
      "  Models included: 6\n"
     ]
    }
   ],
   "source": [
    "detailed_df = detailed_df\n",
    "build_and_plot_ace(detailed_df,\n",
    "                   json_path='rank_without_datacontainmation.json',\n",
    "                   out_file='New_ace_correlation_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Mario Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
