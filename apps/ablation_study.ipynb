{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ace Attorney Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib; sys.path.append(str(pathlib.Path().resolve().parent))\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from tools.serving.api_providers import (\n",
    "    openai_text_reasoning_completion,\n",
    "    anthropic_text_completion,\n",
    "    gemini_text_completion,\n",
    "    together_ai_completion,\n",
    "    xai_grok_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model descriptions...\n",
      "Skipping o3-2025-04-16 - already exists in results\n",
      "Skipping gemini-2.5-pro-preview-03-25 - already exists in results\n",
      "Skipping claude-3-7-sonnet-20250219(thinking) - already exists in results\n",
      "Skipping claude-3-5-sonnet-20241022 - already exists in results\n",
      "Skipping o4-mini-2025-04-16 - already exists in results\n",
      "Skipping Llama-4-Maverick-17B-128E-Instruct-FP8 - already exists in results\n",
      "Computing semantic similarities...\n",
      "Processing model: o3-2025-04-16\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8630\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.9044\n",
      "Processing model: gemini-2.5-pro-preview-03-25\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8667\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8670\n",
      "Processing model: claude-3-7-sonnet-20250219(thinking)\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.7080\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8305\n",
      "Processing model: claude-3-5-sonnet-20241022\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8452\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8156\n",
      "Processing model: o4-mini-2025-04-16\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.5864\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.6253\n",
      "Processing model: Llama-4-Maverick-17B-128E-Instruct-FP8\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.4906\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.7335\n",
      "Processing model: o1-2024-12-17\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8090\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8423\n",
      "Processing model: gpt-4.1-2025-04-14\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.7548\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8125\n",
      "Processing model: gemini-2.0-flash-thinking-exp-1219\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.6588\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.7796\n",
      "Processing model: gemini-2.5-flash-preview-04-17\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.7313\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.7281\n",
      "Processing model: grok-3-mini-beta\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.7817\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8326\n",
      "Analyzing correlations...\n",
      "\n",
      "Correlation Results:\n",
      "Correlation with rank: -0.8545 (p-value = 0.0008)\n",
      "Correlation with game score: 0.8402 (p-value = 0.0012)\n",
      "\n",
      "Detailed Results:\n",
      "                                     Model  First Turnabout Similarity  \\\n",
      "0                            o3-2025-04-16                    0.863010   \n",
      "1             gemini-2.5-pro-preview-03-25                    0.866726   \n",
      "3               claude-3-5-sonnet-20241022                    0.845162   \n",
      "6                            o1-2024-12-17                    0.808985   \n",
      "10                        grok-3-mini-beta                    0.781703   \n",
      "7                       gpt-4.1-2025-04-14                    0.754770   \n",
      "2     claude-3-7-sonnet-20250219(thinking)                    0.707965   \n",
      "9           gemini-2.5-flash-preview-04-17                    0.731344   \n",
      "8       gemini-2.0-flash-thinking-exp-1219                    0.658813   \n",
      "5   Llama-4-Maverick-17B-128E-Instruct-FP8                    0.490601   \n",
      "4                       o4-mini-2025-04-16                    0.586401   \n",
      "\n",
      "    Sister Turnabout Similarity  Total Similarity  Performance Rank  \\\n",
      "0                      0.904428          1.767438                 2   \n",
      "1                      0.866952          1.733678                 3   \n",
      "3                      0.815580          1.660742                 6   \n",
      "6                      0.842305          1.651290                 1   \n",
      "10                     0.832628          1.614331                 5   \n",
      "7                      0.812457          1.567227                 7   \n",
      "2                      0.830473          1.538438                 4   \n",
      "9                      0.728147          1.459491                 8   \n",
      "8                      0.779637          1.438450                 9   \n",
      "5                      0.733536          1.224137                13   \n",
      "4                      0.625346          1.211747                11   \n",
      "\n",
      "    Game Score  \n",
      "0           23  \n",
      "1           20  \n",
      "3            6  \n",
      "6           26  \n",
      "10           7  \n",
      "7            6  \n",
      "2            8  \n",
      "9            4  \n",
      "8            4  \n",
      "5            0  \n",
      "4            1  \n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template with {level name} as the placeholder\n",
    "prompt_template = \"\"\"\n",
    "You are an expert on *Phoenix Wright: Ace Attorney*.\n",
    "\n",
    "LEVEL = “{level name}”\n",
    "\n",
    "Return **exactly** the two sections below, in the order shown, with *nothing* else before or after them.\n",
    "\n",
    "1) Complete Evidence List:\n",
    "- \"Exact In-Game Item Name\" (Type) Relevance: one-sentence explanation of why it helps the defense.\n",
    "- (repeat bullet for every distinct item)\n",
    "\n",
    "2) Cross-Examination Breakdown:\n",
    "- **<Witness Name>, Round X**\n",
    "  - Statement: <key line from testimony>\n",
    "  - Present: <evidence name>  (or “Press” if simply pressing)\n",
    "  - Contradiction Exposed: <logical inconsistency uncovered>\n",
    "  - Impact: <how this advances Phoenix’s case>\n",
    "- (add bullets for every round / witness)\n",
    "\n",
    "**Formatting rules (must follow strictly):**\n",
    "- Use the section headings *exactly* as written (including numbers, parentheses, and colons).\n",
    "- Leave exactly one blank line between sections.\n",
    "- Sections 1 & 2 must be bullet lists that match the patterns shown.\n",
    "- Do **not** add extra commentary, headings, or markdown.\n",
    "- If a detail is unknown in-game, omit it rather than inventing filler.\n",
    "\"\"\"\n",
    "\n",
    "# Define the level names\n",
    "level_names = [\"The First Turnabout\", \"Turnabout Sisters - (Part 1 to Part 4)\"]\n",
    "\n",
    "# Create the actual prompts\n",
    "prompts = [prompt_template.format(**{\"level name\": level_name}) for level_name in level_names]\n",
    "\n",
    "# Step 1: Define prompts and load performance data\n",
    "def setup_study_data():\n",
    "    # Load the performance data\n",
    "    with open(\"./rank_data_03_25_2025.json\", \"r\") as f:\n",
    "        performance_data = json.load(f)\n",
    "    \n",
    "    # Extract Ace Attorney ranks\n",
    "    ace_attorney_data = performance_data[\"Ace Attorney\"][\"results\"]\n",
    "    \n",
    "    # Create a dictionary mapping model names to ranks and scores\n",
    "    model_performance = {}\n",
    "    for result in ace_attorney_data:\n",
    "        model_name = result[\"model\"]\n",
    "        model_performance[model_name] = {\n",
    "            \"rank\": result[\"rank\"],\n",
    "            \"levels_cracked\": result[\"levels_cracked\"],\n",
    "            \"score\": result[\"score\"]\n",
    "        }\n",
    "    \n",
    "    # Load ground truth transcripts\n",
    "    with open(\"./ace_attorney_ground_truth.json\", \"r\") as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    # Extract transcripts in the same order as level_names\n",
    "    transcripts = [ground_truth[level_name] for level_name in level_names]\n",
    "    \n",
    "    return prompts, level_names, model_performance, transcripts\n",
    "\n",
    "# Step 2: Generate Text Descriptions\n",
    "def generate_descriptions(models, prompts, level_names):\n",
    "    # First try to load existing results\n",
    "    try:\n",
    "        with open(\"generated_texts.json\", \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        # Skip if model results already exist\n",
    "        if model_name in results:\n",
    "            print(f\"Skipping {model_name} - already exists in results\")\n",
    "            continue\n",
    "            \n",
    "        results[model_name] = {}\n",
    "        \n",
    "        for prompt, level_name in zip(prompts, level_names):\n",
    "            system_prompt = \"You are an Ace Attorney expert.\"\n",
    "            print(f\"Generating for {model_name} - {level_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate based on model provider\n",
    "                if \"o1-\" in model_name or \"o3-\" in model_name or \"o4-\" in model_name or \"gpt-\" in model_name:\n",
    "                    generated_text = openai_text_reasoning_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "\n",
    "                elif \"claude-3-7-sonnet-20250219(thinking)\" in model_name:\n",
    "                    generated_text = anthropic_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=\"claude-3-7-sonnet-20250219\",\n",
    "                        prompt=prompt,\n",
    "                        thinking=True\n",
    "                    )\n",
    "                    \n",
    "                elif \"claude\" in model_name:\n",
    "                    generated_text = anthropic_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        thinking=False\n",
    "                    )\n",
    "                    \n",
    "                elif \"gemini\" in model_name:\n",
    "                    generated_text = gemini_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt\n",
    "                    )\n",
    "                    \n",
    "                elif \"llama-4-maverick\" in model_name.lower():\n",
    "                    generated_text = together_ai_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                elif \"grok-3\" in model_name:\n",
    "                    generated_text = xai_grok_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                \n",
    "                results[model_name][level_name] = generated_text\n",
    "                \n",
    "                # Save after each successful generation\n",
    "                with open(\"generated_texts.json\", \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating for {model_name} - {level_name}: {str(e)}\")\n",
    "                # Save partial results even if there's an error\n",
    "                with open(\"generated_texts.json\", \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def compute_similarity(generated_texts, transcripts):\n",
    "    # Load SBERT model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    similarity_scores = {}\n",
    "    \n",
    "    for model_name, texts in generated_texts.items():\n",
    "        similarity_scores[model_name] = []\n",
    "        \n",
    "        # Debug print\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "        print(f\"Available texts: {list(texts.keys())}\")\n",
    "        \n",
    "        try:\n",
    "            for i, level_name in enumerate(level_names):\n",
    "                if level_name in texts:\n",
    "                    # Get embeddings\n",
    "                    emb_gen = model.encode(texts[level_name], show_progress_bar=False)\n",
    "                    emb_trans = model.encode(transcripts[i], show_progress_bar=False)\n",
    "                    \n",
    "                    # Calculate cosine similarity\n",
    "                    sim_score = cosine_similarity([emb_gen], [emb_trans])[0][0]\n",
    "                    similarity_scores[model_name].append(sim_score)\n",
    "                    \n",
    "                    print(f\"Computed similarity for {level_name}: {sim_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Missing text for {level_name} in {model_name}\")\n",
    "                    similarity_scores[model_name].append(0.0)  # or np.nan if you prefer\n",
    "                \n",
    "        except KeyError as e:\n",
    "            print(f\"Error processing {model_name}: Missing key {e}\")\n",
    "            print(f\"Available keys: {list(texts.keys())}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# Step 4: Calculate total similarity and correlation with performance\n",
    "def analyze_correlation(similarity_scores, model_performance):\n",
    "    # Calculate total similarity score for each model\n",
    "    total_similarity = {}\n",
    "    for model_name, scores in similarity_scores.items():\n",
    "        total_similarity[model_name] = sum(scores)\n",
    "    \n",
    "    # Prepare data for correlation\n",
    "    models = []\n",
    "    sim_scores = []\n",
    "    ranks = []\n",
    "    game_scores = []\n",
    "    \n",
    "    for model_name, total_sim in total_similarity.items():\n",
    "        if model_name in model_performance:\n",
    "            models.append(model_name)\n",
    "            sim_scores.append(total_sim)\n",
    "            ranks.append(model_performance[model_name][\"rank\"])\n",
    "            game_scores.append(model_performance[model_name][\"score\"])\n",
    "    \n",
    "    # Calculate Spearman's rank correlation with rank\n",
    "    rank_rho, rank_p_val = spearmanr(sim_scores, ranks)\n",
    "    \n",
    "    # Calculate Spearman's rank correlation with game score\n",
    "    score_rho, score_p_val = spearmanr(sim_scores, game_scores)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Model\": models,\n",
    "        \"Total Similarity\": sim_scores,\n",
    "        \"Performance Rank\": ranks,\n",
    "        \"Game Score\": game_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by total similarity\n",
    "    results_df = results_df.sort_values(\"Total Similarity\", ascending=False)\n",
    "    \n",
    "    correlation_results = {\n",
    "        \"rank_correlation\": rank_rho,\n",
    "        \"rank_p_value\": rank_p_val,\n",
    "        \"score_correlation\": score_rho,\n",
    "        \"score_p_value\": score_p_val\n",
    "    }\n",
    "    \n",
    "    return results_df, correlation_results, total_similarity\n",
    "\n",
    "# Step 5: Main Function\n",
    "def run_ablation_study():\n",
    "    # Setup data\n",
    "    prompts, level_names, model_performance, transcripts = setup_study_data()\n",
    "    \n",
    "    # Define models to test as specified by the user\n",
    "    models = [\n",
    "        \"o3-2025-04-16\",\n",
    "        \"gemini-2.5-pro-preview-03-25\",\n",
    "        \"claude-3-7-sonnet-20250219(thinking)\",\n",
    "        \"claude-3-5-sonnet-20241022\",\n",
    "        \"o4-mini-2025-04-16\",\n",
    "        \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Generate descriptions\n",
    "    print(\"Generating model descriptions...\")\n",
    "    generated_texts = generate_descriptions(models, prompts, level_names)\n",
    "    \n",
    "    # Compute similarities\n",
    "    print(\"Computing semantic similarities...\")\n",
    "    similarity_scores = compute_similarity(generated_texts, transcripts)\n",
    "    \n",
    "    # Analyze correlations\n",
    "    print(\"Analyzing correlations...\")\n",
    "    results_df, correlation_results, total_similarity = analyze_correlation(similarity_scores, model_performance)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nCorrelation Results:\")\n",
    "    print(f\"Correlation with rank: {correlation_results['rank_correlation']:.4f} (p-value = {correlation_results['rank_p_value']:.4f})\")\n",
    "    print(f\"Correlation with game score: {correlation_results['score_correlation']:.4f} (p-value = {correlation_results['score_p_value']:.4f})\")\n",
    "    \n",
    "    # Create a detailed DataFrame showing similarity scores for each prompt\n",
    "    detailed_df = pd.DataFrame({\n",
    "        \"Model\": list(similarity_scores.keys()),\n",
    "        \"First Turnabout Similarity\": [scores[0] for scores in similarity_scores.values()],\n",
    "        \"Sister Turnabout Similarity\": [scores[1] for scores in similarity_scores.values()],\n",
    "        \"Total Similarity\": [total_similarity[model] for model in similarity_scores.keys()]\n",
    "    })\n",
    "    \n",
    "    # Add performance data where available\n",
    "    performance_rank = []\n",
    "    performance_score = []\n",
    "    \n",
    "    for model in detailed_df[\"Model\"]:\n",
    "        if model in model_performance:\n",
    "            performance_rank.append(model_performance[model][\"rank\"])\n",
    "            performance_score.append(model_performance[model][\"score\"])\n",
    "        else:\n",
    "            performance_rank.append(np.nan)\n",
    "            performance_score.append(np.nan)\n",
    "    \n",
    "    detailed_df[\"Performance Rank\"] = performance_rank\n",
    "    detailed_df[\"Game Score\"] = performance_score\n",
    "    \n",
    "    # Sort by total similarity\n",
    "    detailed_df = detailed_df.sort_values(\"Total Similarity\", ascending=False)\n",
    "    \n",
    "    return results_df, correlation_results, detailed_df, generated_texts, similarity_scores\n",
    "\n",
    "# Run the study if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, correlation_results, detailed_df, generated_texts, similarity_scores = run_ablation_study()\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(detailed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_to_keep = [\n",
    "#     \"o3-2025-04-16\",\n",
    "#     \"gemini-2.5-pro-preview-03-25\",\n",
    "#     \"claude-3-7-sonnet-20250219(thinking)\",\n",
    "#     \"claude-3-5-sonnet-20241022\",\n",
    "#     \"o4-mini-2025-04-16\",\n",
    "#     \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "# ]\n",
    "\n",
    "# detailed_df = detailed_df[detailed_df[\"Model\"].isin(models_to_keep)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average embedding cosine similarity between cases: 0.6994\n",
      "\n",
      "Updated Results with Embedding Cross Correlation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>First Turnabout Similarity</th>\n",
       "      <th>Sister Turnabout Similarity</th>\n",
       "      <th>Total Similarity</th>\n",
       "      <th>Performance Rank</th>\n",
       "      <th>Game Score</th>\n",
       "      <th>Cross Similarity Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>0.863010</td>\n",
       "      <td>0.904428</td>\n",
       "      <td>1.767438</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.657062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini-2.5-pro-preview-03-25</td>\n",
       "      <td>0.866726</td>\n",
       "      <td>0.866952</td>\n",
       "      <td>1.733678</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.531750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>0.845162</td>\n",
       "      <td>0.815580</td>\n",
       "      <td>1.660742</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.671986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>o1-2024-12-17</td>\n",
       "      <td>0.808985</td>\n",
       "      <td>0.842305</td>\n",
       "      <td>1.651290</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.674810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>grok-3-mini-beta</td>\n",
       "      <td>0.781703</td>\n",
       "      <td>0.832628</td>\n",
       "      <td>1.614331</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.707115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4.1-2025-04-14</td>\n",
       "      <td>0.754770</td>\n",
       "      <td>0.812457</td>\n",
       "      <td>1.567227</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.787370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-3-7-sonnet-20250219(thinking)</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.830473</td>\n",
       "      <td>1.538438</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.588225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gemini-2.5-flash-preview-04-17</td>\n",
       "      <td>0.731344</td>\n",
       "      <td>0.728147</td>\n",
       "      <td>1.459491</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.729042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>0.658813</td>\n",
       "      <td>0.779637</td>\n",
       "      <td>1.438450</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.810311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama-4-Maverick-17B-128E-Instruct-FP8</td>\n",
       "      <td>0.490601</td>\n",
       "      <td>0.733536</td>\n",
       "      <td>1.224137</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.771773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o4-mini-2025-04-16</td>\n",
       "      <td>0.586401</td>\n",
       "      <td>0.625346</td>\n",
       "      <td>1.211747</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.763769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model  First Turnabout Similarity  \\\n",
       "0                            o3-2025-04-16                    0.863010   \n",
       "1             gemini-2.5-pro-preview-03-25                    0.866726   \n",
       "3               claude-3-5-sonnet-20241022                    0.845162   \n",
       "6                            o1-2024-12-17                    0.808985   \n",
       "10                        grok-3-mini-beta                    0.781703   \n",
       "7                       gpt-4.1-2025-04-14                    0.754770   \n",
       "2     claude-3-7-sonnet-20250219(thinking)                    0.707965   \n",
       "9           gemini-2.5-flash-preview-04-17                    0.731344   \n",
       "8       gemini-2.0-flash-thinking-exp-1219                    0.658813   \n",
       "5   Llama-4-Maverick-17B-128E-Instruct-FP8                    0.490601   \n",
       "4                       o4-mini-2025-04-16                    0.586401   \n",
       "\n",
       "    Sister Turnabout Similarity  Total Similarity  Performance Rank  \\\n",
       "0                      0.904428          1.767438                 2   \n",
       "1                      0.866952          1.733678                 3   \n",
       "3                      0.815580          1.660742                 6   \n",
       "6                      0.842305          1.651290                 1   \n",
       "10                     0.832628          1.614331                 5   \n",
       "7                      0.812457          1.567227                 7   \n",
       "2                      0.830473          1.538438                 4   \n",
       "9                      0.728147          1.459491                 8   \n",
       "8                      0.779637          1.438450                 9   \n",
       "5                      0.733536          1.224137                13   \n",
       "4                      0.625346          1.211747                11   \n",
       "\n",
       "    Game Score  Cross Similarity Scores  \n",
       "0           23                 0.657062  \n",
       "1           20                 0.531750  \n",
       "3            6                 0.671986  \n",
       "6           26                 0.674810  \n",
       "10           7                 0.707115  \n",
       "7            6                 0.787370  \n",
       "2            8                 0.588225  \n",
       "9            4                 0.729042  \n",
       "8            4                 0.810311  \n",
       "5            0                 0.771773  \n",
       "4            1                 0.763769  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def add_embedding_cross_correlation(detailed_df, generated_texts):\n",
    "    # Initialize SBERT model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Calculate cross-correlation using embeddings for each model\n",
    "    cross_correlations = []\n",
    "    \n",
    "    for model_name in detailed_df['Model']:\n",
    "        if model_name in generated_texts:\n",
    "            # Check if both required texts exist for this model\n",
    "            if \"The First Turnabout\" in generated_texts[model_name] and \"Turnabout Sisters - (Part 1 to Part 4)\" in generated_texts[model_name]:\n",
    "                # Get the generated texts for both cases\n",
    "                first_turnabout_text = generated_texts[model_name][\"The First Turnabout\"]\n",
    "                sister_turnabout_text = generated_texts[model_name][\"Turnabout Sisters - (Part 1 to Part 4)\"]\n",
    "                \n",
    "                # Get embeddings for both texts\n",
    "                first_turnabout_embedding = model.encode(first_turnabout_text, show_progress_bar=False)\n",
    "                sister_turnabout_embedding = model.encode(sister_turnabout_text, show_progress_bar=False)\n",
    "                \n",
    "                # Calculate cosine similarity between embedding vectors\n",
    "                # Reshape embeddings to 2D arrays for cosine_similarity\n",
    "                sim_score = cosine_similarity([first_turnabout_embedding], [sister_turnabout_embedding])[0][0]\n",
    "                cross_correlations.append(sim_score)\n",
    "            else:\n",
    "                # If either text is missing, append NaN\n",
    "                cross_correlations.append(np.nan)\n",
    "        else:\n",
    "            cross_correlations.append(np.nan)\n",
    "    \n",
    "    # Add the embedding-based cross-correlation scores as a new column\n",
    "    detailed_df['Cross Similarity Scores'] = cross_correlations\n",
    "    \n",
    "    # Calculate average embedding correlation across all models\n",
    "    valid_correlations = [c for c in cross_correlations if not np.isnan(c)]\n",
    "    avg_correlation = np.mean(valid_correlations)\n",
    "    print(f\"\\nAverage embedding cosine similarity between cases: {avg_correlation:.4f}\")\n",
    "    \n",
    "    return detailed_df\n",
    "\n",
    "# To use this function, you would add this after getting your detailed_df:\n",
    "detailed_df = add_embedding_cross_correlation(detailed_df, generated_texts)\n",
    "\n",
    "# Display the updated DataFrame with the new column\n",
    "print(\"\\nUpdated Results with Embedding Cross Correlation Scores:\")\n",
    "detailed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              o3-2025-04-16\n",
       "1               gemini-2.5-pro-preview-03-25\n",
       "3                 claude-3-5-sonnet-20241022\n",
       "6                              o1-2024-12-17\n",
       "10                          grok-3-mini-beta\n",
       "7                         gpt-4.1-2025-04-14\n",
       "2       claude-3-7-sonnet-20250219(thinking)\n",
       "9             gemini-2.5-flash-preview-04-17\n",
       "8         gemini-2.0-flash-thinking-exp-1219\n",
       "5     Llama-4-Maverick-17B-128E-Instruct-FP8\n",
       "4                         o4-mini-2025-04-16\n",
       "Name: Model, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_df[\"Model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_keep = [\n",
    "    \"o3-2025-04-16\",\n",
    "    \"gemini-2.5-pro-preview-03-25\",\n",
    "    \"claude-3-7-sonnet-20250219(thinking)\",\n",
    "    \"claude-3-5-sonnet-20241022\",\n",
    "    \"o4-mini-2025-04-16\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "    # \"o1-2024-12-17\",\n",
    "    # \"grok-3-mini-beta\",\n",
    "    # \"gemini-2.5-flash-preview-04-17\",\n",
    "    # \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "    # \"gpt-4.1-2025-04-14\"\n",
    "]\n",
    "\n",
    "detailed_df = detailed_df[detailed_df[\"Model\"].isin(models_to_keep)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>First Turnabout Similarity</th>\n",
       "      <th>Sister Turnabout Similarity</th>\n",
       "      <th>Total Similarity</th>\n",
       "      <th>Performance Rank</th>\n",
       "      <th>Game Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>0.863010</td>\n",
       "      <td>0.904428</td>\n",
       "      <td>1.767438</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini-2.5-pro-preview-03-25</td>\n",
       "      <td>0.866726</td>\n",
       "      <td>0.866952</td>\n",
       "      <td>1.733678</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>0.845162</td>\n",
       "      <td>0.815580</td>\n",
       "      <td>1.660742</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-3-7-sonnet-20250219(thinking)</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.830473</td>\n",
       "      <td>1.538438</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama-4-Maverick-17B-128E-Instruct-FP8</td>\n",
       "      <td>0.490601</td>\n",
       "      <td>0.733536</td>\n",
       "      <td>1.224137</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o4-mini-2025-04-16</td>\n",
       "      <td>0.586401</td>\n",
       "      <td>0.625346</td>\n",
       "      <td>1.211747</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model  First Turnabout Similarity  \\\n",
       "0                           o3-2025-04-16                    0.863010   \n",
       "1            gemini-2.5-pro-preview-03-25                    0.866726   \n",
       "3              claude-3-5-sonnet-20241022                    0.845162   \n",
       "2    claude-3-7-sonnet-20250219(thinking)                    0.707965   \n",
       "5  Llama-4-Maverick-17B-128E-Instruct-FP8                    0.490601   \n",
       "4                      o4-mini-2025-04-16                    0.586401   \n",
       "\n",
       "   Sister Turnabout Similarity  Total Similarity  Performance Rank  Game Score  \n",
       "0                     0.904428          1.767438                 2          23  \n",
       "1                     0.866952          1.733678                 3          20  \n",
       "3                     0.815580          1.660742                 6           6  \n",
       "2                     0.830473          1.538438                 4           8  \n",
       "5                     0.733536          1.224137                13           0  \n",
       "4                     0.625346          1.211747                11           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground truth cross-correlation: 0.5988\n"
     ]
    }
   ],
   "source": [
    "# Calculate ground truth cross-correlation using SBERT embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "with open(\"./ace_attorney_ground_truth.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "    \n",
    "# Extract transcripts in the same order as level_names\n",
    "transcripts = [ground_truth[level_name] for level_name in level_names]\n",
    "\n",
    "# Get embeddings for both transcripts\n",
    "first_turnabout_embedding = model.encode(transcripts[0], show_progress_bar=False)\n",
    "sister_turnabout_embedding = model.encode(transcripts[1], show_progress_bar=False)\n",
    "\n",
    "# Calculate cosine similarity between embedding vectors\n",
    "sim_score = cosine_similarity([first_turnabout_embedding], [sister_turnabout_embedding])[0][0]\n",
    "print(f\"\\nGround truth cross-correlation: {sim_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation plots have been generated and saved as:\n",
      "1. correlation_analysis.png\n",
      "2. model_performance.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_correlation_analysis(detailed_df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_standardized = detailed_df.copy()\n",
    "    \n",
    "    # Standardize the values\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized[['Total Similarity', 'Game Score', 'Performance Rank']] = scaler.fit_transform(\n",
    "        df_standardized[['Total Similarity', 'Game Score', 'Performance Rank']]\n",
    "    )\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Function to calculate correlation and p-value\n",
    "    def get_correlation_stats(x, y):\n",
    "        corr, p_value = stats.pearsonr(x, y)\n",
    "        return corr, p_value\n",
    "    \n",
    "    # Plot 1: Linear Regression Plot for Game Score\n",
    "    sns.regplot(\n",
    "        data=df_standardized,\n",
    "        x='Total Similarity',\n",
    "        y='Game Score',\n",
    "        ax=ax1,\n",
    "        scatter_kws={'alpha':0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    ax1.set_title('Linear Correlation: Similarity vs Game Score\\n(Standardized Values)')\n",
    "    ax1.set_xlabel('Total Similarity Score (Standardized)')\n",
    "    ax1.set_ylabel('Game Score (Standardized)')\n",
    "    \n",
    "    # Calculate correlation and p-value for Game Score\n",
    "    corr_score, p_value_score = get_correlation_stats(\n",
    "        df_standardized['Total Similarity'], \n",
    "        df_standardized['Game Score']\n",
    "    )\n",
    "    \n",
    "    # Add correlation coefficient and p-value to the plot\n",
    "    significance = \"***\" if p_value_score < 0.001 else \"**\" if p_value_score < 0.01 else \"*\" if p_value_score < 0.05 else \"ns\"\n",
    "    ax1.text(0.05, 0.95, \n",
    "             f'Correlation: {corr_score:.3f} {significance}\\np-value: {p_value_score:.3e}', \n",
    "             transform=ax1.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Plot 2: Linear Regression Plot for Performance Rank\n",
    "    sns.regplot(\n",
    "        data=df_standardized,\n",
    "        x='Total Similarity',\n",
    "        y='Performance Rank',\n",
    "        ax=ax2,\n",
    "        scatter_kws={'alpha':0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    ax2.set_title('Linear Correlation: Similarity vs Performance Rank\\n(Standardized Values)')\n",
    "    ax2.set_xlabel('Total Similarity Score (Standardized)')\n",
    "    ax2.set_ylabel('Performance Rank (Standardized)')\n",
    "    \n",
    "    # Calculate correlation and p-value for Performance Rank\n",
    "    corr_rank, p_value_rank = get_correlation_stats(\n",
    "        df_standardized['Total Similarity'], \n",
    "        df_standardized['Performance Rank']\n",
    "    )\n",
    "    \n",
    "    # Add correlation coefficient and p-value to the plot\n",
    "    significance = \"***\" if p_value_rank < 0.001 else \"**\" if p_value_rank < 0.01 else \"*\" if p_value_rank < 0.05 else \"ns\"\n",
    "    ax2.text(0.05, 0.95, \n",
    "             f'Correlation: {corr_rank:.3f} {significance}\\np-value: {p_value_rank:.3e}', \n",
    "             transform=ax2.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Plot 3: Heatmap\n",
    "    # Standardize all columns for correlation\n",
    "    corr_columns = ['First Turnabout Similarity', \n",
    "                   'Sister Turnabout Similarity',\n",
    "                   'Total Similarity',\n",
    "                   'Game Score',\n",
    "                   'Performance Rank']\n",
    "    \n",
    "    # Calculate correlation matrix and p-values using standardized values\n",
    "    corr_matrix = df_standardized[corr_columns].corr()\n",
    "    p_values = pd.DataFrame(\n",
    "        [[stats.pearsonr(df_standardized[i], df_standardized[j])[1] \n",
    "          for j in corr_columns] for i in corr_columns],\n",
    "        columns=corr_columns,\n",
    "        index=corr_columns\n",
    "    )\n",
    "    \n",
    "    # Create heatmap with significance stars\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        ax=ax3,\n",
    "        fmt='.2f',\n",
    "        square=True\n",
    "    )\n",
    "    ax3.set_title('Correlation Heatmap\\n(Standardized Values)')\n",
    "    \n",
    "    # Add significance stars to heatmap\n",
    "    for i in range(len(corr_columns)):\n",
    "        for j in range(len(corr_columns)):\n",
    "            p_val = p_values.iloc[i, j]\n",
    "            if p_val < 0.001:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"***\", \n",
    "                        ha='center', va='center', color='black')\n",
    "            elif p_val < 0.01:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"**\", \n",
    "                        ha='center', va='center', color='black')\n",
    "            elif p_val < 0.05:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"*\", \n",
    "                        ha='center', va='center', color='black')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_performance(detailed_df):\n",
    "    # Create a copy and standardize values\n",
    "    df_standardized = detailed_df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized[['Total Similarity', 'Game Score']] = scaler.fit_transform(\n",
    "        df_standardized[['Total Similarity', 'Game Score']]\n",
    "    )\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Sort dataframe by Game Score\n",
    "    sorted_df = df_standardized.sort_values('Game Score', ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_df['Model'], sorted_df['Game Score'])\n",
    "    \n",
    "    # Add Total Similarity as scatter points\n",
    "    plt.scatter(range(len(sorted_df)), \n",
    "               sorted_df['Total Similarity'],\n",
    "               color='red',\n",
    "               s=100,\n",
    "               label='Total Similarity')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Model Performance vs Similarity Scores\\n(Standardized Values)')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Standardized Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def generate_correlation_plots(detailed_df):\n",
    "    # Generate both plots\n",
    "    plot_correlation_analysis(detailed_df)\n",
    "    plot_model_performance(detailed_df)\n",
    "    \n",
    "    print(\"Correlation plots have been generated and saved as:\")\n",
    "    print(\"1. correlation_analysis.png\")\n",
    "    print(\"2. model_performance.png\")\n",
    "\n",
    "# Example usage:\n",
    "# After running your ablation study and getting detailed_df:\n",
    "generate_correlation_plots(detailed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ace Attorney without Data Contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1)  Imports\n",
    "###############################################################################\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "###############################################################################\n",
    "# 2)  Load only the Ace-Attorney rows from the new-rank JSON\n",
    "###############################################################################\n",
    "def load_ace_attorney(json_path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with the Ace-Attorney results only:\n",
    "\n",
    "        Model · New Rank · New Score · Evaluated Score\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    ace_rows = []\n",
    "    ace_results = data.get(\"Ace Attorney\", {}).get(\"results\", [])\n",
    "    for res in ace_results:\n",
    "        ace_rows.append({\n",
    "            \"Model\":           res.get(\"model\"),\n",
    "            \"New Rank\":        res.get(\"rank\"),\n",
    "            \"New Score\":       res.get(\"score\"),\n",
    "            \"Evaluated Score\": res.get(\"evaluator result\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(ace_rows)\n",
    "\n",
    "###############################################################################\n",
    "# 3)  Merge with your old similarity table\n",
    "###############################################################################\n",
    "def merge_with_similarity(detailed_df: pd.DataFrame,\n",
    "                          ace_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • detailed_df must already contain  Model  and  Total Similarity.\n",
    "    • Only models that appear in *both* tables survive the merge.\n",
    "    \"\"\"\n",
    "    return pd.merge(\n",
    "        detailed_df[[\"Model\", \"Total Similarity\"]],\n",
    "        ace_df,\n",
    "        on=\"Model\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "###############################################################################\n",
    "# 4)  Four-panel correlation plot (unchanged, but re-posted for completeness)\n",
    "###############################################################################\n",
    "def plot_new_correlations(df,\n",
    "                          similarity_col='Total Similarity',\n",
    "                          new_rank_col='New Rank',\n",
    "                          new_score_col='New Score',\n",
    "                          eval_score_col='Evaluated Score',\n",
    "                          out_file='ace_correlation_analysis.png'):\n",
    "\n",
    "    # ── keep rows that have every column we need ─────────────────────────────\n",
    "    df_use = df[[similarity_col,\n",
    "                 new_rank_col,\n",
    "                 new_score_col,\n",
    "                 eval_score_col]].dropna().copy()\n",
    "\n",
    "    # ── z-score for comparability ────────────────────────────────────────────\n",
    "    scaler = StandardScaler()\n",
    "    df_use[df_use.columns] = scaler.fit_transform(df_use)\n",
    "\n",
    "    # ── canvas & spec ────────────────────────────────────────────────────────\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(32, 6))  # now 5 panels\n",
    "    specs = [\n",
    "        (similarity_col, new_rank_col,  'Similarity vs New Rank'),\n",
    "        (similarity_col, new_score_col, 'Similarity vs New Score'),\n",
    "        (eval_score_col, new_rank_col,  'Evaluated Score vs New Rank'),\n",
    "        (eval_score_col, new_score_col, 'Evaluated Score vs New Score'),\n",
    "        (eval_score_col, similarity_col, 'Evaluated Score vs Similarity'),  # new one\n",
    "    ]\n",
    "\n",
    "    # helper to annotate r & p\n",
    "    def annotate(ax, x, y):\n",
    "        r, p = stats.pearsonr(df_use[x], df_use[y])\n",
    "        stars = \"***\" if p < 1e-3 else \"**\" if p < 1e-2 else \"*\" if p < .05 else \"ns\"\n",
    "        ax.text(0.05, 0.95,\n",
    "                f\"R = {r:.3f} {stars}\\np = {p:.2e}\",\n",
    "                transform=ax.transAxes,\n",
    "                ha='left', va='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    # draw each panel\n",
    "    for ax, (x, y, title) in zip(axes, specs):\n",
    "        sns.regplot(data=df_use, x=x, y=y,\n",
    "                    scatter_kws={'alpha': .5},\n",
    "                    line_kws={'color': 'red'},\n",
    "                    ax=ax)\n",
    "        ax.set_title(title + '\\n(Standardised)')\n",
    "        ax.set_xlabel(f'{x} (z-score)')\n",
    "        ax.set_ylabel(f'{y} (z-score)')\n",
    "        annotate(ax, x, y)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "###############################################################################\n",
    "# 5)  End-to-end driver\n",
    "###############################################################################\n",
    "def build_and_plot_ace(detailed_df,\n",
    "                       json_path='rank_without_datacontainmation.json',\n",
    "                       out_file='ace_correlation_analysis.png'):\n",
    "\n",
    "    ace_df     = load_ace_attorney(json_path)\n",
    "    merged_df  = merge_with_similarity(detailed_df, ace_df)\n",
    "    plot_new_correlations(merged_df, out_file=out_file)\n",
    "\n",
    "    print(f\"✔ Four-panel Ace-Attorney figure saved as “{out_file}”\")\n",
    "    print(f\"  Models included: {merged_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Four-panel Ace-Attorney figure saved as “New_ace_correlation_analysis.png”\n",
      "  Models included: 6\n"
     ]
    }
   ],
   "source": [
    "detailed_df = detailed_df\n",
    "build_and_plot_ace(detailed_df,\n",
    "                   json_path='rank_without_datacontainmation.json',\n",
    "                   out_file='New_ace_correlation_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super Mario Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
