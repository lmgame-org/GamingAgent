{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ace Attorney Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuxua\\miniconda3\\envs\\local_cua\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, pathlib; sys.path.append(str(pathlib.Path().resolve().parent))\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import base64\n",
    "\n",
    "from tools.serving.api_providers import (\n",
    "    openai_text_reasoning_completion,\n",
    "    anthropic_text_completion,\n",
    "    gemini_text_completion,\n",
    "    together_ai_completion,\n",
    "    xai_grok_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model descriptions...\n",
      "Skipping o1-2024-12-17 - already exists in results\n",
      "Skipping o3-2025-04-16 - already exists in results\n",
      "Skipping gemini-2.5-pro-exp-03-25 - already exists in results\n",
      "Skipping claude-3-7-sonnet-20250219(thinking) - already exists in results\n",
      "Skipping gpt-4.1-2025-04-14 - already exists in results\n",
      "Skipping claude-3-5-sonnet-20241022 - already exists in results\n",
      "Skipping gemini-2.0-flash-thinking-exp-1219 - already exists in results\n",
      "Skipping gemini-2.5-flash-preview-04-17 - already exists in results\n",
      "Skipping o4-mini-2025-04-16 - already exists in results\n",
      "Skipping Llama-4-Maverick-17B-128E-Instruct-FP8 - already exists in results\n",
      "Skipping grok-3-beta - already exists in results\n",
      "Generating for grok-3-mini-beta - The First Turnabout\n",
      "current reasoning effort: high\n",
      "Generating for grok-3-mini-beta - Turnabout Sisters - (Part 1 to Part 4)\n",
      "current reasoning effort: high\n",
      "Computing semantic similarities...\n",
      "Processing model: o1-2024-12-17\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8923\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8381\n",
      "Processing model: o3-2025-04-16\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8234\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8985\n",
      "Processing model: gemini-2.5-pro-exp-03-25\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.9032\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.9025\n",
      "Processing model: claude-3-7-sonnet-20250219(thinking)\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8547\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.9088\n",
      "Processing model: gpt-4.1-2025-04-14\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8467\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8394\n",
      "Processing model: claude-3-5-sonnet-20241022\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8049\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8730\n",
      "Processing model: gemini-2.0-flash-thinking-exp-1219\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8656\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8925\n",
      "Processing model: gemini-2.5-flash-preview-04-17\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.9105\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.6918\n",
      "Processing model: o4-mini-2025-04-16\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.7855\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.7560\n",
      "Processing model: Llama-4-Maverick-17B-128E-Instruct-FP8\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.6057\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.6892\n",
      "Processing model: grok-3-beta\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.9023\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8231\n",
      "Processing model: grok-3-mini-beta\n",
      "Available texts: ['The First Turnabout', 'Turnabout Sisters - (Part 1 to Part 4)']\n",
      "Computed similarity for The First Turnabout: 0.8222\n",
      "Computed similarity for Turnabout Sisters - (Part 1 to Part 4): 0.8238\n",
      "Analyzing correlations...\n",
      "\n",
      "Correlation Results:\n",
      "Correlation with rank: -0.6636 (p-value = 0.0260)\n",
      "Correlation with game score: 0.6986 (p-value = 0.0168)\n",
      "\n",
      "Detailed Results:\n",
      "                                     Model  First Turnabout Similarity  \\\n",
      "2                 gemini-2.5-pro-exp-03-25                    0.903199   \n",
      "3     claude-3-7-sonnet-20250219(thinking)                    0.854651   \n",
      "6       gemini-2.0-flash-thinking-exp-1219                    0.865594   \n",
      "0                            o1-2024-12-17                    0.892339   \n",
      "10                             grok-3-beta                    0.902313   \n",
      "1                            o3-2025-04-16                    0.823373   \n",
      "4                       gpt-4.1-2025-04-14                    0.846685   \n",
      "5               claude-3-5-sonnet-20241022                    0.804925   \n",
      "11                        grok-3-mini-beta                    0.822153   \n",
      "7           gemini-2.5-flash-preview-04-17                    0.910507   \n",
      "8                       o4-mini-2025-04-16                    0.785536   \n",
      "9   Llama-4-Maverick-17B-128E-Instruct-FP8                    0.605661   \n",
      "\n",
      "    Sister Turnabout Similarity  Total Similarity  Performance Rank  \\\n",
      "2                      0.902520          1.805719               3.0   \n",
      "3                      0.908786          1.763437               4.0   \n",
      "6                      0.892488          1.758082               9.0   \n",
      "0                      0.838106          1.730446               1.0   \n",
      "10                     0.823115          1.725428               NaN   \n",
      "1                      0.898478          1.721851               2.0   \n",
      "4                      0.839394          1.686079               7.0   \n",
      "5                      0.873020          1.677945               6.0   \n",
      "11                     0.823807          1.645959               5.0   \n",
      "7                      0.691844          1.602350               8.0   \n",
      "8                      0.755971          1.541507              11.0   \n",
      "9                      0.689155          1.294815              13.0   \n",
      "\n",
      "    Game Score  \n",
      "2         20.0  \n",
      "3          8.0  \n",
      "6          4.0  \n",
      "0         26.0  \n",
      "10         NaN  \n",
      "1         23.0  \n",
      "4          6.0  \n",
      "5          6.0  \n",
      "11         7.0  \n",
      "7          4.0  \n",
      "8          1.0  \n",
      "9          0.0  \n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template with {level name} as the placeholder\n",
    "prompt_template = \"\"\"\n",
    "You are an expert on *Phoenix Wright: Ace Attorney*.\n",
    "\n",
    "LEVEL = “{level name}”\n",
    "\n",
    "Return **exactly** the three sections below, in the order shown, with *nothing* else before or after them.\n",
    "\n",
    "1) Detailed Narrative Description:\n",
    "<Write one coherent paragraph in strict chronological order, including:\n",
    "  • exact dates/times (use the game’s timeline),\n",
    "  • locations,\n",
    "  • all relevant characters’ motives,\n",
    "  • any off-camera investigation Phoenix, Mia, or their allies perform.>\n",
    "\n",
    "2) Complete Evidence List:\n",
    "- \"Exact In-Game Item Name\" (Type) Relevance: one-sentence explanation of why it helps the defense.\n",
    "- (repeat bullet for every distinct item)\n",
    "\n",
    "3) Cross-Examination Breakdown:\n",
    "- **<Witness Name>, Round X**\n",
    "  - Statement: <key line from testimony>\n",
    "  - Present: <evidence name>  (or “Press” if simply pressing)\n",
    "  - Contradiction Exposed: <logical inconsistency uncovered>\n",
    "  - Impact: <how this advances Phoenix’s case>\n",
    "- (add bullets for every round / witness)\n",
    "\n",
    "**Formatting rules (must follow strictly):**\n",
    "- Use the section headings *exactly* as written (including numbers, parentheses, and colons).\n",
    "- Leave exactly one blank line between sections.\n",
    "- Section 1 is plain prose (no lists).\n",
    "- Sections 2 & 3 must be bullet lists that match the patterns shown.\n",
    "- Do **not** add extra commentary, headings, or markdown.\n",
    "- If a detail is unknown in-game, omit it rather than inventing filler.\n",
    "\"\"\"\n",
    "\n",
    "# Define the level names\n",
    "level_names = [\"The First Turnabout\", \"Turnabout Sisters - (Part 1 to Part 4)\"]\n",
    "\n",
    "# Create the actual prompts\n",
    "prompts = [prompt_template.format(**{\"level name\": level_name}) for level_name in level_names]\n",
    "\n",
    "# Step 1: Define prompts and load performance data\n",
    "def setup_study_data():\n",
    "    # Load the performance data\n",
    "    with open(\"./rank_data_03_25_2025.json\", \"r\") as f:\n",
    "        performance_data = json.load(f)\n",
    "    \n",
    "    # Extract Ace Attorney ranks\n",
    "    ace_attorney_data = performance_data[\"Ace Attorney\"][\"results\"]\n",
    "    \n",
    "    # Create a dictionary mapping model names to ranks and scores\n",
    "    model_performance = {}\n",
    "    for result in ace_attorney_data:\n",
    "        model_name = result[\"model\"]\n",
    "        model_performance[model_name] = {\n",
    "            \"rank\": result[\"rank\"],\n",
    "            \"levels_cracked\": result[\"levels_cracked\"],\n",
    "            \"score\": result[\"score\"]\n",
    "        }\n",
    "    \n",
    "    # Load ground truth transcripts\n",
    "    with open(\"./ace_attorney_ground_truth.json\", \"r\") as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    # Extract transcripts in the same order as level_names\n",
    "    transcripts = [ground_truth[level_name] for level_name in level_names]\n",
    "    \n",
    "    return prompts, level_names, model_performance, transcripts\n",
    "\n",
    "# Step 2: Generate Text Descriptions\n",
    "def generate_descriptions(models, prompts, level_names):\n",
    "    # First try to load existing results\n",
    "    try:\n",
    "        with open(\"generated_texts.json\", \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        # Skip if model results already exist\n",
    "        if model_name in results:\n",
    "            print(f\"Skipping {model_name} - already exists in results\")\n",
    "            continue\n",
    "            \n",
    "        results[model_name] = {}\n",
    "        \n",
    "        for prompt, level_name in zip(prompts, level_names):\n",
    "            system_prompt = \"You are an Ace Attorney expert.\"\n",
    "            print(f\"Generating for {model_name} - {level_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate based on model provider\n",
    "                if \"o1-\" in model_name or \"o3-\" in model_name or \"o4-\" in model_name or \"gpt-\" in model_name:\n",
    "                    generated_text = openai_text_reasoning_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "\n",
    "                elif \"claude-3-7-sonnet-20250219(thinking)\" in model_name:\n",
    "                    generated_text = anthropic_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=\"claude-3-7-sonnet-20250219\",\n",
    "                        prompt=prompt,\n",
    "                        thinking=True\n",
    "                    )\n",
    "                    \n",
    "                elif \"claude\" in model_name:\n",
    "                    generated_text = anthropic_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        thinking=False\n",
    "                    )\n",
    "                    \n",
    "                elif \"gemini\" in model_name:\n",
    "                    generated_text = gemini_text_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt\n",
    "                    )\n",
    "                    \n",
    "                elif \"llama-4-maverick\" in model_name.lower():\n",
    "                    generated_text = together_ai_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                elif \"grok-3\" in model_name:\n",
    "                    generated_text = xai_grok_completion(\n",
    "                        system_prompt=system_prompt,\n",
    "                        model_name=model_name,\n",
    "                        prompt=prompt,\n",
    "                        temperature=0\n",
    "                    )\n",
    "                \n",
    "                results[model_name][level_name] = generated_text\n",
    "                \n",
    "                # Save after each successful generation\n",
    "                with open(\"generated_texts.json\", \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating for {model_name} - {level_name}: {str(e)}\")\n",
    "                # Save partial results even if there's an error\n",
    "                with open(\"generated_texts.json\", \"w\") as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "                continue\n",
    "            \n",
    "    return results\n",
    "\n",
    "def compute_similarity(generated_texts, transcripts):\n",
    "    # Load SBERT model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    similarity_scores = {}\n",
    "    \n",
    "    for model_name, texts in generated_texts.items():\n",
    "        similarity_scores[model_name] = []\n",
    "        \n",
    "        # Debug print\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "        print(f\"Available texts: {list(texts.keys())}\")\n",
    "        \n",
    "        try:\n",
    "            for i, level_name in enumerate(level_names):\n",
    "                if level_name in texts:\n",
    "                    # Get embeddings\n",
    "                    emb_gen = model.encode(texts[level_name], show_progress_bar=False)\n",
    "                    emb_trans = model.encode(transcripts[i], show_progress_bar=False)\n",
    "                    \n",
    "                    # Calculate cosine similarity\n",
    "                    sim_score = cosine_similarity([emb_gen], [emb_trans])[0][0]\n",
    "                    similarity_scores[model_name].append(sim_score)\n",
    "                    \n",
    "                    print(f\"Computed similarity for {level_name}: {sim_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Missing text for {level_name} in {model_name}\")\n",
    "                    similarity_scores[model_name].append(0.0)  # or np.nan if you prefer\n",
    "                \n",
    "        except KeyError as e:\n",
    "            print(f\"Error processing {model_name}: Missing key {e}\")\n",
    "            print(f\"Available keys: {list(texts.keys())}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "# Step 4: Calculate total similarity and correlation with performance\n",
    "def analyze_correlation(similarity_scores, model_performance):\n",
    "    # Calculate total similarity score for each model\n",
    "    total_similarity = {}\n",
    "    for model_name, scores in similarity_scores.items():\n",
    "        total_similarity[model_name] = sum(scores)\n",
    "    \n",
    "    # Prepare data for correlation\n",
    "    models = []\n",
    "    sim_scores = []\n",
    "    ranks = []\n",
    "    game_scores = []\n",
    "    \n",
    "    for model_name, total_sim in total_similarity.items():\n",
    "        if model_name in model_performance:\n",
    "            models.append(model_name)\n",
    "            sim_scores.append(total_sim)\n",
    "            ranks.append(model_performance[model_name][\"rank\"])\n",
    "            game_scores.append(model_performance[model_name][\"score\"])\n",
    "    \n",
    "    # Calculate Spearman's rank correlation with rank\n",
    "    rank_rho, rank_p_val = spearmanr(sim_scores, ranks)\n",
    "    \n",
    "    # Calculate Spearman's rank correlation with game score\n",
    "    score_rho, score_p_val = spearmanr(sim_scores, game_scores)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Model\": models,\n",
    "        \"Total Similarity\": sim_scores,\n",
    "        \"Performance Rank\": ranks,\n",
    "        \"Game Score\": game_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by total similarity\n",
    "    results_df = results_df.sort_values(\"Total Similarity\", ascending=False)\n",
    "    \n",
    "    correlation_results = {\n",
    "        \"rank_correlation\": rank_rho,\n",
    "        \"rank_p_value\": rank_p_val,\n",
    "        \"score_correlation\": score_rho,\n",
    "        \"score_p_value\": score_p_val\n",
    "    }\n",
    "    \n",
    "    return results_df, correlation_results, total_similarity\n",
    "\n",
    "# Step 5: Main Function\n",
    "def run_ablation_study():\n",
    "    # Setup data\n",
    "    prompts, level_names, model_performance, transcripts = setup_study_data()\n",
    "    \n",
    "    # Define models to test as specified by the user\n",
    "    models = [\n",
    "        \"o1-2024-12-17\",\n",
    "        \"o3-2025-04-16\",\n",
    "        \"gemini-2.5-pro-exp-03-25\",\n",
    "        \"claude-3-7-sonnet-20250219(thinking)\",\n",
    "        \"gpt-4.1-2025-04-14\",\n",
    "        \"claude-3-5-sonnet-20241022\",\n",
    "        \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "        \"gemini-2.5-flash-preview-04-17\",\n",
    "        \"o4-mini-2025-04-16\",\n",
    "        \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "        \"grok-3-mini-beta\"\n",
    "    ]\n",
    "    \n",
    "    # Generate descriptions\n",
    "    print(\"Generating model descriptions...\")\n",
    "    generated_texts = generate_descriptions(models, prompts, level_names)\n",
    "    \n",
    "    # Compute similarities\n",
    "    print(\"Computing semantic similarities...\")\n",
    "    similarity_scores = compute_similarity(generated_texts, transcripts)\n",
    "    \n",
    "    # Analyze correlations\n",
    "    print(\"Analyzing correlations...\")\n",
    "    results_df, correlation_results, total_similarity = analyze_correlation(similarity_scores, model_performance)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nCorrelation Results:\")\n",
    "    print(f\"Correlation with rank: {correlation_results['rank_correlation']:.4f} (p-value = {correlation_results['rank_p_value']:.4f})\")\n",
    "    print(f\"Correlation with game score: {correlation_results['score_correlation']:.4f} (p-value = {correlation_results['score_p_value']:.4f})\")\n",
    "    \n",
    "    # Create a detailed DataFrame showing similarity scores for each prompt\n",
    "    detailed_df = pd.DataFrame({\n",
    "        \"Model\": list(similarity_scores.keys()),\n",
    "        \"First Turnabout Similarity\": [scores[0] for scores in similarity_scores.values()],\n",
    "        \"Sister Turnabout Similarity\": [scores[1] for scores in similarity_scores.values()],\n",
    "        \"Total Similarity\": [total_similarity[model] for model in similarity_scores.keys()]\n",
    "    })\n",
    "    \n",
    "    # Add performance data where available\n",
    "    performance_rank = []\n",
    "    performance_score = []\n",
    "    \n",
    "    for model in detailed_df[\"Model\"]:\n",
    "        if model in model_performance:\n",
    "            performance_rank.append(model_performance[model][\"rank\"])\n",
    "            performance_score.append(model_performance[model][\"score\"])\n",
    "        else:\n",
    "            performance_rank.append(np.nan)\n",
    "            performance_score.append(np.nan)\n",
    "    \n",
    "    detailed_df[\"Performance Rank\"] = performance_rank\n",
    "    detailed_df[\"Game Score\"] = performance_score\n",
    "    \n",
    "    # Sort by total similarity\n",
    "    detailed_df = detailed_df.sort_values(\"Total Similarity\", ascending=False)\n",
    "    \n",
    "    return results_df, correlation_results, detailed_df, generated_texts, similarity_scores\n",
    "\n",
    "# Run the study if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, correlation_results, detailed_df, generated_texts, similarity_scores = run_ablation_study()\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(detailed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average embedding cosine similarity between cases: 0.6925\n",
      "\n",
      "Updated Results with Embedding Cross Correlation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>First Turnabout Similarity</th>\n",
       "      <th>Sister Turnabout Similarity</th>\n",
       "      <th>Total Similarity</th>\n",
       "      <th>Performance Rank</th>\n",
       "      <th>Game Score</th>\n",
       "      <th>Embedding Cross Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>0.903199</td>\n",
       "      <td>0.902520</td>\n",
       "      <td>1.805719</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.600194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-7-sonnet-20250219(thinking)</td>\n",
       "      <td>0.854651</td>\n",
       "      <td>0.908786</td>\n",
       "      <td>1.763437</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.652369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>0.865594</td>\n",
       "      <td>0.892488</td>\n",
       "      <td>1.758082</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.686458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o1-2024-12-17</td>\n",
       "      <td>0.892339</td>\n",
       "      <td>0.838106</td>\n",
       "      <td>1.730446</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.611148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>grok-3-beta</td>\n",
       "      <td>0.902313</td>\n",
       "      <td>0.823115</td>\n",
       "      <td>1.725428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.655004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>0.823373</td>\n",
       "      <td>0.898478</td>\n",
       "      <td>1.721851</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.630078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1-2025-04-14</td>\n",
       "      <td>0.846685</td>\n",
       "      <td>0.839394</td>\n",
       "      <td>1.686079</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.649394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>0.804925</td>\n",
       "      <td>0.873020</td>\n",
       "      <td>1.677945</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.672999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grok-3-mini-beta</td>\n",
       "      <td>0.822153</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>1.645959</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.769331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gemini-2.5-flash-preview-04-17</td>\n",
       "      <td>0.910507</td>\n",
       "      <td>0.691844</td>\n",
       "      <td>1.602350</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.876161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>o4-mini-2025-04-16</td>\n",
       "      <td>0.785536</td>\n",
       "      <td>0.755971</td>\n",
       "      <td>1.541507</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.669578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Llama-4-Maverick-17B-128E-Instruct-FP8</td>\n",
       "      <td>0.605661</td>\n",
       "      <td>0.689155</td>\n",
       "      <td>1.294815</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.837322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model  First Turnabout Similarity  \\\n",
       "2                 gemini-2.5-pro-exp-03-25                    0.903199   \n",
       "3     claude-3-7-sonnet-20250219(thinking)                    0.854651   \n",
       "6       gemini-2.0-flash-thinking-exp-1219                    0.865594   \n",
       "0                            o1-2024-12-17                    0.892339   \n",
       "10                             grok-3-beta                    0.902313   \n",
       "1                            o3-2025-04-16                    0.823373   \n",
       "4                       gpt-4.1-2025-04-14                    0.846685   \n",
       "5               claude-3-5-sonnet-20241022                    0.804925   \n",
       "11                        grok-3-mini-beta                    0.822153   \n",
       "7           gemini-2.5-flash-preview-04-17                    0.910507   \n",
       "8                       o4-mini-2025-04-16                    0.785536   \n",
       "9   Llama-4-Maverick-17B-128E-Instruct-FP8                    0.605661   \n",
       "\n",
       "    Sister Turnabout Similarity  Total Similarity  Performance Rank  \\\n",
       "2                      0.902520          1.805719               3.0   \n",
       "3                      0.908786          1.763437               4.0   \n",
       "6                      0.892488          1.758082               9.0   \n",
       "0                      0.838106          1.730446               1.0   \n",
       "10                     0.823115          1.725428               NaN   \n",
       "1                      0.898478          1.721851               2.0   \n",
       "4                      0.839394          1.686079               7.0   \n",
       "5                      0.873020          1.677945               6.0   \n",
       "11                     0.823807          1.645959               5.0   \n",
       "7                      0.691844          1.602350               8.0   \n",
       "8                      0.755971          1.541507              11.0   \n",
       "9                      0.689155          1.294815              13.0   \n",
       "\n",
       "    Game Score  Embedding Cross Correlation  \n",
       "2         20.0                     0.600194  \n",
       "3          8.0                     0.652369  \n",
       "6          4.0                     0.686458  \n",
       "0         26.0                     0.611148  \n",
       "10         NaN                     0.655004  \n",
       "1         23.0                     0.630078  \n",
       "4          6.0                     0.649394  \n",
       "5          6.0                     0.672999  \n",
       "11         7.0                     0.769331  \n",
       "7          4.0                     0.876161  \n",
       "8          1.0                     0.669578  \n",
       "9          0.0                     0.837322  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def add_embedding_cross_correlation(detailed_df, generated_texts):\n",
    "    # Initialize SBERT model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Calculate cross-correlation using embeddings for each model\n",
    "    cross_correlations = []\n",
    "    \n",
    "    for model_name in detailed_df['Model']:\n",
    "        if model_name in generated_texts:\n",
    "            # Check if both required texts exist for this model\n",
    "            if \"The First Turnabout\" in generated_texts[model_name] and \"Turnabout Sisters - (Part 1 to Part 4)\" in generated_texts[model_name]:\n",
    "                # Get the generated texts for both cases\n",
    "                first_turnabout_text = generated_texts[model_name][\"The First Turnabout\"]\n",
    "                sister_turnabout_text = generated_texts[model_name][\"Turnabout Sisters - (Part 1 to Part 4)\"]\n",
    "                \n",
    "                # Get embeddings for both texts\n",
    "                first_turnabout_embedding = model.encode(first_turnabout_text, show_progress_bar=False)\n",
    "                sister_turnabout_embedding = model.encode(sister_turnabout_text, show_progress_bar=False)\n",
    "                \n",
    "                # Calculate cosine similarity between embedding vectors\n",
    "                # Reshape embeddings to 2D arrays for cosine_similarity\n",
    "                sim_score = cosine_similarity([first_turnabout_embedding], [sister_turnabout_embedding])[0][0]\n",
    "                cross_correlations.append(sim_score)\n",
    "            else:\n",
    "                # If either text is missing, append NaN\n",
    "                cross_correlations.append(np.nan)\n",
    "        else:\n",
    "            cross_correlations.append(np.nan)\n",
    "    \n",
    "    # Add the embedding-based cross-correlation scores as a new column\n",
    "    detailed_df['Cross Similarity Scores'] = cross_correlations\n",
    "    \n",
    "    # Calculate average embedding correlation across all models\n",
    "    valid_correlations = [c for c in cross_correlations if not np.isnan(c)]\n",
    "    avg_correlation = np.mean(valid_correlations)\n",
    "    print(f\"\\nAverage embedding cosine similarity between cases: {avg_correlation:.4f}\")\n",
    "    \n",
    "    return detailed_df\n",
    "\n",
    "# To use this function, you would add this after getting your detailed_df:\n",
    "detailed_df = add_embedding_cross_correlation(detailed_df, generated_texts)\n",
    "\n",
    "# Display the updated DataFrame with the new column\n",
    "print(\"\\nUpdated Results with Embedding Cross Correlation Scores:\")\n",
    "detailed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground truth cross-correlation: 0.6803\n"
     ]
    }
   ],
   "source": [
    "# Calculate ground truth cross-correlation using SBERT embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "with open(\"./ace_attorney_ground_truth.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "    \n",
    "# Extract transcripts in the same order as level_names\n",
    "transcripts = [ground_truth[level_name] for level_name in level_names]\n",
    "\n",
    "# Get embeddings for both transcripts\n",
    "first_turnabout_embedding = model.encode(transcripts[0], show_progress_bar=False)\n",
    "sister_turnabout_embedding = model.encode(transcripts[1], show_progress_bar=False)\n",
    "\n",
    "# Calculate cosine similarity between embedding vectors\n",
    "sim_score = cosine_similarity([first_turnabout_embedding], [sister_turnabout_embedding])[0][0]\n",
    "print(f\"\\nGround truth cross-correlation: {sim_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation plots have been generated and saved as:\n",
      "1. correlation_analysis.png\n",
      "2. model_performance.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_correlation_analysis(detailed_df):\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_standardized = detailed_df.copy()\n",
    "    \n",
    "    # Standardize the values\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized[['Total Similarity', 'Game Score', 'Performance Rank']] = scaler.fit_transform(\n",
    "        df_standardized[['Total Similarity', 'Game Score', 'Performance Rank']]\n",
    "    )\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Function to calculate correlation and p-value\n",
    "    def get_correlation_stats(x, y):\n",
    "        corr, p_value = stats.pearsonr(x, y)\n",
    "        return corr, p_value\n",
    "    \n",
    "    # Plot 1: Linear Regression Plot for Game Score\n",
    "    sns.regplot(\n",
    "        data=df_standardized,\n",
    "        x='Total Similarity',\n",
    "        y='Game Score',\n",
    "        ax=ax1,\n",
    "        scatter_kws={'alpha':0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    ax1.set_title('Linear Correlation: Similarity vs Game Score\\n(Standardized Values)')\n",
    "    ax1.set_xlabel('Total Similarity Score (Standardized)')\n",
    "    ax1.set_ylabel('Game Score (Standardized)')\n",
    "    \n",
    "    # Calculate correlation and p-value for Game Score\n",
    "    corr_score, p_value_score = get_correlation_stats(\n",
    "        df_standardized['Total Similarity'], \n",
    "        df_standardized['Game Score']\n",
    "    )\n",
    "    \n",
    "    # Add correlation coefficient and p-value to the plot\n",
    "    significance = \"***\" if p_value_score < 0.001 else \"**\" if p_value_score < 0.01 else \"*\" if p_value_score < 0.05 else \"ns\"\n",
    "    ax1.text(0.05, 0.95, \n",
    "             f'Correlation: {corr_score:.3f} {significance}\\np-value: {p_value_score:.3e}', \n",
    "             transform=ax1.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Plot 2: Linear Regression Plot for Performance Rank\n",
    "    sns.regplot(\n",
    "        data=df_standardized,\n",
    "        x='Total Similarity',\n",
    "        y='Performance Rank',\n",
    "        ax=ax2,\n",
    "        scatter_kws={'alpha':0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    ax2.set_title('Linear Correlation: Similarity vs Performance Rank\\n(Standardized Values)')\n",
    "    ax2.set_xlabel('Total Similarity Score (Standardized)')\n",
    "    ax2.set_ylabel('Performance Rank (Standardized)')\n",
    "    \n",
    "    # Calculate correlation and p-value for Performance Rank\n",
    "    corr_rank, p_value_rank = get_correlation_stats(\n",
    "        df_standardized['Total Similarity'], \n",
    "        df_standardized['Performance Rank']\n",
    "    )\n",
    "    \n",
    "    # Add correlation coefficient and p-value to the plot\n",
    "    significance = \"***\" if p_value_rank < 0.001 else \"**\" if p_value_rank < 0.01 else \"*\" if p_value_rank < 0.05 else \"ns\"\n",
    "    ax2.text(0.05, 0.95, \n",
    "             f'Correlation: {corr_rank:.3f} {significance}\\np-value: {p_value_rank:.3e}', \n",
    "             transform=ax2.transAxes, \n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Plot 3: Heatmap\n",
    "    # Standardize all columns for correlation\n",
    "    corr_columns = ['First Turnabout Similarity', \n",
    "                   'Sister Turnabout Similarity',\n",
    "                   'Total Similarity',\n",
    "                   'Game Score',\n",
    "                   'Performance Rank']\n",
    "    \n",
    "    # Calculate correlation matrix and p-values using standardized values\n",
    "    corr_matrix = df_standardized[corr_columns].corr()\n",
    "    p_values = pd.DataFrame(\n",
    "        [[stats.pearsonr(df_standardized[i], df_standardized[j])[1] \n",
    "          for j in corr_columns] for i in corr_columns],\n",
    "        columns=corr_columns,\n",
    "        index=corr_columns\n",
    "    )\n",
    "    \n",
    "    # Create heatmap with significance stars\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        ax=ax3,\n",
    "        fmt='.2f',\n",
    "        square=True\n",
    "    )\n",
    "    ax3.set_title('Correlation Heatmap\\n(Standardized Values)')\n",
    "    \n",
    "    # Add significance stars to heatmap\n",
    "    for i in range(len(corr_columns)):\n",
    "        for j in range(len(corr_columns)):\n",
    "            p_val = p_values.iloc[i, j]\n",
    "            if p_val < 0.001:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"***\", \n",
    "                        ha='center', va='center', color='black')\n",
    "            elif p_val < 0.01:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"**\", \n",
    "                        ha='center', va='center', color='black')\n",
    "            elif p_val < 0.05:\n",
    "                ax3.text(j + 0.5, i + 0.5, \"*\", \n",
    "                        ha='center', va='center', color='black')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_model_performance(detailed_df):\n",
    "    # Create a copy and standardize values\n",
    "    df_standardized = detailed_df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    df_standardized[['Total Similarity', 'Game Score']] = scaler.fit_transform(\n",
    "        df_standardized[['Total Similarity', 'Game Score']]\n",
    "    )\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Sort dataframe by Game Score\n",
    "    sorted_df = df_standardized.sort_values('Game Score', ascending=False)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(sorted_df['Model'], sorted_df['Game Score'])\n",
    "    \n",
    "    # Add Total Similarity as scatter points\n",
    "    plt.scatter(range(len(sorted_df)), \n",
    "               sorted_df['Total Similarity'],\n",
    "               color='red',\n",
    "               s=100,\n",
    "               label='Total Similarity')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Model Performance vs Similarity Scores\\n(Standardized Values)')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Standardized Score')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def generate_correlation_plots(detailed_df):\n",
    "    # Generate both plots\n",
    "    plot_correlation_analysis(detailed_df)\n",
    "    plot_model_performance(detailed_df)\n",
    "    \n",
    "    print(\"Correlation plots have been generated and saved as:\")\n",
    "    print(\"1. correlation_analysis.png\")\n",
    "    print(\"2. model_performance.png\")\n",
    "\n",
    "# Example usage:\n",
    "# After running your ablation study and getting detailed_df:\n",
    "generate_correlation_plots(detailed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Mario Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuxua\\miniconda3\\envs\\local_cua\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1-1': 'GameName: Super Mario Bros.\\nLevelNumber: 1-1\\nLevelDetails: area=overworld;\\n              onscreen=flat_path,bushes,little_goomba,question_block;\\n              upcoming=six_block_triangle_q_bricks_mushroom,triple_pipes_goombas_between,\\n                       bonus_pipe_19_coins_skip,hidden_1up_block,pit_after_pipes,\\n                       question_block_item,falling_goombas_block_row,ten_coin_brick,\\n                       starman_brick,second_q_triangle_fireflower,koopa_troopa,\\n                       extra_goombas,brick_question_row,pyramid_hard_blocks_gap,\\n                       double_pyramid_hard_blocks_pit,exit_pipe_from_bonus,\\n                       two_goombas_four_block_row,inaccessible_pipe_end,staircase,flagpole'}\n"
     ]
    }
   ],
   "source": [
    "import sys, pathlib; sys.path.append(str(pathlib.Path().resolve().parent))\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import base64\n",
    "\n",
    "from tools.serving.api_providers import (\n",
    "    openai_text_reasoning_completion,\n",
    "    anthropic_text_completion,\n",
    "    gemini_text_completion,\n",
    "    together_ai_completion,\n",
    "    xai_grok_completion,\n",
    "    openai_completion,\n",
    "    anthropic_completion,\n",
    "    gemini_completion,\n",
    "    together_ai_completion,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"o4-mini-2025-04-16\",\n",
    "    \"o3-2025-04-16\",\n",
    "    \"gemini-2.5-pro-exp-03-25\",\n",
    "    \"claude-3-7-sonnet-20250219\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    \"claude-3-5-sonnet-20241022\"]\n",
    "\n",
    "# Prompt template for vision models\n",
    "PROMPT = \"\"\"You are given ONE RGB frame from an NES game.\n",
    "Respond with EXACTLY these three lines—no extra text, no blank lines:\n",
    "\n",
    "GameName: <full title or UNKNOWN>\n",
    "LevelNumber: <world-stage, e.g. 1-1, or UNKNOWN>\n",
    "LevelDetails: <semi-colon–separated list in this template>\n",
    "              area=<area_type>;\n",
    "              onscreen=<comma-separated facts>;\n",
    "              upcoming=<comma-separated fine-grained events for rest of level>\n",
    "\n",
    "Formatting rules\n",
    "• <area_type> one of: overworld, underground, water, castle, bonus\n",
    "• <onscreen> list ONLY objects & terrain entirely visible NOW\n",
    "• <upcoming> list EVERY key event that will happen later in THIS level,\n",
    "  expressed as lowercase snake-case tokens, in left→right order\n",
    "  (see examples). If nothing, write [].\n",
    "• No synonyms, plurals, or re-ordering: use the exact vocabulary below.\n",
    "  ── allowed upcoming tokens ──\n",
    "  six_block_triangle_q_bricks_mushroom, triple_pipes_goombas_between,\n",
    "  bonus_pipe_19_coins_skip, hidden_1up_block, pit_after_pipes,\n",
    "  question_block_item, falling_goombas_block_row, ten_coin_brick,\n",
    "  starman_brick, second_q_triangle_fireflower, koopa_troopa,\n",
    "  extra_goombas, brick_question_row, pyramid_hard_blocks_gap,\n",
    "  double_pyramid_hard_blocks_pit, exit_pipe_from_bonus,\n",
    "  two_goombas_four_block_row, inaccessible_pipe_end, staircase, flagpole\n",
    "• If unsure of a field, write UNKNOWN (not empty).\n",
    "• Do NOT break the three-line structure under any circumstance.\"\"\"\n",
    "\n",
    "with open(\"super_mario_bros_ground_truth.json\", \"r\") as f:\n",
    "  ground_truth = json.load(f)\n",
    "ground_truth[\"1-1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Encode an image file to base64.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def analyze_single_image(image_path, level_id=\"1-1\"):\n",
    "    \"\"\"Analyze a single Mario screenshot using all models.\"\"\"\n",
    "    # Create output dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Result file\n",
    "    result_file = \"super_mario_bros_generated_text.json\"\n",
    "    \n",
    "    # Load existing results if file exists\n",
    "    try:\n",
    "        with open(result_file, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"Loaded existing results from {result_file}\")\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        results = {}\n",
    "    \n",
    "    # System prompt\n",
    "    system_prompt = \"You are an expert at identifying NES game content.\"\n",
    "    \n",
    "    # Encode the image once\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    # Process each model\n",
    "    for model_name in MODEL_NAMES:\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        \n",
    "        # Initialize model's results if not already present\n",
    "        if model_name not in results:\n",
    "            results[model_name] = {}\n",
    "        \n",
    "        try:\n",
    "            # Call the appropriate API based on model type\n",
    "            if \"o1-\" in model_name or \"o3-\" in model_name or \"o4-\" in model_name or \"gpt-\" in model_name:\n",
    "                response = openai_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=model_name,\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=PROMPT,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "            elif \"claude\" in model_name:\n",
    "                response = anthropic_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=model_name,\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=PROMPT,\n",
    "                    thinking=False\n",
    "                )\n",
    "                \n",
    "            elif \"gemini\" in model_name:\n",
    "                response = gemini_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=model_name,\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=PROMPT\n",
    "                )\n",
    "                \n",
    "            elif \"llama-4-maverick\" in model_name.lower():\n",
    "                response = together_ai_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=model_name,\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=PROMPT\n",
    "                )\n",
    "                \n",
    "            elif \"grok\" in model_name.lower():\n",
    "                # Note: grok may not support images directly\n",
    "                print(f\"Skipping {model_name} - image support not available\")\n",
    "                continue\n",
    "            \n",
    "            # Store the result\n",
    "            results[model_name][level_id] = response\n",
    "            print(f\"Result for {model_name}:\\n{response}\\n\")\n",
    "            \n",
    "            # Save after each successful generation\n",
    "            with open(result_file, \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your Mario screenshot\n",
    "    image_path = \"mario_1-1.png\"  # Replace with your actual image path\n",
    "    \n",
    "    # Level ID (derived from filename or specified manually)\n",
    "    level_id = \"1-1\"\n",
    "    \n",
    "    # Analyze the image\n",
    "    results = analyze_single_image(image_path, level_id)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"Results saved to super_mario_bros_generated_text.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Replace with your image directory path\n",
    "    image_dir = \"mario_screenshots\"\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(image_dir):\n",
    "        print(f\"Directory {image_dir} does not exist. Please create it and add screenshot images.\")\n",
    "        print(\"Each image filename should match its level ID in the ground truth file.\")\n",
    "    else:\n",
    "        main(image_dir) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_cua",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
