{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Mario Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib; sys.path.append(str(pathlib.Path().resolve().parent))\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "import base64\n",
    "\n",
    "from tools.serving.api_providers import (\n",
    "    openai_text_reasoning_completion,\n",
    "    anthropic_text_completion,\n",
    "    gemini_text_completion,\n",
    "    together_ai_completion,\n",
    "    xai_grok_completion,\n",
    "    openai_completion,\n",
    "    anthropic_completion,\n",
    "    gemini_completion,\n",
    "    together_ai_completion,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"o4-mini-2025-04-16\",\n",
    "    \"o3-2025-04-16\",\n",
    "    \"gemini-2.5-pro-exp-03-25\",\n",
    "    \"claude-3-7-sonnet-20250219\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    \"claude-3-5-sonnet-20241022\"]\n",
    "\n",
    "# Prompt template for vision models\n",
    "PROMPT = \"\"\"You are given ONE RGB frame from a super mario bros level\n",
    "\n",
    "Provide EXACTLY {num_timestamps} timestamped observations describing the level in chronological order.\n",
    "\n",
    "FORMAT REQUIREMENTS (STRICTLY ENFORCE):\n",
    "- Each observation MUST follow this exact pattern: [T#] One single concise sentence.\n",
    "- Each entry MUST be separated by exactly two line breaks\n",
    "- Each observation MUST be ONLY ONE SENTENCE long (15-25 words maximum)\n",
    "- DO NOT use bullets, numbering, or any special formatting\n",
    "- NO introduction or conclusion text\n",
    "\n",
    "EXAMPLE FORMAT:\n",
    "[T1] This level starts out on a flat, open path.\n",
    "\n",
    "[T2] As Mario moves forward, a Little Goomba appears and starts walking towards him.\n",
    "\n",
    "[T3] Mario then reaches the first Question Block of the game.\n",
    "\n",
    "CONTENT REQUIREMENTS:\n",
    "- [T1] MUST describe Mario's starting position\n",
    "- Number entries sequentially from [T1] to [T{num_timestamps}]\n",
    "- Each observation MUST focus on ONE specific element or section\n",
    "- Use exact Super Mario terminology: \"Question Block\", \"Brick Block\", \"Little Goomba\", \"Koopa Troopa\", \"Piranha Plant\", etc.\n",
    "- Include precise descriptions of terrain features, enemies, items, obstacles, and architectural elements\n",
    "- Cover the entire level from start to finish in proper sequence\n",
    "- End with the flagpole and level conclusion\n",
    "\n",
    "REMEMBER: You MUST provide EXACTLY {num_timestamps} observations, no more and no less.\n",
    "\"\"\"\n",
    "with open(\"super_mario_bros_ground_truth.json\", \"r\") as f:\n",
    "  ground_truth = json.load(f)\n",
    "ground_truth[\"1-1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 18 timestamps for level 1-1 based on ground truth\n",
      "You are given ONE RGB frame from a super mario bros level\n",
      "\n",
      "Provide EXACTLY 18 timestamped observations describing the level in chronological order.\n",
      "\n",
      "FORMAT REQUIREMENTS (STRICTLY ENFORCE):\n",
      "- Each observation MUST follow this exact pattern: [T#] One single concise sentence.\n",
      "- Each entry MUST be separated by exactly two line breaks\n",
      "- Each observation MUST be ONLY ONE SENTENCE long (15-25 words maximum)\n",
      "- DO NOT use bullets, numbering, or any special formatting\n",
      "- NO introduction or conclusion text\n",
      "\n",
      "EXAMPLE FORMAT:\n",
      "[T1] This level starts out on a flat, open path.\n",
      "\n",
      "[T2] As Mario moves forward, a Little Goomba appears and starts walking towards him.\n",
      "\n",
      "[T3] Mario then reaches the first Question Block of the game.\n",
      "\n",
      "CONTENT REQUIREMENTS:\n",
      "- [T1] MUST describe Mario's starting position\n",
      "- Number entries sequentially from [T1] to [T18]\n",
      "- Each observation MUST focus on ONE specific element or section\n",
      "- Use exact Super Mario terminology: \"Question Block\", \"Brick Block\", \"Little Goomba\", \"Koopa Troopa\", \"Piranha Plant\", etc.\n",
      "- Include precise descriptions of terrain features, enemies, items, obstacles, and architectural elements\n",
      "- Cover the entire level from start to finish in proper sequence\n",
      "- End with the flagpole and level conclusion\n",
      "\n",
      "REMEMBER: You MUST provide EXACTLY 18 observations, no more and no less.\n",
      "\n",
      "\n",
      "Processing model: o4-mini-2025-04-16\n",
      "Result for o4-mini-2025-04-16:\n",
      "[T1] Mario spawns as small Mario at the leftmost edge of the flat grass platform at the very start of the level.\n",
      "\n",
      "[T2] A series of isolated Brick Blocks appear overhead shortly ahead of Mario's starting position to provide early breakable cover.\n",
      "\n",
      "[T3] Just past the first bricks appears a floating Question Block containing a Super Mushroom that grants Mario growth upon activation.\n",
      "\n",
      "[T4] Mario then encounters a pair of Brick Blocks forming a small ceiling that can be broken if Mario gains sufficient size.\n",
      "\n",
      "[T5] A Little Goomba emerges from the right and waddles toward Mario across the flat ground, posing the first enemy challenge.\n",
      "\n",
      "[T6] A green Koopa Troopa walks slowly along the ground and retreats into its shell when stomped by Mario's downward jump.\n",
      "\n",
      "[T7] Mario approaches a one-block wide gap requiring a running jump to safely clear without falling into the abyss.\n",
      "\n",
      "[T8] Beyond the gap, the ground resumes and reveals a taller set of three stacked Brick Blocks offering breakable platforms overhead.\n",
      "\n",
      "[T9] A warp pipe extends from the ground housing a Piranha Plant that periodically emerges and retreats in timed intervals.\n",
      "\n",
      "[T10] After the first pipe, a series of low green hills and rounded bushes decorate the background against a clear blue sky.\n",
      "\n",
      "[T11] Further on, Mario reaches a cluster of three consecutive Question Blocks, one containing a Fire Flower to enable projectile attacks.\n",
      "\n",
      "[T12] Past these blocks, a pair of green Koopa Troopas patrol side by side across the platform, requiring careful timing.\n",
      "\n",
      "[T13] Mario then ascends a two-block high staircase made of Brick Blocks leading to an elevated plateau with better vantage.\n",
      "\n",
      "[T14] On the plateau, two adjacent pipes offer additional Piranha Plant obstacles that require patience to pass safely.\n",
      "\n",
      "[T15] Descending from the plateau, Mario navigates a second one-block gap that flows immediately back into flat ground stretch.\n",
      "\n",
      "[T16] A final Goomba strolls near the level end as the distant flagpole and small fortress castle come into view on the horizon.\n",
      "\n",
      "[T17] Mario sprints toward the tall flagpole and jumps to grab the slider, triggering the descending flag and point bonus display.\n",
      "\n",
      "[T18] The lowered flag reveals a small castle entrance where Mario marches inside, completing World 1-1 with the familiar level ending scene.\n",
      "\n",
      "\n",
      "Processing model: o3-2025-04-16\n",
      "Result for o3-2025-04-16:\n",
      "[T1] Mario starts small on the far-left edge of a straight brick-lined ground with a green hill backdrop.\n",
      "\n",
      "[T2] Immediately ahead, five Brick Blocks and one central Question Block hover overhead, while the first Little Goomba waddles rightward towards Mario.\n",
      "\n",
      "[T3] Striking the Question Block yields a Super Mushroom that slides right along the ground past the hill, ready for collection.\n",
      "\n",
      "[T4] Mario next encounters a pair of spaced green pipes emerging from the ground, each occasionally spawning a snapping Piranha Plant.\n",
      "\n",
      "[T5] Between those pipes, two more Little Goombas approach in tandem over flat terrain, offering easy stomp combo opportunities.\n",
      "\n",
      "[T6] After the second pipe, three floating Question Blocks form a row above bricks, with the middle hiding a helpful Fire Flower.\n",
      "\n",
      "[T7] A lone Koopa Troopa patrols beneath these blocks, giving Mario the chance to kick its shell through nearby enemies.\n",
      "\n",
      "[T8] Shortly past this, a tall staircase of ground bricks leads up then down, concealing a hidden 1-Up Block atop the first step.\n",
      "\n",
      "[T9] Dropping from the staircase, Mario meets a trio of Little Goombas marching single file across more brick-patterned ground.\n",
      "\n",
      "[T10] An orange Warp Pipe here offers optional entrance to an underground coin room, returning Mario above via a vertical pipe ahead.\n",
      "\n",
      "[T11] Back outside, five floating Brick Blocks form a low ceiling, with the far-right block containing the famous multi-coin power-up.\n",
      "\n",
      "[T12] A gap appears next, spanned by three suspended Brick Blocks that Mario can use as stepping stones to cross safely.\n",
      "\n",
      "[T13] Past the gap, Mario encounters the first Red Koopa Troopa pacing on a small platform of bricks before a second pit.\n",
      "\n",
      "[T14] Clearing this pit, a large cluster of Brick Blocks and Question Blocks towers overhead, with one Question Block yielding a Starman.\n",
      "\n",
      "[T15] Invincibility lets Mario plow through a line of six Little Goombas and a Green Koopa Troopa patrolling the long descent.\n",
      "\n",
      "[T16] He then ascends a final twin-tier brick staircase, knocking coins from a solitary Brick Block midway up the rise.\n",
      "\n",
      "[T17] Reaching the summit, Mario jumps toward the tall Flagpole, slides down, and topples the flag as fireworks prepare.\n",
      "\n",
      "[T18] Finally, he walks into the small stone castle, the level ending fanfare playing and scoreboard tallying his remaining time.\n",
      "\n",
      "\n",
      "Processing model: gemini-2.5-pro-exp-03-25\n",
      "error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      "]\n",
      "Error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      "]\n",
      "Result for gemini-2.5-pro-exp-03-25:\n",
      "\n",
      "\n",
      "\n",
      "Processing model: claude-3-7-sonnet-20250219\n",
      "anthropic vision-text activated... thinking: False\n",
      "Result for claude-3-7-sonnet-20250219:\n",
      "[T1] Mario begins World 1-1 standing on a brick path with zero points and 400 seconds on the timer.\n",
      "\n",
      "\n",
      "[T2] A small green hill rises in the background to Mario's left, creating the iconic Mushroom Kingdom landscape.\n",
      "\n",
      "\n",
      "[T3] The score display shows \"MARIO 000000\" with no coins collected yet at the start of the level.\n",
      "\n",
      "\n",
      "[T4] A single white cloud floats in the bright blue sky above the starting area.\n",
      "\n",
      "\n",
      "[T5] The brick path stretches ahead, forming the foundation of the level's initial platform.\n",
      "\n",
      "\n",
      "[T6] The HUD indicates this is \"WORLD 1-1,\" the very first level of Super Mario Bros.\n",
      "\n",
      "\n",
      "[T7] Mario appears in his small form, wearing his signature red and brown outfit.\n",
      "\n",
      "\n",
      "[T8] Green bushes are visible in the distance on the right side of the screen.\n",
      "\n",
      "\n",
      "[T9] The brown brick blocks that form the ground have a distinctive pattern with alternating light and dark sections.\n",
      "\n",
      "\n",
      "[T10] No enemies are visible in this initial screen, giving the player a safe starting area.\n",
      "\n",
      "\n",
      "[T11] The timer continues to count down from 400, creating urgency to complete the level.\n",
      "\n",
      "\n",
      "[T12] The coin counter shows \"×00\" indicating Mario hasn't collected any coins yet.\n",
      "\n",
      "\n",
      "[T13] The level's color palette uses the NES's characteristic bright blues, greens, and browns.\n",
      "\n",
      "\n",
      "[T14] Mario stands ready to begin his journey toward the castle where Princess Toadstool is being held.\n",
      "\n",
      "\n",
      "[T15] The ground blocks feature a brick-like texture typical of the Super Mario Bros. aesthetic.\n",
      "\n",
      "\n",
      "[T16] This opening scene establishes the side-scrolling gameplay that would define the platformer genre.\n",
      "\n",
      "\n",
      "[T17] The simplicity of the starting area allows new players to become familiar with the controls.\n",
      "\n",
      "\n",
      "[T18] As Mario moves right, he'll encounter Question Blocks, enemies, and eventually the level-ending flagpole.\n",
      "\n",
      "\n",
      "Processing model: Llama-4-Maverick-17B-128E-Instruct-FP8\n",
      "Result for Llama-4-Maverick-17B-128E-Instruct-FP8:\n",
      "[T1] Mario starts on a green hill with a few black dots, likely representing grass or other terrain features.\n",
      "\n",
      "\n",
      "[T2] The hill is situated on top of a brick platform that stretches across the bottom of the screen.\n",
      "\n",
      "\n",
      "[T3] The brick platform appears to be made up of individual brick blocks, some of which may be breakable.\n",
      "\n",
      "\n",
      "[T4] A small patch of green terrain is visible on the right side of the screen, possibly indicating a separate hill or mound.\n",
      "\n",
      "\n",
      "[T5] The sky above is a clear blue color with a single white cloud floating in the center.\n",
      "\n",
      "\n",
      "[T6] The cloud is positioned directly above the middle of the screen, suggesting it may be a visual landmark.\n",
      "\n",
      "\n",
      "[T7] Mario is facing towards the right side of the screen, indicating the direction of progression.\n",
      "\n",
      "\n",
      "[T8] The presence of a score counter and coin counter at the top of the screen suggests that Mario has not yet collected any coins.\n",
      "\n",
      "\n",
      "[T9] The world number is displayed as \"1-1\", indicating that this is the first level of the game.\n",
      "\n",
      "\n",
      "[T10] The time remaining is shown as \"400\", implying that Mario has 400 seconds to complete the level.\n",
      "\n",
      "\n",
      "[T11] The absence of any visible enemies or obstacles suggests that the initial section of the level is relatively safe.\n",
      "\n",
      "\n",
      "[T12] As Mario moves forward, he will likely encounter the edge of the brick platform and potentially drop down.\n",
      "\n",
      "\n",
      "[T13] The drop down from the brick platform may lead to a new terrain feature or obstacle.\n",
      "\n",
      "\n",
      "[T14] The green hill on the left may be a starting point for a series of platforms or hills.\n",
      "\n",
      "\n",
      "[T15] The presence of a clear path to the right suggests that Mario will continue in that direction.\n",
      "\n",
      "\n",
      "[T16] The level may feature a series of pipes, bricks, or other architectural elements as Mario progresses.\n",
      "\n",
      "\n",
      "[T17] The ultimate goal is likely to reach the flagpole, which is not yet visible on the screen.\n",
      "\n",
      "\n",
      "[T18] The level will likely conclude with Mario reaching the flagpole and descending down to complete the level.\n",
      "\n",
      "\n",
      "Processing model: claude-3-5-sonnet-20241022\n",
      "anthropic vision-text activated... thinking: False\n",
      "Result for claude-3-5-sonnet-20241022:\n",
      "[T1] Mario begins on a brick platform at World 1-1 with 400 seconds on the timer and zero coins collected.\n",
      "\n",
      "\n",
      "[T2] A small green hill rises in the background, dotted with sparse vegetation.\n",
      "\n",
      "\n",
      "[T3] A single white cloud floats peacefully in the bright blue sky above the level.\n",
      "\n",
      "\n",
      "[T4] The ground consists of a continuous line of solid brick blocks forming the main walking platform.\n",
      "\n",
      "\n",
      "[T5] The brick platform extends forward, creating a stable path for Mario to traverse.\n",
      "\n",
      "\n",
      "[T6] Small bushes with rounded tops appear in the far background, adding depth to the scene.\n",
      "\n",
      "\n",
      "[T7] The score counter at the top shows \"000000\", indicating the level has just begun.\n",
      "\n",
      "\n",
      "[T8] Mario's sprite appears in his classic orange and brown color scheme, standing atop the bricks.\n",
      "\n",
      "\n",
      "[T9] The terrain ahead suggests this is the iconic opening sequence of the first Super Mario Bros level.\n",
      "\n",
      "\n",
      "[T10] A coin counter displays \"×00\" in the top status bar, awaiting the first coin collection.\n",
      "\n",
      "\n",
      "[T11] The \"WORLD 1-1\" indicator confirms this is the very first level of the game.\n",
      "\n",
      "\n",
      "[T12] The simple layout ahead provides new players a safe space to learn basic movement controls.\n",
      "\n",
      "\n",
      "[T13] The brick platform continues uniformly, maintaining the same height throughout this section.\n",
      "\n",
      "\n",
      "[T14] Subtle pixelation in the cloud sprite showcases the NES system's classic 8-bit graphics style.\n",
      "\n",
      "\n",
      "[T15] The green hill's simple design establishes the Mushroom Kingdom's characteristic visual style.\n",
      "\n",
      "\n",
      "[T16] The status bar remains clearly visible at the top, displaying all essential game information.\n",
      "\n",
      "\n",
      "[T17] This opening area is intentionally free of enemies to help players acclimate to the controls.\n",
      "\n",
      "\n",
      "[T18] The bright blue sky background creates the cheerful atmosphere Super Mario Bros is famous for.\n",
      "\n",
      "Loaded existing results from super_mario_bros_generated_text.json\n",
      "Using 19 timestamps for level 1-2 based on ground truth\n",
      "You are given ONE RGB frame from a super mario bros level\n",
      "\n",
      "Provide EXACTLY 19 timestamped observations describing the level in chronological order.\n",
      "\n",
      "FORMAT REQUIREMENTS (STRICTLY ENFORCE):\n",
      "- Each observation MUST follow this exact pattern: [T#] One single concise sentence.\n",
      "- Each entry MUST be separated by exactly two line breaks\n",
      "- Each observation MUST be ONLY ONE SENTENCE long (15-25 words maximum)\n",
      "- DO NOT use bullets, numbering, or any special formatting\n",
      "- NO introduction or conclusion text\n",
      "\n",
      "EXAMPLE FORMAT:\n",
      "[T1] This level starts out on a flat, open path.\n",
      "\n",
      "[T2] As Mario moves forward, a Little Goomba appears and starts walking towards him.\n",
      "\n",
      "[T3] Mario then reaches the first Question Block of the game.\n",
      "\n",
      "CONTENT REQUIREMENTS:\n",
      "- [T1] MUST describe Mario's starting position\n",
      "- Number entries sequentially from [T1] to [T19]\n",
      "- Each observation MUST focus on ONE specific element or section\n",
      "- Use exact Super Mario terminology: \"Question Block\", \"Brick Block\", \"Little Goomba\", \"Koopa Troopa\", \"Piranha Plant\", etc.\n",
      "- Include precise descriptions of terrain features, enemies, items, obstacles, and architectural elements\n",
      "- Cover the entire level from start to finish in proper sequence\n",
      "- End with the flagpole and level conclusion\n",
      "\n",
      "REMEMBER: You MUST provide EXACTLY 19 observations, no more and no less.\n",
      "\n",
      "\n",
      "Processing model: o4-mini-2025-04-16\n",
      "Result for o4-mini-2025-04-16:\n",
      "[T1] Mario emerges from a green Warp Pipe at level start, landing on flat underground floor beneath a brick ceiling.\n",
      "\n",
      "  \n",
      "[T2] The cavern ceiling is formed by uniform rows of static Brick Blocks several tiles above Mario’s head.\n",
      "\n",
      "  \n",
      "[T3] Mario runs across the flat blue stone flooring made of repeating rectangular block tiles under dim cavern lighting.\n",
      "\n",
      "  \n",
      "[T4] He approaches a horizontal row of four suspended Question Blocks hovering menacingly over the floor.\n",
      "\n",
      "  \n",
      "[T5] Mario leaps up to hit the second Question Block which releases a shiny floating coin.\n",
      "\n",
      "  \n",
      "[T6] A small Little Goomba scuttles out from the underground darkness and shuffles menacingly towards Mario.\n",
      "\n",
      "  \n",
      "[T7] Mario stomps the Little Goomba quickly to bounce it off-screen and earn extra score points.\n",
      "\n",
      "  \n",
      "[T8] He nears a lone green Warp Pipe topped and sided by Brick Blocks with a Piranha Plant lurking within its opening.\n",
      "\n",
      "  \n",
      "[T9] Mario times his jump carefully to avoid the Piranha Plant’s chomping attack from the Pipe.\n",
      "\n",
      "  \n",
      "[T10] After clearing the Pipe, he encounters a descending staircase of staggered Brick Blocks leading downwards.\n",
      "\n",
      "  \n",
      "[T11] Mario navigates through a narrow tunnel bounded by two opposing Brick Block ledges and low ceiling.\n",
      "\n",
      "  \n",
      "[T12] He then ascends a symmetrical staircase of Brick Blocks rising toward another green Pipe exit.\n",
      "\n",
      "  \n",
      "[T13] Mario jumps into the second green Warp Pipe to briefly travel through an underground passage.\n",
      "\n",
      "  \n",
      "[T14] Mario emerges from the Warp Pipe onto a short above-ground platform with blue stone flooring.\n",
      "\n",
      "  \n",
      "[T15] He sprints past a series of tall ice-blue block pillars arranged like stalagmites under dim lighting.\n",
      "\n",
      "  \n",
      "[T16] Mario leaps across a small gap in the flooring to avoid falling back into the cave.\n",
      "\n",
      "  \n",
      "[T17] Mario approaches a tall green exit Pipe that marks the path to the level conclusion.\n",
      "\n",
      "  \n",
      "[T18] He jumps over the Pipe and sees the flagpole silhouetted against an evening sky beyond.\n",
      "\n",
      "  \n",
      "[T19] Mario slides down the flagpole as fireworks explode and the level clear jingle triumphantly plays.\n",
      "\n",
      "\n",
      "Processing model: o3-2025-04-16\n",
      "Result for o3-2025-04-16:\n",
      "[T1] Mario emerges from the entrance pipe onto a flat blue stone floor beneath a low brick ceiling.\n",
      "\n",
      "[T2] Ahead, two Little Goombas patrol the narrow tunnel, forcing Mario to jump or stomp in the constrained space.\n",
      "\n",
      "[T3] A trio of overhead Question Blocks appears next, with the center block hiding a Super Mushroom that becomes a Fire Flower if already Super.\n",
      "\n",
      "[T4] Mario climbs a staircase of Brick Blocks that leads into a higher tunnel segment lined with floating coins for easy collection.\n",
      "\n",
      "[T5] At the tunnel’s end, a tall green pipe intermittently produces a Piranha Plant, obstructing passage unless Mario times his movement.\n",
      "\n",
      "[T6] Just beyond, pairs of horizontal elevator lifts slide over a bottomless pit, requiring careful jumps between their alternating positions.\n",
      "\n",
      "[T7] On the other side, a low ceiling of Brick Blocks hides coin caches while three Little Goombas march beneath, crowding the pathway.\n",
      "\n",
      "[T8] Mario then reaches ascending Brick Block steps where a hidden 1-Up block appears if he hits the correct invisible spot.\n",
      "\n",
      "[T9] Further along, alternating pipes contain Piranha Plants while a lone green Koopa Troopa patrols the ground between them.\n",
      "\n",
      "[T10] A quartet of mid-air Question Blocks above a gap offers coins and a safe perch if Mario performs a running jump.\n",
      "\n",
      "[T11] One downward-facing pipe afterward leads to an optional underground bonus room brimming with coins against a dark backdrop.\n",
      "\n",
      "[T12] Exiting the bonus room through a return pipe places Mario on a lower platform beside another elevator lift pair.\n",
      "\n",
      "[T13] He soon encounters a vertical shaft where ascending and descending lifts enable reaching the brick ceiling and potential Warp Zone path.\n",
      "\n",
      "[T14] By breaking ceiling Brick Blocks, Mario can run atop the level, bypass enemies, and discover the hidden Warp Zone pipes at the far end.\n",
      "\n",
      "[T15] Following the normal corridor, Mario meets additional Little Goombas and two more Piranha Plant pipes before approaching the exit area.\n",
      "\n",
      "[T16] A final overhead Question Block here conceals a Starman, granting brief invincibility to sprint through remaining obstacles unscathed.\n",
      "\n",
      "[T17] The last upward pipe transports Mario back to the surface, depositing him on a short grass platform before the flagpole.\n",
      "\n",
      "[T18] He ascends a staircase of Brick Blocks, hops over a solitary Little Goomba, and positions for the final leap.\n",
      "\n",
      "[T19] Mario jumps, slides down the flagpole, and walks into the small castle as the level’s victory fanfare plays.\n",
      "\n",
      "\n",
      "Processing model: gemini-2.5-pro-exp-03-25\n",
      "error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      "]\n",
      "Error: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      "]\n",
      "Result for gemini-2.5-pro-exp-03-25:\n",
      "\n",
      "\n",
      "\n",
      "Processing model: claude-3-7-sonnet-20250219\n",
      "anthropic vision-text activated... thinking: False\n",
      "Result for claude-3-7-sonnet-20250219:\n",
      "[T1] Mario stands on a row of five Question Blocks in World 1-2, an underground level.\n",
      "\n",
      "\n",
      "[T2] The player has accumulated 21,700 points and collected 15 coins so far.\n",
      "\n",
      "\n",
      "[T3] The timer shows 371 seconds remaining to complete the level.\n",
      "\n",
      "\n",
      "[T4] A series of teal brick platforms creates a stair-like formation to the right of Mario.\n",
      "\n",
      "\n",
      "[T5] The ceiling and floor of the underground cavern are lined with solid teal bricks.\n",
      "\n",
      "\n",
      "[T6] The Question Blocks Mario stands on likely contain coins or power-ups when hit from below.\n",
      "\n",
      "\n",
      "[T7] The dark background indicates this is one of the subterranean levels in Super Mario Bros.\n",
      "\n",
      "\n",
      "[T8] Mario appears to be in his small form rather than Super Mario or Fire Mario.\n",
      "\n",
      "\n",
      "[T9] The stair-like formation consists of four ascending steps made of solid blocks.\n",
      "\n",
      "\n",
      "[T10] No enemies are visible in this particular frame of the underground level.\n",
      "\n",
      "\n",
      "[T11] The level design features the characteristic blue-green color palette of underground stages.\n",
      "\n",
      "\n",
      "[T12] Mario must navigate across the Question Blocks to reach the platforms on the right.\n",
      "\n",
      "\n",
      "[T13] The gap between Mario's position and the brick stairs requires a precise jump.\n",
      "\n",
      "\n",
      "[T14] This section of World 1-2 demonstrates the classic platforming challenges of Super Mario Bros.\n",
      "\n",
      "\n",
      "[T15] The Question Blocks are arranged in a straight line, providing a safe platform.\n",
      "\n",
      "\n",
      "[T16] The underground setting creates a more confined space compared to overworld levels.\n",
      "\n",
      "\n",
      "[T17] This area appears to be near the beginning portion of World 1-2.\n",
      "\n",
      "\n",
      "[T18] The brick staircase likely leads to another section of the level with additional challenges.\n",
      "\n",
      "\n",
      "[T19] After completing this section, Mario will continue through the underground maze toward the level exit.\n",
      "\n",
      "\n",
      "Processing model: Llama-4-Maverick-17B-128E-Instruct-FP8\n",
      "Result for Llama-4-Maverick-17B-128E-Instruct-FP8:\n",
      "[T1] Mario starts on a platform consisting of five consecutive Question Blocks.\n",
      "\n",
      "\n",
      "[T2] The Question Block platform is situated above a solid ground made of Brick Blocks.\n",
      "\n",
      "\n",
      "[T3] To the right of Mario, there are three stacked Brick Blocks forming a small tower.\n",
      "\n",
      "\n",
      "[T4] Further to the right, there are four stacked Brick Blocks, with the top one being slightly offset.\n",
      "\n",
      "\n",
      "[T5] The ground level is made of Brick Blocks that stretch across the screen.\n",
      "\n",
      "\n",
      "[T6] The Question Block platform Mario is standing on is suspended in mid-air.\n",
      "\n",
      "\n",
      "[T7] There are no visible enemies or obstacles near Mario's starting position.\n",
      "\n",
      "\n",
      "[T8] The sky is not visible, as the top of the screen is occupied by the HUD.\n",
      "\n",
      "\n",
      "[T9] The HUD displays Mario's score, the number of coins collected, the world number, and the time remaining.\n",
      "\n",
      "\n",
      "[T10] The world number is 1-2, indicating that this is the second level of World 1.\n",
      "\n",
      "\n",
      "[T11] Mario has 371 seconds to complete the level.\n",
      "\n",
      "\n",
      "[T12] Mario is standing on the third Question Block from the left.\n",
      "\n",
      "\n",
      "[T13] The Question Blocks are identical and have a blue question mark on them.\n",
      "\n",
      "\n",
      "[T14] There are no visible power-ups or items on the screen.\n",
      "\n",
      "\n",
      "[T15] The Brick Blocks on the ground have a distinctive pattern.\n",
      "\n",
      "\n",
      "[T16] The level appears to be a standard overworld level with no visible water or underground sections.\n",
      "\n",
      "\n",
      "[T17] There are no visible pipes or tunnels on the screen.\n",
      "\n",
      "\n",
      "[T18] The level seems to be progressing from left to right.\n",
      "\n",
      "\n",
      "[T19] The level is expected to continue with more challenges and obstacles as Mario moves forward.\n",
      "\n",
      "\n",
      "Processing model: claude-3-5-sonnet-20241022\n",
      "anthropic vision-text activated... thinking: False\n",
      "Result for claude-3-5-sonnet-20241022:\n",
      "[T1] Mario begins standing atop five Question Blocks in World 1-2, an underground level.\n",
      "\n",
      "\n",
      "[T2] The score counter shows 021700 points while Mario has collected 15 coins.\n",
      "\n",
      "\n",
      "[T3] A row of turquoise blocks forms the ceiling structure throughout this underground passage.\n",
      "\n",
      "\n",
      "[T4] Three ascending block steps appear to the right of Mario's starting position.\n",
      "\n",
      "\n",
      "[T5] The ground is made of solid turquoise blocks with a distinctive geometric pattern.\n",
      "\n",
      "\n",
      "[T6] The Question Blocks Mario stands on contain unknown power-ups or coins waiting to be hit.\n",
      "\n",
      "\n",
      "[T7] The dark background suggests this is one of the subterranean levels in World 1.\n",
      "\n",
      "\n",
      "[T8] The timer displays 371 seconds remaining to complete the underground challenge.\n",
      "\n",
      "\n",
      "[T9] The stepped formation ahead suggests a platforming sequence is required to progress.\n",
      "\n",
      "\n",
      "[T10] Each step in the ascending staircase is one block higher than the previous.\n",
      "\n",
      "\n",
      "[T11] The underground setting creates a claustrophobic atmosphere compared to overworld levels.\n",
      "\n",
      "\n",
      "[T12] The turquoise blocks used throughout give this level its signature cave-like appearance.\n",
      "\n",
      "\n",
      "[T13] Mario must carefully time his jumps to traverse the stepped platform sequence.\n",
      "\n",
      "\n",
      "[T14] The Question Blocks' golden color stands out against the dark underground background.\n",
      "\n",
      "\n",
      "[T15] The level's architecture forces players to master precise jumping mechanics.\n",
      "\n",
      "\n",
      "[T16] The geometric pattern on the ground blocks creates a distinct visual texture.\n",
      "\n",
      "\n",
      "[T17] This underground section requires different strategies than surface-level stages.\n",
      "\n",
      "\n",
      "[T18] The confined space demands careful movement to avoid hitting the ceiling.\n",
      "\n",
      "\n",
      "[T19] Successfully navigating these platforms leads to the next section of World 1-2.\n",
      "\n",
      "\n",
      "Analysis complete!\n",
      "Results saved to super_mario_bros_generated_text.json\n"
     ]
    }
   ],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Encode an image file to base64.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def get_num_timestamps(ground_truth_file, level_id):\n",
    "    \"\"\"\n",
    "    Parse the ground truth file to determine the number of timestamps for a specific level.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(ground_truth_file, 'r') as f:\n",
    "            ground_truth = json.load(f)\n",
    "            \n",
    "        if level_id in ground_truth:\n",
    "            # Use regex to find all timestamp patterns [T1], [T2], etc.\n",
    "            timestamps = re.findall(r'\\[T\\d+\\]', ground_truth[level_id])\n",
    "            return len(timestamps)\n",
    "        else:\n",
    "            # Default fallback if level not found\n",
    "            print(f\"Warning: Level {level_id} not found in ground truth. Using default values.\")\n",
    "            return 18 if level_id == \"1-1\" else 19\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing ground truth file: {e}\")\n",
    "        # Default fallback in case of errors\n",
    "        return 18 if level_id == \"1-1\" else 19\n",
    "\n",
    "def analyze_single_image(image_path, level_id=\"1-1\", ground_truth_file=\"super_mario_bros_ground_truth.json\"):\n",
    "    \"\"\"Analyze a single Mario screenshot using all models.\"\"\"\n",
    "    # Create output dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Result file\n",
    "    result_file = \"super_mario_bros_generated_text.json\"\n",
    "    \n",
    "    # Load existing results if file exists\n",
    "    try:\n",
    "        with open(result_file, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"Loaded existing results from {result_file}\")\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        results = {}\n",
    "    \n",
    "    # System prompt\n",
    "    system_prompt = \"You are an expert at identifying NES game content.\"\n",
    "    \n",
    "    # Get number of timestamps from ground truth\n",
    "    num_timestamps = get_num_timestamps(ground_truth_file, level_id)\n",
    "    print(f\"Using {num_timestamps} timestamps for level {level_id} based on ground truth\")\n",
    "    \n",
    "    # Fill placeholders in the prompt\n",
    "    current_prompt = PROMPT.format(num_timestamps=num_timestamps)\n",
    "\n",
    "    print(current_prompt)\n",
    "    \n",
    "    # Encode the image once\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    # Process each model\n",
    "    for model_name in MODEL_NAMES:\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        \n",
    "\n",
    "\n",
    "        # Initialize model's results if not already present\n",
    "        if model_name not in results:\n",
    "            results[model_name] = {}\n",
    "\n",
    "        if model_name in results and level_id in results[model_name]:\n",
    "            print(f\"\\nSkipping model: {model_name} (already in results)\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Call the appropriate API based on model type\n",
    "            if \"o1-\" in model_name or \"o3-\" in model_name or \"o4-\" in model_name or \"gpt-\" in model_name:\n",
    "                response = openai_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=model_name,\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=current_prompt,\n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "            elif \"claude\" in model_name:\n",
    "                response = anthropic_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=model_name,\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=current_prompt,\n",
    "                    thinking=False\n",
    "                )\n",
    "                \n",
    "            elif \"gemini\" in model_name:\n",
    "                response = gemini_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=model_name,\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=current_prompt\n",
    "                )\n",
    "                \n",
    "            elif \"llama-4-maverick\" in model_name.lower():\n",
    "                response = together_ai_completion(\n",
    "                    system_prompt=system_prompt,\n",
    "                    model_name=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "                    base64_image=base64_image,\n",
    "                    prompt=current_prompt\n",
    "                )\n",
    "                \n",
    "            elif \"grok\" in model_name.lower():\n",
    "                # Note: grok may not support images directly\n",
    "                print(f\"Skipping {model_name} - image support not available\")\n",
    "                continue\n",
    "            \n",
    "            # Store the result\n",
    "            results[model_name][level_id] = response\n",
    "            print(f\"Result for {model_name}:\\n{response}\\n\")\n",
    "            \n",
    "            # Save after each successful generation\n",
    "            with open(result_file, \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model_name}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your Mario screenshot\n",
    "    image_path_lst = [\"super_mario_bros_1-1_screenshot.png\" , \"super_mario_bros_1-2_screenshot.png\"] # Replace with your actual image path\n",
    "    \n",
    "    # Level ID (derived from filename or specified manually)\n",
    "    level_id = [\"1-1\" , \"1-2\"]\n",
    "\n",
    "    for img_path, level_id in zip(image_path_lst, level_id):\n",
    "        results = analyze_single_image(img_path, level_id)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"Results saved to super_mario_bros_generated_text.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth cross-level similarity (1-1 to 1-2): 0.8448\n",
      "o4-mini-2025-04-16 1-1 vs GT 1-1: 0.7084\n",
      "o4-mini-2025-04-16 1-2 vs GT 1-2: 0.7254\n",
      "o4-mini-2025-04-16 cross-similarity (1-1 to 1-2): 0.8527\n",
      "o4-mini-2025-04-16 1-1 vs GT 1-2: 0.7076\n",
      "o4-mini-2025-04-16 1-2 vs GT 1-1: 0.7991\n",
      "o3-2025-04-16 1-1 vs GT 1-1: 0.7531\n",
      "o3-2025-04-16 1-2 vs GT 1-2: 0.8134\n",
      "o3-2025-04-16 cross-similarity (1-1 to 1-2): 0.8830\n",
      "o3-2025-04-16 1-1 vs GT 1-2: 0.7490\n",
      "o3-2025-04-16 1-2 vs GT 1-1: 0.8010\n",
      "Skipping gemini-2.5-pro-exp-03-25 due to missing data\n",
      "claude-3-7-sonnet-20250219 1-1 vs GT 1-1: 0.6792\n",
      "claude-3-7-sonnet-20250219 1-2 vs GT 1-2: 0.6280\n",
      "claude-3-7-sonnet-20250219 cross-similarity (1-1 to 1-2): 0.8211\n",
      "claude-3-7-sonnet-20250219 1-1 vs GT 1-2: 0.5601\n",
      "claude-3-7-sonnet-20250219 1-2 vs GT 1-1: 0.7505\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-1 vs GT 1-1: 0.6436\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-2 vs GT 1-2: 0.5903\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 cross-similarity (1-1 to 1-2): 0.8461\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-1 vs GT 1-2: 0.5427\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-2 vs GT 1-1: 0.6515\n",
      "claude-3-5-sonnet-20241022 1-1 vs GT 1-1: 0.6870\n",
      "claude-3-5-sonnet-20241022 1-2 vs GT 1-2: 0.6044\n",
      "claude-3-5-sonnet-20241022 cross-similarity (1-1 to 1-2): 0.7959\n",
      "claude-3-5-sonnet-20241022 1-1 vs GT 1-2: 0.5870\n",
      "claude-3-5-sonnet-20241022 1-2 vs GT 1-1: 0.7011\n",
      "\n",
      "Timestamped Format Similarity Scores:\n",
      "                                    Model       1-1       1-2  cross_1-1_1-2  \\\n",
      "1                           o3-2025-04-16  0.753076  0.813358       0.882990   \n",
      "0                      o4-mini-2025-04-16  0.708403  0.725378       0.852682   \n",
      "3              claude-3-7-sonnet-20250219  0.679185  0.627961       0.821135   \n",
      "5              claude-3-5-sonnet-20241022  0.686953  0.604426       0.795933   \n",
      "4  Llama-4-Maverick-17B-128E-Instruct-FP8  0.643627  0.590322       0.846092   \n",
      "2                gemini-2.5-pro-exp-03-25       NaN       NaN            NaN   \n",
      "6                            ground_truth       NaN       NaN       0.844817   \n",
      "\n",
      "   cross_model1-1_gt1-2  cross_model1-2_gt1-1  cross_gt1-1_gt1-2       sum  \n",
      "1              0.749037              0.800998                NaN  1.566435  \n",
      "0              0.707597              0.799113                NaN  1.433780  \n",
      "3              0.560105              0.750460                NaN  1.307146  \n",
      "5              0.587022              0.701139                NaN  1.291379  \n",
      "4              0.542652              0.651531                NaN  1.233949  \n",
      "2                   NaN                   NaN                NaN  0.000000  \n",
      "6                   NaN                   NaN           0.844817  0.000000  \n",
      "Results saved to mario_timestamped_similarity.csv\n",
      "Visualizations saved as mario_timestamped_heatmap.png and mario_timestamped_similarity.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def compute_similarity(generated_file, ground_truth_file):\n",
    "    \"\"\"\n",
    "    Compute similarity between generated texts and ground truth using BERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        generated_file: Path to the JSON file with generated texts\n",
    "        ground_truth_file: Path to the JSON file with ground truth\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with similarity scores\n",
    "    \"\"\"\n",
    "    # Load generated texts\n",
    "    with open(generated_file, 'r') as f:\n",
    "        generated_texts = json.load(f)\n",
    "    \n",
    "    # Load ground truth\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    # Initialize BERT model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Pre-compute ground truth embeddings\n",
    "    gt_embeddings = {}\n",
    "    for level_id, text in ground_truth.items():\n",
    "        gt_embeddings[level_id] = model.encode(text, show_progress_bar=False)\n",
    "    \n",
    "    # Calculate cross-similarity between ground truth 1-1 and 1-2\n",
    "    gt_cross_sim = cosine_similarity([gt_embeddings[\"1-1\"]], [gt_embeddings[\"1-2\"]])[0][0]\n",
    "    print(f\"Ground Truth cross-level similarity (1-1 to 1-2): {gt_cross_sim:.4f}\")\n",
    "    \n",
    "    # Create a dictionary to store similarity scores\n",
    "    similarity_scores = {}\n",
    "    \n",
    "    # For each model in the generated texts\n",
    "    for model_name, level_data in generated_texts.items():\n",
    "        similarity_scores[model_name] = {}\n",
    "        \n",
    "        # Skip models with empty data\n",
    "        if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "            print(f\"Skipping {model_name} due to missing data\")\n",
    "            continue\n",
    "            \n",
    "        # Get embeddings for model outputs\n",
    "        model_embeddings = {}\n",
    "        for level_id, text in level_data.items():\n",
    "            if text:  # Skip empty texts\n",
    "                model_embeddings[level_id] = model.encode(text, show_progress_bar=False)\n",
    "        \n",
    "        # Calculate similarity to corresponding ground truth\n",
    "        for level_id, embedding in model_embeddings.items():\n",
    "            if level_id in gt_embeddings:\n",
    "                sim_score = cosine_similarity([embedding], [gt_embeddings[level_id]])[0][0]\n",
    "                similarity_scores[model_name][level_id] = sim_score\n",
    "                print(f\"{model_name} {level_id} vs GT {level_id}: {sim_score:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity between model outputs\n",
    "        if \"1-1\" in model_embeddings and \"1-2\" in model_embeddings:\n",
    "            model_cross_sim = cosine_similarity([model_embeddings[\"1-1\"]], [model_embeddings[\"1-2\"]])[0][0]\n",
    "            similarity_scores[model_name][\"cross_1-1_1-2\"] = model_cross_sim\n",
    "            print(f\"{model_name} cross-similarity (1-1 to 1-2): {model_cross_sim:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity: model 1-1 to GT 1-2\n",
    "        if \"1-1\" in model_embeddings:\n",
    "            cross_sim_1 = cosine_similarity([model_embeddings[\"1-1\"]], [gt_embeddings[\"1-2\"]])[0][0]\n",
    "            similarity_scores[model_name][\"cross_model1-1_gt1-2\"] = cross_sim_1\n",
    "            print(f\"{model_name} 1-1 vs GT 1-2: {cross_sim_1:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity: model 1-2 to GT 1-1\n",
    "        if \"1-2\" in model_embeddings:\n",
    "            cross_sim_2 = cosine_similarity([model_embeddings[\"1-2\"]], [gt_embeddings[\"1-1\"]])[0][0]\n",
    "            similarity_scores[model_name][\"cross_model1-2_gt1-1\"] = cross_sim_2\n",
    "            print(f\"{model_name} 1-2 vs GT 1-1: {cross_sim_2:.4f}\")\n",
    "    \n",
    "    # Add ground truth cross-similarity for reference\n",
    "    similarity_scores[\"ground_truth\"] = {\n",
    "        \"cross_1-1_1-2\": gt_cross_sim,\n",
    "        \"cross_gt1-1_gt1-2\": gt_cross_sim\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    data = []\n",
    "    for model, scores in similarity_scores.items():\n",
    "        row_data = {'Model': model}\n",
    "        row_data.update(scores)\n",
    "        data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add row with sum of similarity scores per model\n",
    "    if '1-1' in df.columns and '1-2' in df.columns:\n",
    "        # Only sum the direct comparison scores for levels 1-1 and 1-2\n",
    "        df['sum'] = df[['1-1', '1-2']].sum(axis=1, skipna=True)\n",
    "        \n",
    "    # Sort by the sum column if it exists\n",
    "    if 'sum' in df.columns:\n",
    "        df = df.sort_values('sum', ascending=False)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def visualize_results(df):\n",
    "    \"\"\"\n",
    "    Create visualizations for the similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with similarity scores\n",
    "        \n",
    "    Returns:\n",
    "        None (displays plots)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set the style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Create a heatmap (exclude the sum column)\n",
    "    heat_df = df.drop(columns=['Model', 'sum'], errors='ignore')\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(heat_df, annot=True, cmap='viridis', fmt='.3f')\n",
    "    plt.title('Similarity Scores')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mario_timestamped_heatmap.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a bar chart for sum similarity\n",
    "    if 'sum' in df.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.bar(df['Model'], df['sum'])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Sum of Similarity Scores')\n",
    "        plt.title('Sum of Similarity Scores by Model')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('mario_timestamped_similarity.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    generated_file = \"super_mario_bros_generated_text.json\"\n",
    "    ground_truth_file = \"super_mario_bros_ground_truth.json\"\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity_df = compute_similarity(generated_file, ground_truth_file)\n",
    "    \n",
    "    # Display the similarity table\n",
    "    print(\"\\nTimestamped Format Similarity Scores:\")\n",
    "    print(similarity_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    similarity_df.to_csv(\"mario_timestamped_similarity.csv\")\n",
    "    print(\"Results saved to mario_timestamped_similarity.csv\")\n",
    "    \n",
    "    # Visualize the results\n",
    "    visualize_results(similarity_df)\n",
    "    print(\"Visualizations saved as mario_timestamped_heatmap.png and mario_timestamped_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>1-1</th>\n",
       "      <th>1-2</th>\n",
       "      <th>cross_1-1_1-2</th>\n",
       "      <th>cross_model1-1_gt1-2</th>\n",
       "      <th>cross_model1-2_gt1-1</th>\n",
       "      <th>cross_gt1-1_gt1-2</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>0.753076</td>\n",
       "      <td>0.813358</td>\n",
       "      <td>0.882990</td>\n",
       "      <td>0.749037</td>\n",
       "      <td>0.800998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.566435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o4-mini-2025-04-16</td>\n",
       "      <td>0.708403</td>\n",
       "      <td>0.725378</td>\n",
       "      <td>0.852682</td>\n",
       "      <td>0.707597</td>\n",
       "      <td>0.799113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.433780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>0.679185</td>\n",
       "      <td>0.627961</td>\n",
       "      <td>0.821135</td>\n",
       "      <td>0.560105</td>\n",
       "      <td>0.750460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.307146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>0.686953</td>\n",
       "      <td>0.604426</td>\n",
       "      <td>0.795933</td>\n",
       "      <td>0.587022</td>\n",
       "      <td>0.701139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.291379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-4-Maverick-17B-128E-Instruct-FP8</td>\n",
       "      <td>0.643627</td>\n",
       "      <td>0.590322</td>\n",
       "      <td>0.846092</td>\n",
       "      <td>0.542652</td>\n",
       "      <td>0.651531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.233949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ground_truth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844817</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model       1-1       1-2  cross_1-1_1-2  \\\n",
       "1                           o3-2025-04-16  0.753076  0.813358       0.882990   \n",
       "0                      o4-mini-2025-04-16  0.708403  0.725378       0.852682   \n",
       "3              claude-3-7-sonnet-20250219  0.679185  0.627961       0.821135   \n",
       "5              claude-3-5-sonnet-20241022  0.686953  0.604426       0.795933   \n",
       "4  Llama-4-Maverick-17B-128E-Instruct-FP8  0.643627  0.590322       0.846092   \n",
       "2                gemini-2.5-pro-exp-03-25       NaN       NaN            NaN   \n",
       "6                            ground_truth       NaN       NaN       0.844817   \n",
       "\n",
       "   cross_model1-1_gt1-2  cross_model1-2_gt1-1  cross_gt1-1_gt1-2       sum  \n",
       "1              0.749037              0.800998                NaN  1.566435  \n",
       "0              0.707597              0.799113                NaN  1.433780  \n",
       "3              0.560105              0.750460                NaN  1.307146  \n",
       "5              0.587022              0.701139                NaN  1.291379  \n",
       "4              0.542652              0.651531                NaN  1.233949  \n",
       "2                   NaN                   NaN                NaN  0.000000  \n",
       "6                   NaN                   NaN           0.844817  0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth cross-level similarity (1-1 to 1-2): 0.9318\n",
      "o4-mini-2025-04-16 1-1 vs GT 1-1: 0.9769\n",
      "o4-mini-2025-04-16 1-2 vs GT 1-2: 0.9692\n",
      "o4-mini-2025-04-16 cross-similarity (1-1 to 1-2): 0.9259\n",
      "o4-mini-2025-04-16 1-1 vs GT 1-2: 0.9161\n",
      "o4-mini-2025-04-16 1-2 vs GT 1-1: 0.9185\n",
      "o3-2025-04-16 1-1 vs GT 1-1: 0.9860\n",
      "o3-2025-04-16 1-2 vs GT 1-2: 0.9808\n",
      "o3-2025-04-16 cross-similarity (1-1 to 1-2): 0.9341\n",
      "o3-2025-04-16 1-1 vs GT 1-2: 0.9212\n",
      "o3-2025-04-16 1-2 vs GT 1-1: 0.9246\n",
      "Skipping gemini-2.5-pro-exp-03-25 due to missing data\n",
      "claude-3-7-sonnet-20250219 1-1 vs GT 1-1: 0.9799\n",
      "claude-3-7-sonnet-20250219 1-2 vs GT 1-2: 0.9845\n",
      "claude-3-7-sonnet-20250219 cross-similarity (1-1 to 1-2): 0.9243\n",
      "claude-3-7-sonnet-20250219 1-1 vs GT 1-2: 0.9159\n",
      "claude-3-7-sonnet-20250219 1-2 vs GT 1-1: 0.9291\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-1 vs GT 1-1: 0.9797\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-2 vs GT 1-2: 0.9803\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 cross-similarity (1-1 to 1-2): 0.9304\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-1 vs GT 1-2: 0.9118\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-2 vs GT 1-1: 0.9373\n",
      "claude-3-5-sonnet-20241022 1-1 vs GT 1-1: 0.9808\n",
      "claude-3-5-sonnet-20241022 1-2 vs GT 1-2: 0.9808\n",
      "claude-3-5-sonnet-20241022 cross-similarity (1-1 to 1-2): 0.9155\n",
      "claude-3-5-sonnet-20241022 1-1 vs GT 1-2: 0.9115\n",
      "claude-3-5-sonnet-20241022 1-2 vs GT 1-1: 0.9285\n",
      "\n",
      "CLIP Multimodal Similarity Scores:\n",
      "                                    Model       1-1       1-2  cross_1-1_1-2  \\\n",
      "1                           o3-2025-04-16  0.986031  0.980797       0.934107   \n",
      "3              claude-3-7-sonnet-20250219  0.979858  0.984483       0.924314   \n",
      "5              claude-3-5-sonnet-20241022  0.980797  0.980784       0.915502   \n",
      "4  Llama-4-Maverick-17B-128E-Instruct-FP8  0.979699  0.980348       0.930417   \n",
      "0                      o4-mini-2025-04-16  0.976879  0.969245       0.925878   \n",
      "2                gemini-2.5-pro-exp-03-25       NaN       NaN            NaN   \n",
      "6                            ground_truth       NaN       NaN       0.931819   \n",
      "\n",
      "   cross_model1-1_gt1-2  cross_model1-2_gt1-1  cross_gt1-1_gt1-2       sum  \n",
      "1              0.921240              0.924551                NaN  1.966828  \n",
      "3              0.915868              0.929115                NaN  1.964341  \n",
      "5              0.911508              0.928487                NaN  1.961581  \n",
      "4              0.911844              0.937271                NaN  1.960047  \n",
      "0              0.916054              0.918542                NaN  1.946123  \n",
      "2                   NaN                   NaN                NaN  0.000000  \n",
      "6                   NaN                   NaN           0.931819  0.000000  \n",
      "Results saved to mario_clip_similarity.csv\n",
      "Visualizations saved as mario_clip_heatmap.png and mario_clip_similarity.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "import re\n",
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def compute_clip_similarity(generated_file, ground_truth_file, level_1_1_image_path, level_1_2_image_path):\n",
    "    \"\"\"\n",
    "    Compute similarity between generated texts and ground truth using CLIP embeddings\n",
    "    that combine text and image information at each timestamp.\n",
    "    \n",
    "    Args:\n",
    "        generated_file: Path to the JSON file with generated texts\n",
    "        ground_truth_file: Path to the JSON file with ground truth\n",
    "        level_1_1_image_path: Path to the image for level 1-1\n",
    "        level_1_2_image_path: Path to the image for level 1-2\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with similarity scores\n",
    "    \"\"\"\n",
    "    # Load generated texts\n",
    "    with open(generated_file, 'r') as f:\n",
    "        generated_texts = json.load(f)\n",
    "    \n",
    "    # Load ground truth\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    # Initialize CLIP model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "    # Load and preprocess level images\n",
    "    level_images = {}\n",
    "    if os.path.exists(level_1_1_image_path):\n",
    "        level_images[\"1-1\"] = preprocess(Image.open(level_1_1_image_path)).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        print(f\"Warning: Image not found for level 1-1 at {level_1_1_image_path}\")\n",
    "    \n",
    "    if os.path.exists(level_1_2_image_path):\n",
    "        level_images[\"1-2\"] = preprocess(Image.open(level_1_2_image_path)).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        print(f\"Warning: Image not found for level 1-2 at {level_1_2_image_path}\")\n",
    "    \n",
    "    # Helper function to extract timestamps and their text\n",
    "    def extract_timestamps(text):\n",
    "        # Extract all timestamp sections like [T1], [T2], etc.\n",
    "        timestamps = re.findall(r'\\[T\\d+\\](.*?)(?=\\[T\\d+\\]|\\Z)', text, re.DOTALL)\n",
    "        # Clean up\n",
    "        timestamps = [t.strip() for t in timestamps]\n",
    "        return timestamps\n",
    "    \n",
    "    # Helper function to get CLIP embeddings for text and image\n",
    "    def get_clip_embedding(text, level_id):\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize([text]).to(device)\n",
    "            text_embedding = model.encode_text(text_tokens)\n",
    "            text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            if level_id in level_images:\n",
    "                image_embedding = model.encode_image(level_images[level_id])\n",
    "                image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)\n",
    "                # Average text and image embeddings\n",
    "                combined_embedding = (text_embedding + image_embedding) / 2\n",
    "                # combined_embedding = text_embedding\n",
    "                return combined_embedding.cpu().numpy()[0]\n",
    "            else:\n",
    "                return text_embedding.cpu().numpy()[0]\n",
    "    \n",
    "    # Process ground truth and get embeddings\n",
    "    gt_embeddings = {}\n",
    "    for level_id, text in ground_truth.items():\n",
    "        timestamp_texts = extract_timestamps(text)\n",
    "        timestamp_embeddings = []\n",
    "        \n",
    "        for timestamp_text in timestamp_texts:\n",
    "            embedding = get_clip_embedding(timestamp_text, level_id)\n",
    "            timestamp_embeddings.append(embedding)\n",
    "        \n",
    "        # Average all timestamp embeddings for this level\n",
    "        if timestamp_embeddings:\n",
    "            gt_embeddings[level_id] = np.mean(timestamp_embeddings, axis=0)\n",
    "    \n",
    "    # Calculate cross-similarity between ground truth 1-1 and 1-2\n",
    "    gt_cross_sim = -1\n",
    "    if \"1-1\" in gt_embeddings and \"1-2\" in gt_embeddings:\n",
    "        gt_cross_sim = cosine_similarity([gt_embeddings[\"1-1\"]], [gt_embeddings[\"1-2\"]])[0][0]\n",
    "        print(f\"Ground Truth cross-level similarity (1-1 to 1-2): {gt_cross_sim:.4f}\")\n",
    "    \n",
    "    # Create a dictionary to store similarity scores\n",
    "    similarity_scores = {}\n",
    "    \n",
    "    # For each model in the generated texts\n",
    "    for model_name, level_data in generated_texts.items():\n",
    "        similarity_scores[model_name] = {}\n",
    "        \n",
    "        # Skip models with empty data\n",
    "        if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "            print(f\"Skipping {model_name} due to missing data\")\n",
    "            continue\n",
    "            \n",
    "        # Get embeddings for model outputs\n",
    "        model_embeddings = {}\n",
    "        for level_id, text in level_data.items():\n",
    "            if text:  # Skip empty texts\n",
    "                timestamp_texts = extract_timestamps(text)\n",
    "                timestamp_embeddings = []\n",
    "                \n",
    "                for timestamp_text in timestamp_texts:\n",
    "                    embedding = get_clip_embedding(timestamp_text, level_id)\n",
    "                    timestamp_embeddings.append(embedding)\n",
    "                \n",
    "                # Average all timestamp embeddings for this level\n",
    "                if timestamp_embeddings:\n",
    "                    model_embeddings[level_id] = np.mean(timestamp_embeddings, axis=0)\n",
    "        \n",
    "        # Calculate similarity to corresponding ground truth\n",
    "        for level_id, embedding in model_embeddings.items():\n",
    "            if level_id in gt_embeddings:\n",
    "                sim_score = cosine_similarity([embedding], [gt_embeddings[level_id]])[0][0]\n",
    "                similarity_scores[model_name][level_id] = sim_score\n",
    "                print(f\"{model_name} {level_id} vs GT {level_id}: {sim_score:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity between model outputs\n",
    "        if \"1-1\" in model_embeddings and \"1-2\" in model_embeddings:\n",
    "            model_cross_sim = cosine_similarity([model_embeddings[\"1-1\"]], [model_embeddings[\"1-2\"]])[0][0]\n",
    "            similarity_scores[model_name][\"cross_1-1_1-2\"] = model_cross_sim\n",
    "            print(f\"{model_name} cross-similarity (1-1 to 1-2): {model_cross_sim:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity: model 1-1 to GT 1-2\n",
    "        if \"1-1\" in model_embeddings and \"1-2\" in gt_embeddings:\n",
    "            cross_sim_1 = cosine_similarity([model_embeddings[\"1-1\"]], [gt_embeddings[\"1-2\"]])[0][0]\n",
    "            similarity_scores[model_name][\"cross_model1-1_gt1-2\"] = cross_sim_1\n",
    "            print(f\"{model_name} 1-1 vs GT 1-2: {cross_sim_1:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity: model 1-2 to GT 1-1\n",
    "        if \"1-2\" in model_embeddings and \"1-1\" in gt_embeddings:\n",
    "            cross_sim_2 = cosine_similarity([model_embeddings[\"1-2\"]], [gt_embeddings[\"1-1\"]])[0][0]\n",
    "            similarity_scores[model_name][\"cross_model1-2_gt1-1\"] = cross_sim_2\n",
    "            print(f\"{model_name} 1-2 vs GT 1-1: {cross_sim_2:.4f}\")\n",
    "    \n",
    "    # Add ground truth cross-similarity for reference\n",
    "    similarity_scores[\"ground_truth\"] = {\n",
    "        \"cross_1-1_1-2\": gt_cross_sim,\n",
    "        \"cross_gt1-1_gt1-2\": gt_cross_sim\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    data = []\n",
    "    for model, scores in similarity_scores.items():\n",
    "        row_data = {'Model': model}\n",
    "        row_data.update(scores)\n",
    "        data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add row with sum of similarity scores per model\n",
    "    if '1-1' in df.columns and '1-2' in df.columns:\n",
    "        # Only sum the direct comparison scores for levels 1-1 and 1-2\n",
    "        df['sum'] = df[['1-1', '1-2']].sum(axis=1, skipna=True)\n",
    "        \n",
    "    # Sort by the sum column if it exists\n",
    "    if 'sum' in df.columns:\n",
    "        df = df.sort_values('sum', ascending=False)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Modify the main function to use CLIP similarity\n",
    "if __name__ == \"__main__\":\n",
    "    generated_file = \"super_mario_bros_generated_text.json\"\n",
    "    ground_truth_file = \"super_mario_bros_ground_truth.json\"\n",
    "    level_1_1_image_path = \"super_mario_bros_1-1_screenshot.png\"\n",
    "    level_1_2_image_path = \"super_mario_bros_1-2_screenshot.png\"\n",
    "    \n",
    "    # Compute CLIP similarity\n",
    "    similarity_df = compute_clip_similarity(\n",
    "        generated_file, \n",
    "        ground_truth_file, \n",
    "        level_1_1_image_path, \n",
    "        level_1_2_image_path\n",
    "    )\n",
    "    \n",
    "    # Display the similarity table\n",
    "    print(\"\\nCLIP Multimodal Similarity Scores:\")\n",
    "    print(similarity_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    similarity_df.to_csv(\"mario_clip_similarity.csv\")\n",
    "    print(\"Results saved to mario_clip_similarity.csv\")\n",
    "    \n",
    "    # Visualize the results (using the same function from the original code)\n",
    "    visualize_results(similarity_df)\n",
    "    print(\"Visualizations saved as mario_clip_heatmap.png and mario_clip_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>1-1</th>\n",
       "      <th>1-2</th>\n",
       "      <th>cross_1-1_1-2</th>\n",
       "      <th>cross_model1-1_gt1-2</th>\n",
       "      <th>cross_model1-2_gt1-1</th>\n",
       "      <th>cross_gt1-1_gt1-2</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>0.986031</td>\n",
       "      <td>0.980797</td>\n",
       "      <td>0.934107</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>0.924551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.966828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>0.979858</td>\n",
       "      <td>0.984483</td>\n",
       "      <td>0.924314</td>\n",
       "      <td>0.915868</td>\n",
       "      <td>0.929115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.964341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>0.980797</td>\n",
       "      <td>0.980784</td>\n",
       "      <td>0.915502</td>\n",
       "      <td>0.911508</td>\n",
       "      <td>0.928487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.961581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-4-Maverick-17B-128E-Instruct-FP8</td>\n",
       "      <td>0.979699</td>\n",
       "      <td>0.980348</td>\n",
       "      <td>0.930417</td>\n",
       "      <td>0.911844</td>\n",
       "      <td>0.937271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.960047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o4-mini-2025-04-16</td>\n",
       "      <td>0.976879</td>\n",
       "      <td>0.969245</td>\n",
       "      <td>0.925878</td>\n",
       "      <td>0.916054</td>\n",
       "      <td>0.918542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.946123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ground_truth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.931819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.931819</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model       1-1       1-2  cross_1-1_1-2  \\\n",
       "1                           o3-2025-04-16  0.986031  0.980797       0.934107   \n",
       "3              claude-3-7-sonnet-20250219  0.979858  0.984483       0.924314   \n",
       "5              claude-3-5-sonnet-20241022  0.980797  0.980784       0.915502   \n",
       "4  Llama-4-Maverick-17B-128E-Instruct-FP8  0.979699  0.980348       0.930417   \n",
       "0                      o4-mini-2025-04-16  0.976879  0.969245       0.925878   \n",
       "2                gemini-2.5-pro-exp-03-25       NaN       NaN            NaN   \n",
       "6                            ground_truth       NaN       NaN       0.931819   \n",
       "\n",
       "   cross_model1-1_gt1-2  cross_model1-2_gt1-1  cross_gt1-1_gt1-2       sum  \n",
       "1              0.921240              0.924551                NaN  1.966828  \n",
       "3              0.915868              0.929115                NaN  1.964341  \n",
       "5              0.911508              0.928487                NaN  1.961581  \n",
       "4              0.911844              0.937271                NaN  1.960047  \n",
       "0              0.916054              0.918542                NaN  1.946123  \n",
       "2                   NaN                   NaN                NaN  0.000000  \n",
       "6                   NaN                   NaN           0.931819  0.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth cross-level similarity (1-1 to 1-2): 0.9217\n",
      "o4-mini-2025-04-16 1-1 vs GT 1-1: 0.2956 (averaged across 18 timestamps)\n",
      "o4-mini-2025-04-16 1-2 vs GT 1-2: 0.3088 (averaged across 19 timestamps)\n",
      "o4-mini-2025-04-16 cross-similarity (1-1 to 1-2): 0.4350\n",
      "o4-mini-2025-04-16 1-1 vs GT 1-2: 0.3390\n",
      "o4-mini-2025-04-16 1-2 vs GT 1-1: 0.3086\n",
      "o3-2025-04-16 1-1 vs GT 1-1: 0.3889 (averaged across 18 timestamps)\n",
      "o3-2025-04-16 1-2 vs GT 1-2: 0.3761 (averaged across 19 timestamps)\n",
      "o3-2025-04-16 cross-similarity (1-1 to 1-2): 0.4573\n",
      "o3-2025-04-16 1-1 vs GT 1-2: 0.3389\n",
      "o3-2025-04-16 1-2 vs GT 1-1: 0.3601\n",
      "Skipping gemini-2.5-pro-exp-03-25 due to missing data\n",
      "claude-3-7-sonnet-20250219 1-1 vs GT 1-1: 0.2144 (averaged across 18 timestamps)\n",
      "claude-3-7-sonnet-20250219 1-2 vs GT 1-2: 0.2628 (averaged across 19 timestamps)\n",
      "claude-3-7-sonnet-20250219 cross-similarity (1-1 to 1-2): 0.3323\n",
      "claude-3-7-sonnet-20250219 1-1 vs GT 1-2: 0.2337\n",
      "claude-3-7-sonnet-20250219 1-2 vs GT 1-1: 0.2837\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-1 vs GT 1-1: 0.2407 (averaged across 18 timestamps)\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-2 vs GT 1-2: 0.2549 (averaged across 19 timestamps)\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 cross-similarity (1-1 to 1-2): 0.3446\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-1 vs GT 1-2: 0.2137\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8 1-2 vs GT 1-1: 0.2549\n",
      "claude-3-5-sonnet-20241022 1-1 vs GT 1-1: 0.1900 (averaged across 18 timestamps)\n",
      "claude-3-5-sonnet-20241022 1-2 vs GT 1-2: 0.2642 (averaged across 19 timestamps)\n",
      "claude-3-5-sonnet-20241022 cross-similarity (1-1 to 1-2): 0.2339\n",
      "claude-3-5-sonnet-20241022 1-1 vs GT 1-2: 0.2241\n",
      "claude-3-5-sonnet-20241022 1-2 vs GT 1-1: 0.2530\n",
      "\n",
      "BERT Per-Timestamp Similarity Scores:\n",
      "                                    Model       1-1       1-2  cross_1-1_1-2  \\\n",
      "1                           o3-2025-04-16  0.388851  0.376146       0.457320   \n",
      "0                      o4-mini-2025-04-16  0.295597  0.308760       0.434997   \n",
      "4  Llama-4-Maverick-17B-128E-Instruct-FP8  0.240731  0.254900       0.344591   \n",
      "3              claude-3-7-sonnet-20250219  0.214365  0.262811       0.332272   \n",
      "5              claude-3-5-sonnet-20241022  0.190045  0.264206       0.233872   \n",
      "2                gemini-2.5-pro-exp-03-25       NaN       NaN            NaN   \n",
      "6                            ground_truth       NaN       NaN       0.921718   \n",
      "\n",
      "   cross_model1-1_gt1-2  cross_model1-2_gt1-1  cross_gt1-1_gt1-2       sum  \n",
      "1              0.338909              0.360100                NaN  0.764997  \n",
      "0              0.338996              0.308615                NaN  0.604356  \n",
      "4              0.213658              0.254887                NaN  0.495631  \n",
      "3              0.233702              0.283712                NaN  0.477176  \n",
      "5              0.224096              0.252955                NaN  0.454251  \n",
      "2                   NaN                   NaN                NaN  0.000000  \n",
      "6                   NaN                   NaN           0.921718  0.000000  \n",
      "Results saved to mario_bert_timestamp_similarity.csv\n",
      "Visualizations saved as mario_bert_timestamp_heatmap.png and mario_bert_timestamp_similarity.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "def compute_clip_to_bert_timestamp_similarity(generated_file, ground_truth_file):\n",
    "    \"\"\"\n",
    "    Compute similarity between generated texts and ground truth using BERT embeddings\n",
    "    for each timestamp separately, then average the similarities.\n",
    "    Based on the CLIP structure but using BERT encodings only (no images).\n",
    "    \n",
    "    Args:\n",
    "        generated_file: Path to the JSON file with generated texts\n",
    "        ground_truth_file: Path to the JSON file with ground truth\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with similarity scores\n",
    "    \"\"\"\n",
    "    # Load generated texts\n",
    "    with open(generated_file, 'r') as f:\n",
    "        generated_texts = json.load(f)\n",
    "    \n",
    "    # Load ground truth\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    # Initialize BERT model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Helper function to extract timestamps and their text\n",
    "    def extract_timestamps(text):\n",
    "        # Extract all timestamp sections like [T1], [T2], etc.\n",
    "        timestamps = re.findall(r'\\[T\\d+\\](.*?)(?=\\[T\\d+\\]|\\Z)', text, re.DOTALL)\n",
    "        # Clean up\n",
    "        timestamps = [t.strip() for t in timestamps]\n",
    "        return timestamps\n",
    "    \n",
    "    # Helper function to get BERT embeddings for text\n",
    "    def get_bert_embedding(text):\n",
    "        return model.encode(text, show_progress_bar=False)\n",
    "    \n",
    "    # Process ground truth and get embeddings\n",
    "    gt_embeddings = {}\n",
    "    gt_timestamp_embeddings = {}\n",
    "    \n",
    "    for level_id, text in ground_truth.items():\n",
    "        timestamp_texts = extract_timestamps(text)\n",
    "        \n",
    "        # Store individual timestamp embeddings\n",
    "        gt_timestamp_embeddings[level_id] = []\n",
    "        for timestamp_text in timestamp_texts:\n",
    "            embedding = get_bert_embedding(timestamp_text)\n",
    "            gt_timestamp_embeddings[level_id].append(embedding)\n",
    "        \n",
    "        # Average all timestamp embeddings for this level (for whole-level comparison)\n",
    "        if gt_timestamp_embeddings[level_id]:\n",
    "            gt_embeddings[level_id] = np.mean(gt_timestamp_embeddings[level_id], axis=0)\n",
    "    \n",
    "    # Calculate cross-similarity between ground truth 1-1 and 1-2\n",
    "    gt_cross_sim = -1\n",
    "    if \"1-1\" in gt_embeddings and \"1-2\" in gt_embeddings:\n",
    "        gt_cross_sim = cosine_similarity([gt_embeddings[\"1-1\"]], [gt_embeddings[\"1-2\"]])[0][0]\n",
    "        print(f\"Ground Truth cross-level similarity (1-1 to 1-2): {gt_cross_sim:.4f}\")\n",
    "    \n",
    "    # Create a dictionary to store similarity scores\n",
    "    similarity_scores = {}\n",
    "    \n",
    "    # For each model in the generated texts\n",
    "    for model_name, level_data in generated_texts.items():\n",
    "        similarity_scores[model_name] = {}\n",
    "        \n",
    "        # Skip models with empty data\n",
    "        if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "            print(f\"Skipping {model_name} due to missing data\")\n",
    "            continue\n",
    "            \n",
    "        # Get embeddings for model outputs\n",
    "        model_embeddings = {}\n",
    "        model_timestamp_embeddings = {}\n",
    "        \n",
    "        for level_id, text in level_data.items():\n",
    "            if text:  # Skip empty texts\n",
    "                timestamp_texts = extract_timestamps(text)\n",
    "                \n",
    "                # Store individual timestamp embeddings\n",
    "                model_timestamp_embeddings[level_id] = []\n",
    "                for timestamp_text in timestamp_texts:\n",
    "                    embedding = get_bert_embedding(timestamp_text)\n",
    "                    model_timestamp_embeddings[level_id].append(embedding)\n",
    "                \n",
    "                # Average all timestamp embeddings for this level (for whole-level comparison)\n",
    "                if model_timestamp_embeddings[level_id]:\n",
    "                    model_embeddings[level_id] = np.mean(model_timestamp_embeddings[level_id], axis=0)\n",
    "        \n",
    "        # Calculate per-timestamp similarity and average for each level\n",
    "        for level_id in [\"1-1\", \"1-2\"]:\n",
    "            if level_id in model_timestamp_embeddings and level_id in gt_timestamp_embeddings:\n",
    "                timestamp_similarities = []\n",
    "                min_timestamps = min(len(model_timestamp_embeddings[level_id]), \n",
    "                                    len(gt_timestamp_embeddings[level_id]))\n",
    "                \n",
    "                for i in range(min_timestamps):\n",
    "                    model_emb = model_timestamp_embeddings[level_id][i]\n",
    "                    gt_emb = gt_timestamp_embeddings[level_id][i]\n",
    "                    sim = cosine_similarity([model_emb], [gt_emb])[0][0]\n",
    "                    timestamp_similarities.append(sim)\n",
    "                \n",
    "                avg_similarity = np.mean(timestamp_similarities) if timestamp_similarities else 0\n",
    "                similarity_scores[model_name][level_id] = avg_similarity\n",
    "                print(f\"{model_name} {level_id} vs GT {level_id}: {avg_similarity:.4f} (averaged across {min_timestamps} timestamps)\")\n",
    "        \n",
    "        # Calculate cross-similarity between model outputs (1-1 and 1-2)\n",
    "        if \"1-1\" in model_timestamp_embeddings and \"1-2\" in model_timestamp_embeddings:\n",
    "            cross_similarities = []\n",
    "            min_timestamps = min(len(model_timestamp_embeddings[\"1-1\"]), \n",
    "                               len(model_timestamp_embeddings[\"1-2\"]))\n",
    "            \n",
    "            for i in range(min_timestamps):\n",
    "                emb_1_1 = model_timestamp_embeddings[\"1-1\"][i]\n",
    "                emb_1_2 = model_timestamp_embeddings[\"1-2\"][i]\n",
    "                sim = cosine_similarity([emb_1_1], [emb_1_2])[0][0]\n",
    "                cross_similarities.append(sim)\n",
    "            \n",
    "            model_cross_sim = np.mean(cross_similarities) if cross_similarities else 0\n",
    "            similarity_scores[model_name][\"cross_1-1_1-2\"] = model_cross_sim\n",
    "            print(f\"{model_name} cross-similarity (1-1 to 1-2): {model_cross_sim:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity: model 1-1 to GT 1-2\n",
    "        if \"1-1\" in model_timestamp_embeddings and \"1-2\" in gt_timestamp_embeddings:\n",
    "            cross_sim_1 = []\n",
    "            min_timestamps = min(len(model_timestamp_embeddings[\"1-1\"]), \n",
    "                               len(gt_timestamp_embeddings[\"1-2\"]))\n",
    "            \n",
    "            for i in range(min_timestamps):\n",
    "                emb_model = model_timestamp_embeddings[\"1-1\"][i]\n",
    "                emb_gt = gt_timestamp_embeddings[\"1-2\"][i]\n",
    "                sim = cosine_similarity([emb_model], [emb_gt])[0][0]\n",
    "                cross_sim_1.append(sim)\n",
    "            \n",
    "            avg_cross_sim_1 = np.mean(cross_sim_1) if cross_sim_1 else 0\n",
    "            similarity_scores[model_name][\"cross_model1-1_gt1-2\"] = avg_cross_sim_1\n",
    "            print(f\"{model_name} 1-1 vs GT 1-2: {avg_cross_sim_1:.4f}\")\n",
    "        \n",
    "        # Calculate cross-similarity: model 1-2 to GT 1-1\n",
    "        if \"1-2\" in model_timestamp_embeddings and \"1-1\" in gt_timestamp_embeddings:\n",
    "            cross_sim_2 = []\n",
    "            min_timestamps = min(len(model_timestamp_embeddings[\"1-2\"]), \n",
    "                               len(gt_timestamp_embeddings[\"1-1\"]))\n",
    "            \n",
    "            for i in range(min_timestamps):\n",
    "                emb_model = model_timestamp_embeddings[\"1-2\"][i]\n",
    "                emb_gt = gt_timestamp_embeddings[\"1-1\"][i]\n",
    "                sim = cosine_similarity([emb_model], [emb_gt])[0][0]\n",
    "                cross_sim_2.append(sim)\n",
    "            \n",
    "            avg_cross_sim_2 = np.mean(cross_sim_2) if cross_sim_2 else 0\n",
    "            similarity_scores[model_name][\"cross_model1-2_gt1-1\"] = avg_cross_sim_2\n",
    "            print(f\"{model_name} 1-2 vs GT 1-1: {avg_cross_sim_2:.4f}\")\n",
    "    \n",
    "    # Add ground truth cross-similarity for reference\n",
    "    similarity_scores[\"ground_truth\"] = {\n",
    "        \"cross_1-1_1-2\": gt_cross_sim,\n",
    "        \"cross_gt1-1_gt1-2\": gt_cross_sim\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    data = []\n",
    "    for model, scores in similarity_scores.items():\n",
    "        row_data = {'Model': model}\n",
    "        row_data.update(scores)\n",
    "        data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add row with sum of similarity scores per model\n",
    "    if '1-1' in df.columns and '1-2' in df.columns:\n",
    "        # Only sum the direct comparison scores for levels 1-1 and 1-2\n",
    "        df['sum'] = df[['1-1', '1-2']].sum(axis=1, skipna=True)\n",
    "        \n",
    "    # Sort by the sum column if it exists\n",
    "    if 'sum' in df.columns:\n",
    "        df = df.sort_values('sum', ascending=False)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    generated_file = \"super_mario_bros_generated_text.json\"\n",
    "    ground_truth_file = \"super_mario_bros_ground_truth.json\"\n",
    "    \n",
    "    # Compute BERT similarity per timestamp\n",
    "    similarity_df = compute_clip_to_bert_timestamp_similarity(generated_file, ground_truth_file)\n",
    "    \n",
    "    # Display the similarity table\n",
    "    print(\"\\nBERT Per-Timestamp Similarity Scores:\")\n",
    "    print(similarity_df)\n",
    "    \n",
    "    # Save to CSV\n",
    "    similarity_df.to_csv(\"mario_bert_timestamp_similarity.csv\")\n",
    "    print(\"Results saved to mario_bert_timestamp_similarity.csv\")\n",
    "    \n",
    "    # Visualize the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set the style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Create a heatmap (exclude the sum column)\n",
    "    heat_df = similarity_df.drop(columns=['Model', 'sum'], errors='ignore')\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(heat_df, annot=True, cmap='viridis', fmt='.3f')\n",
    "    plt.title('BERT Per-Timestamp Similarity Scores')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mario_bert_timestamp_heatmap.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a bar chart for sum similarity\n",
    "    if 'sum' in similarity_df.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.bar(similarity_df['Model'], similarity_df['sum'])\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Sum of Similarity Scores')\n",
    "        plt.title('Sum of Per-Timestamp Similarity Scores by Model')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('mario_bert_timestamp_similarity.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"Visualizations saved as mario_bert_timestamp_heatmap.png and mario_bert_timestamp_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>1-1</th>\n",
       "      <th>1-2</th>\n",
       "      <th>cross_1-1_1-2</th>\n",
       "      <th>cross_model1-1_gt1-2</th>\n",
       "      <th>cross_model1-2_gt1-1</th>\n",
       "      <th>cross_gt1-1_gt1-2</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o3-2025-04-16</td>\n",
       "      <td>0.388851</td>\n",
       "      <td>0.376146</td>\n",
       "      <td>0.457320</td>\n",
       "      <td>0.338909</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.764997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o4-mini-2025-04-16</td>\n",
       "      <td>0.295597</td>\n",
       "      <td>0.308760</td>\n",
       "      <td>0.434997</td>\n",
       "      <td>0.338996</td>\n",
       "      <td>0.308615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.604356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-4-Maverick-17B-128E-Instruct-FP8</td>\n",
       "      <td>0.240731</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.344591</td>\n",
       "      <td>0.213658</td>\n",
       "      <td>0.254887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-7-sonnet-20250219</td>\n",
       "      <td>0.214365</td>\n",
       "      <td>0.262811</td>\n",
       "      <td>0.332272</td>\n",
       "      <td>0.233702</td>\n",
       "      <td>0.283712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>0.190045</td>\n",
       "      <td>0.264206</td>\n",
       "      <td>0.233872</td>\n",
       "      <td>0.224096</td>\n",
       "      <td>0.252955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.454251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-2.5-pro-exp-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ground_truth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921718</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Model       1-1       1-2  cross_1-1_1-2  \\\n",
       "1                           o3-2025-04-16  0.388851  0.376146       0.457320   \n",
       "0                      o4-mini-2025-04-16  0.295597  0.308760       0.434997   \n",
       "4  Llama-4-Maverick-17B-128E-Instruct-FP8  0.240731  0.254900       0.344591   \n",
       "3              claude-3-7-sonnet-20250219  0.214365  0.262811       0.332272   \n",
       "5              claude-3-5-sonnet-20241022  0.190045  0.264206       0.233872   \n",
       "2                gemini-2.5-pro-exp-03-25       NaN       NaN            NaN   \n",
       "6                            ground_truth       NaN       NaN       0.921718   \n",
       "\n",
       "   cross_model1-1_gt1-2  cross_model1-2_gt1-1  cross_gt1-1_gt1-2       sum  \n",
       "1              0.338909              0.360100                NaN  0.764997  \n",
       "0              0.338996              0.308615                NaN  0.604356  \n",
       "4              0.213658              0.254887                NaN  0.495631  \n",
       "3              0.233702              0.283712                NaN  0.477176  \n",
       "5              0.224096              0.252955                NaN  0.454251  \n",
       "2                   NaN                   NaN                NaN  0.000000  \n",
       "6                   NaN                   NaN           0.921718  0.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import torch\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import os\n",
    "\n",
    "# def extract_timepoints(text):\n",
    "#     \"\"\"Extract timepoints from text with their descriptions.\"\"\"\n",
    "#     # Extract [T1], [T2], etc. and their content\n",
    "#     pattern = r'\\[T(\\d+)\\](.*?)(?=\\[T\\d+\\]|$)'\n",
    "#     matches = re.findall(pattern, text, re.DOTALL)\n",
    "    \n",
    "#     timepoints = {}\n",
    "#     for num, content in matches:\n",
    "#         # Clean up the content (remove extra whitespace, newlines, etc.)\n",
    "#         content = re.sub(r'\\s+', ' ', content).strip()\n",
    "#         # Truncate to 200 characters if longer\n",
    "#         if len(content) > 200:\n",
    "#             content = content[:200]\n",
    "#         timepoints[int(num)] = content\n",
    "    \n",
    "#     return timepoints\n",
    "\n",
    "# def compute_similarity_with_clip(generated_file, ground_truth_file):\n",
    "#     \"\"\"\n",
    "#     Compute similarity between generated texts and ground truth using CLIP embeddings.\n",
    "    \n",
    "#     Args:\n",
    "#         generated_file: Path to the JSON file with generated texts\n",
    "#         ground_truth_file: Path to the JSON file with ground truth\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with similarity scores\n",
    "#     \"\"\"\n",
    "#     # Load generated texts\n",
    "#     with open(generated_file, 'r') as f:\n",
    "#         generated_texts = json.load(f)\n",
    "    \n",
    "#     # Load ground truth\n",
    "#     with open(ground_truth_file, 'r') as f:\n",
    "#         ground_truth = json.load(f)\n",
    "    \n",
    "#     # Initialize CLIP model\n",
    "#     print(\"Loading CLIP model...\")\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "#     # Load and preprocess images - use current directory\n",
    "#     level_images = {}\n",
    "#     image_files = {\n",
    "#         '1-1': \"super_mario_bros_1-1_screenshot.png\",\n",
    "#         '1-2': \"super_mario_bros_1-2_screenshot.png\"\n",
    "#     }\n",
    "    \n",
    "#     for level_id, filename in image_files.items():\n",
    "#         try:\n",
    "#             # Look for image in current directory\n",
    "#             image_path = filename\n",
    "#             print(f\"Looking for image at: {os.path.abspath(image_path)}\")\n",
    "            \n",
    "#             if os.path.exists(image_path):\n",
    "#                 image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "#                 with torch.no_grad():\n",
    "#                     level_images[level_id] = model.encode_image(image)\n",
    "#                 print(f\"Successfully loaded image for level {level_id}\")\n",
    "#             else:\n",
    "#                 print(f\"Image file not found: {image_path}\")\n",
    "#                 level_images[level_id] = None\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading image for level {level_id}: {e}\")\n",
    "#             level_images[level_id] = None\n",
    "    \n",
    "#     # If no images were found, use text-only mode\n",
    "#     if all(img is None for img in level_images.values()):\n",
    "#         print(\"No images found, switching to text-only CLIP mode\")\n",
    "    \n",
    "#     # Store similarity scores\n",
    "#     results = []\n",
    "    \n",
    "#     # Process each model\n",
    "#     for model_name, level_data in generated_texts.items():\n",
    "#         model_results = {'Model': model_name}\n",
    "        \n",
    "#         # Skip models with empty data\n",
    "#         if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "#             print(f\"Skipping {model_name} due to missing data\")\n",
    "#             continue\n",
    "        \n",
    "#         # Process each level\n",
    "#         for level_id in ['1-1', '1-2']:\n",
    "#             if level_id not in level_data or not level_data[level_id]:\n",
    "#                 continue\n",
    "                \n",
    "#             # Extract timepoints from generated text and ground truth\n",
    "#             gen_timepoints = extract_timepoints(level_data[level_id])\n",
    "#             gt_timepoints = extract_timepoints(ground_truth[level_id])\n",
    "            \n",
    "#             # Calculate similarity for each timepoint\n",
    "#             timepoint_similarities = []\n",
    "            \n",
    "#             # Find common timepoints\n",
    "#             common_timepoints = set(gen_timepoints.keys()) & set(gt_timepoints.keys())\n",
    "            \n",
    "#             if not common_timepoints:\n",
    "#                 print(f\"No common timepoints found for {model_name} level {level_id}\")\n",
    "#                 continue\n",
    "                \n",
    "#             print(f\"Found {len(common_timepoints)} common timepoints for {model_name} level {level_id}\")\n",
    "            \n",
    "#             for tp in common_timepoints:\n",
    "#                 gen_text = gen_timepoints[tp]\n",
    "#                 gt_text = gt_timepoints[tp]\n",
    "                \n",
    "#                 # Encode texts with CLIP\n",
    "#                 gen_text_tokens = clip.tokenize([gen_text]).to(device)\n",
    "#                 gt_text_tokens = clip.tokenize([gt_text]).to(device)\n",
    "                \n",
    "#                 with torch.no_grad():\n",
    "#                     gen_text_features = model.encode_text(gen_text_tokens)\n",
    "#                     gt_text_features = model.encode_text(gt_text_tokens)\n",
    "                \n",
    "#                 # If we have an image for this level, include it in the calculation\n",
    "#                 if level_images[level_id] is not None:\n",
    "#                     # Combine text and image features\n",
    "#                     image_features = level_images[level_id]\n",
    "                    \n",
    "#                     # Normalize features\n",
    "#                     gen_text_features = gen_text_features / gen_text_features.norm(dim=-1, keepdim=True)\n",
    "#                     gt_text_features = gt_text_features / gt_text_features.norm(dim=-1, keepdim=True)\n",
    "#                     image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                    \n",
    "#                     # Create multimodal embeddings (average of text and image)\n",
    "#                     gen_combined = (gen_text_features + image_features) / 2\n",
    "#                     gt_combined = (gt_text_features + image_features) / 2\n",
    "                    \n",
    "#                     # Calculate cosine similarity\n",
    "#                     similarity = torch.cosine_similarity(gen_combined, gt_combined).item()\n",
    "#                 else:\n",
    "#                     # Text-only mode: just compare text embeddings\n",
    "#                     gen_text_features = gen_text_features / gen_text_features.norm(dim=-1, keepdim=True)\n",
    "#                     gt_text_features = gt_text_features / gt_text_features.norm(dim=-1, keepdim=True)\n",
    "#                     similarity = torch.cosine_similarity(gen_text_features, gt_text_features).item()\n",
    "                \n",
    "#                 timepoint_similarities.append(similarity)\n",
    "            \n",
    "#             # Average similarity across timepoints\n",
    "#             if timepoint_similarities:\n",
    "#                 avg_similarity = np.mean(timepoint_similarities)\n",
    "#                 model_results[level_id] = avg_similarity\n",
    "#                 print(f\"{model_name} {level_id} - Average similarity: {avg_similarity:.4f} (across {len(timepoint_similarities)} timepoints)\")\n",
    "        \n",
    "#         # Calculate cross-level similarities\n",
    "#         if '1-1' in model_results and '1-2' in model_results:\n",
    "#             # Cross-similarity calculation\n",
    "#             model_results['cross_1-1_1-2'] = (model_results['1-1'] + model_results['1-2']) / 2\n",
    "            \n",
    "#             # Sum of both levels for ranking\n",
    "#             model_results['sum'] = model_results['1-1'] + model_results['1-2']\n",
    "            \n",
    "#         results.append(model_results)\n",
    "    \n",
    "#     # Add ground truth timepoint analysis as reference\n",
    "#     gt_results = {'Model': 'ground_truth'}\n",
    "    \n",
    "#     for level_id in ['1-1', '1-2']:\n",
    "#         gt_timepoints = extract_timepoints(ground_truth[level_id])\n",
    "#         if gt_timepoints:\n",
    "#             gt_results[f'timepoints_{level_id}'] = len(gt_timepoints)\n",
    "    \n",
    "#     results.append(gt_results)\n",
    "    \n",
    "#     # Create DataFrame\n",
    "#     df = pd.DataFrame(results)\n",
    "    \n",
    "#     # Sort by sum if available\n",
    "#     if 'sum' in df.columns:\n",
    "#         df = df.sort_values(by='sum', ascending=False)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     generated_file = \"super_mario_bros_generated_text.json\"\n",
    "#     ground_truth_file = \"super_mario_bros_ground_truth.json\"\n",
    "    \n",
    "#     # Install CLIP if not already installed\n",
    "#     # !pip install git+https://github.com/openai/CLIP.git\n",
    "    \n",
    "#     # Compute similarity\n",
    "#     similarity_df = compute_similarity_with_clip(generated_file, ground_truth_file)\n",
    "    \n",
    "#     # Display the similarity table\n",
    "#     print(\"\\nTimestamped CLIP Similarity Scores:\")\n",
    "#     print(similarity_df)\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     similarity_df.to_csv(\"mario_clip_similarity.csv\")\n",
    "#     print(\"Results saved to mario_clip_similarity.csv\")\n",
    "    \n",
    "#     # Create visualizations if data is available\n",
    "#     if 'sum' in similarity_df.columns and len(similarity_df) > 1:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         import seaborn as sns\n",
    "        \n",
    "#         # Plot sum of similarities\n",
    "#         plt.figure(figsize=(14, 8))\n",
    "#         sns.set(style=\"whitegrid\")\n",
    "#         models = similarity_df[similarity_df['Model'] != 'ground_truth']\n",
    "#         if not models.empty:\n",
    "#             ax = sns.barplot(x=models['Model'], y=models['sum'])\n",
    "#             plt.xticks(rotation=45, ha='right')\n",
    "#             plt.xlabel('Model')\n",
    "#             plt.ylabel('Sum of Timestamped Similarities')\n",
    "#             plt.title('CLIP-based Timestamped Similarity by Model')\n",
    "#             plt.tight_layout()\n",
    "#             plt.savefig('mario_clip_similarity.png', dpi=300)\n",
    "#             print(\"Visualization saved as mario_clip_similarity.png\")\n",
    "#         else:\n",
    "#             print(\"Not enough data for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from rouge_score import rouge_scorer\n",
    "# import re\n",
    "\n",
    "# def parse_level_details(text):\n",
    "#     \"\"\"\n",
    "#     Extract only the LevelDetails section from the text.\n",
    "    \n",
    "#     Args:\n",
    "#         text: Full text containing GameName, LevelNumber, Area, and LevelDetails\n",
    "        \n",
    "#     Returns:\n",
    "#         String containing only the LevelDetails part\n",
    "#     \"\"\"\n",
    "#     lines = text.strip().split('\\n')\n",
    "#     level_details = \"\"\n",
    "#     in_level_details = False\n",
    "    \n",
    "#     for i, line in enumerate(lines):\n",
    "#         if line.strip().startswith(\"LevelDetails:\"):\n",
    "#             in_level_details = True\n",
    "#             # Extract content after \"LevelDetails:\"\n",
    "#             level_details = line[line.find(\"LevelDetails:\") + len(\"LevelDetails:\"):].strip()\n",
    "#         elif in_level_details:\n",
    "#             # Continue collecting LevelDetails until we hit a new section header\n",
    "#             if (line.strip().startswith(\"GameName:\") or \n",
    "#                 line.strip().startswith(\"LevelNumber:\") or\n",
    "#                 line.strip().startswith(\"Area:\")):\n",
    "#                 in_level_details = False\n",
    "#             else:\n",
    "#                 # Append non-header lines to maintain original formatting\n",
    "#                 level_details += \" \" + line.strip()\n",
    "    \n",
    "#     # If no LevelDetails found explicitly, try to infer it from remaining lines\n",
    "#     if not level_details:\n",
    "#         # Skip GameName, LevelNumber, and Area lines\n",
    "#         remaining_lines = []\n",
    "#         for line in lines:\n",
    "#             if (not line.strip().startswith(\"GameName:\") and \n",
    "#                 not line.strip().startswith(\"LevelNumber:\") and\n",
    "#                 not line.strip().startswith(\"Area:\") and line.strip()):\n",
    "#                 remaining_lines.append(line.strip())\n",
    "        \n",
    "#         if remaining_lines:\n",
    "#             level_details = \" \".join(remaining_lines)\n",
    "    \n",
    "#     return level_details\n",
    "\n",
    "# def simple_tokenize(text):\n",
    "#     \"\"\"\n",
    "#     Simple tokenization without NLTK dependency.\n",
    "    \n",
    "#     Args:\n",
    "#         text: String to tokenize\n",
    "        \n",
    "#     Returns:\n",
    "#         List of tokens\n",
    "#     \"\"\"\n",
    "#     # Convert to lowercase and replace punctuation with spaces\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "#     # Split by whitespace and filter out empty strings\n",
    "#     tokens = [token.strip() for token in text.split()]\n",
    "#     tokens = [token for token in tokens if token]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# def compute_rouge_similarity(generated_file, ground_truth_file):\n",
    "#     \"\"\"\n",
    "#     Compute similarity between generated texts and ground truth using ROUGE-L.\n",
    "    \n",
    "#     Args:\n",
    "#         generated_file: Path to the JSON file with generated texts\n",
    "#         ground_truth_file: Path to the JSON file with ground truth\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with similarity scores\n",
    "#     \"\"\"\n",
    "#     # Load generated texts\n",
    "#     with open(generated_file, 'r') as f:\n",
    "#         generated_texts = json.load(f)\n",
    "    \n",
    "#     # Load ground truth\n",
    "#     with open(ground_truth_file, 'r') as f:\n",
    "#         ground_truth = json.load(f)\n",
    "    \n",
    "#     # Initialize ROUGE scorer\n",
    "#     scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "#     # Extract LevelDetails from ground truth\n",
    "#     gt_level_details = {}\n",
    "#     for level_id, text in ground_truth.items():\n",
    "#         details = parse_level_details(text)\n",
    "#         gt_level_details[level_id] = details\n",
    "    \n",
    "#     # Calculate cross-similarity between ground truth 1-1 and 1-2\n",
    "#     if \"1-1\" in gt_level_details and \"1-2\" in gt_level_details:\n",
    "#         gt_cross_score = scorer.score(gt_level_details[\"1-1\"], gt_level_details[\"1-2\"])\n",
    "#         gt_cross_f1 = gt_cross_score[\"rougeL\"].fmeasure\n",
    "#         print(f\"Ground Truth cross-level similarity (1-1 to 1-2): {gt_cross_f1:.4f}\")\n",
    "#     else:\n",
    "#         gt_cross_f1 = None\n",
    "#         print(\"Cannot calculate ground truth cross-similarity - missing details\")\n",
    "    \n",
    "#     # Create a dictionary to store similarity scores\n",
    "#     similarity_scores = {}\n",
    "    \n",
    "#     # For each model in the generated texts\n",
    "#     for model_name, level_data in generated_texts.items():\n",
    "#         similarity_scores[model_name] = {}\n",
    "        \n",
    "#         # Skip models with empty data\n",
    "#         if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "#             print(f\"Skipping {model_name} due to missing data\")\n",
    "#             continue\n",
    "            \n",
    "#         # Extract LevelDetails from model outputs\n",
    "#         model_level_details = {}\n",
    "#         for level_id, text in level_data.items():\n",
    "#             if text:  # Skip empty texts\n",
    "#                 details = parse_level_details(text)\n",
    "#                 model_level_details[level_id] = details\n",
    "        \n",
    "#         # Calculate similarity to corresponding ground truth\n",
    "#         for level_id, details in model_level_details.items():\n",
    "#             if level_id in gt_level_details:\n",
    "#                 score = scorer.score(details, gt_level_details[level_id])\n",
    "#                 f1_score = score[\"rougeL\"].fmeasure\n",
    "#                 similarity_scores[model_name][level_id] = f1_score\n",
    "#                 print(f\"{model_name} {level_id} vs GT {level_id}: {f1_score:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity between model outputs\n",
    "#         if \"1-1\" in model_level_details and \"1-2\" in model_level_details:\n",
    "#             model_cross_score = scorer.score(model_level_details[\"1-1\"], model_level_details[\"1-2\"])\n",
    "#             model_cross_f1 = model_cross_score[\"rougeL\"].fmeasure\n",
    "#             similarity_scores[model_name][\"cross_1-1_1-2\"] = model_cross_f1\n",
    "#             print(f\"{model_name} cross-similarity (1-1 to 1-2): {model_cross_f1:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity: model 1-1 to GT 1-2\n",
    "#         if \"1-1\" in model_level_details and \"1-2\" in gt_level_details:\n",
    "#             cross_score_1 = scorer.score(model_level_details[\"1-1\"], gt_level_details[\"1-2\"])\n",
    "#             cross_f1_1 = cross_score_1[\"rougeL\"].fmeasure\n",
    "#             similarity_scores[model_name][\"cross_model1-1_gt1-2\"] = cross_f1_1\n",
    "#             print(f\"{model_name} 1-1 vs GT 1-2: {cross_f1_1:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity: model 1-2 to GT 1-1\n",
    "#         if \"1-2\" in model_level_details and \"1-1\" in gt_level_details:\n",
    "#             cross_score_2 = scorer.score(model_level_details[\"1-2\"], gt_level_details[\"1-1\"])\n",
    "#             cross_f1_2 = cross_score_2[\"rougeL\"].fmeasure\n",
    "#             similarity_scores[model_name][\"cross_model1-2_gt1-1\"] = cross_f1_2\n",
    "#             print(f\"{model_name} 1-2 vs GT 1-1: {cross_f1_2:.4f}\")\n",
    "    \n",
    "#     # Add ground truth cross-similarity for reference\n",
    "#     if gt_cross_f1 is not None:\n",
    "#         similarity_scores[\"ground_truth\"] = {\n",
    "#             \"cross_gt1-1_gt1-2\": gt_cross_f1\n",
    "#         }\n",
    "    \n",
    "#     # Create DataFrame for better visualization\n",
    "#     data = []\n",
    "#     for model, scores in similarity_scores.items():\n",
    "#         model_row = {\"Model\": model}\n",
    "#         for metric, score in scores.items():\n",
    "#             model_row[metric] = score\n",
    "#         data.append(model_row)\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Add row with sum of similarity scores per model\n",
    "#     if not df.empty and '1-1' in df.columns and '1-2' in df.columns:\n",
    "#         # Only sum the direct comparison scores for levels 1-1 and 1-2\n",
    "#         df['sum'] = df[['1-1', '1-2']].sum(axis=1, skipna=True)\n",
    "        \n",
    "#         # Sort by the sum of 1-1 and 1-2 similarity (descending)\n",
    "#         df = df.sort_values('sum', ascending=False)\n",
    "        \n",
    "#     return df\n",
    "\n",
    "# def analyze_term_overlap(generated_file, ground_truth_file):\n",
    "#     \"\"\"\n",
    "#     Analyze key term overlap without using NLTK.\n",
    "    \n",
    "#     Args:\n",
    "#         generated_file: Path to the JSON file with generated texts\n",
    "#         ground_truth_file: Path to the JSON file with ground truth\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with key term overlap statistics\n",
    "#     \"\"\"\n",
    "#     # Load generated texts\n",
    "#     with open(generated_file, 'r') as f:\n",
    "#         generated_texts = json.load(f)\n",
    "    \n",
    "#     # Load ground truth\n",
    "#     with open(ground_truth_file, 'r') as f:\n",
    "#         ground_truth = json.load(f)\n",
    "    \n",
    "#     # Extract LevelDetails from ground truth\n",
    "#     gt_level_details = {}\n",
    "#     for level_id, text in ground_truth.items():\n",
    "#         details = parse_level_details(text)\n",
    "#         gt_level_details[level_id] = details\n",
    "    \n",
    "#     # Tokenize ground truth level details\n",
    "#     gt_tokens = {}\n",
    "#     for level_id, details in gt_level_details.items():\n",
    "#         tokens = set(simple_tokenize(details))\n",
    "#         gt_tokens[level_id] = tokens\n",
    "    \n",
    "#     # Calculate key terms that appear in ground truth\n",
    "#     key_mario_terms = {\n",
    "#         \"goomba\", \"koopa\", \"troopa\", \"pipe\", \"mushroom\", \"block\", \n",
    "#         \"coin\", \"brick\", \"star\", \"flagpole\", \"fire\", \"flower\", \n",
    "#         \"pit\", \"gap\", \"stair\", \"staircase\", \"warp\", \"bonus\"\n",
    "#     }\n",
    "    \n",
    "#     # Initialize results dictionary\n",
    "#     term_results = {}\n",
    "    \n",
    "#     # For each model in the generated texts\n",
    "#     for model_name, level_data in generated_texts.items():\n",
    "#         term_results[model_name] = {}\n",
    "        \n",
    "#         # Skip models with empty data\n",
    "#         if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "#             print(f\"Skipping {model_name} due to missing data\")\n",
    "#             continue\n",
    "            \n",
    "#         # Extract LevelDetails from model outputs\n",
    "#         model_level_details = {}\n",
    "#         for level_id, text in level_data.items():\n",
    "#             if text:  # Skip empty texts\n",
    "#                 details = parse_level_details(text)\n",
    "#                 model_level_details[level_id] = details\n",
    "        \n",
    "#         # Tokenize model level details\n",
    "#         model_tokens = {}\n",
    "#         for level_id, details in model_level_details.items():\n",
    "#             tokens = set(simple_tokenize(details))\n",
    "#             model_tokens[level_id] = tokens\n",
    "        \n",
    "#         # Calculate overlap with key terms for each level\n",
    "#         for level_id, tokens in model_tokens.items():\n",
    "#             if level_id in gt_tokens:\n",
    "#                 # Calculate key term coverage\n",
    "#                 model_key_terms = tokens.intersection(key_mario_terms)\n",
    "#                 gt_key_terms = gt_tokens[level_id].intersection(key_mario_terms)\n",
    "                \n",
    "#                 key_term_count = len(model_key_terms)\n",
    "#                 key_term_overlap = len(model_key_terms.intersection(gt_key_terms))\n",
    "#                 key_term_coverage = key_term_overlap / len(gt_key_terms) if gt_key_terms else 0\n",
    "                \n",
    "#                 term_results[model_name][f\"{level_id}_key_terms\"] = key_term_count\n",
    "#                 term_results[model_name][f\"{level_id}_key_term_overlap\"] = key_term_overlap\n",
    "#                 term_results[model_name][f\"{level_id}_key_term_coverage\"] = key_term_coverage\n",
    "        \n",
    "#         # Calculate average key term coverage across levels\n",
    "#         if \"1-1_key_term_coverage\" in term_results[model_name] and \"1-2_key_term_coverage\" in term_results[model_name]:\n",
    "#             avg_coverage = (term_results[model_name][\"1-1_key_term_coverage\"] + \n",
    "#                            term_results[model_name][\"1-2_key_term_coverage\"]) / 2\n",
    "#             term_results[model_name][\"avg_key_term_coverage\"] = avg_coverage\n",
    "    \n",
    "#     # Create DataFrame for better visualization\n",
    "#     data = []\n",
    "#     for model, results in term_results.items():\n",
    "#         model_row = {\"Model\": model}\n",
    "#         for metric, value in results.items():\n",
    "#             model_row[metric] = value\n",
    "#         data.append(model_row)\n",
    "    \n",
    "#     term_df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Sort by average key term coverage (descending)\n",
    "#     if not term_df.empty and \"avg_key_term_coverage\" in term_df.columns:\n",
    "#         term_df = term_df.sort_values(\"avg_key_term_coverage\", ascending=False)\n",
    "    \n",
    "#     return term_df\n",
    "\n",
    "# def visualize_results(df):\n",
    "#     \"\"\"\n",
    "#     Create visualizations for the similarity scores.\n",
    "    \n",
    "#     Args:\n",
    "#         df: DataFrame with similarity scores\n",
    "        \n",
    "#     Returns:\n",
    "#         None (displays plots)\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import seaborn as sns\n",
    "    \n",
    "#     # Set the style\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "    \n",
    "#     # Create a heatmap (exclude the sum column)\n",
    "#     plt.figure(figsize=(14, 10))\n",
    "#     # Exclude Model and sum columns for heatmap\n",
    "#     heatmap_cols = [col for col in df.columns if col != 'Model' and col != 'sum']\n",
    "#     if len(heatmap_cols) > 0:\n",
    "#         heatmap_df = df.set_index('Model')[heatmap_cols]\n",
    "#         sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.4f')\n",
    "#         plt.title('ROUGE-L F1 Similarity Scores for LevelDetails')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_rouge_similarity_heatmap.png', dpi=300)\n",
    "#         plt.close()\n",
    "    \n",
    "#     # Create a bar chart for level similarities\n",
    "#     plt.figure(figsize=(14, 8))\n",
    "#     # Filter for only direct level comparison columns\n",
    "#     level_cols = ['1-1', '1-2']\n",
    "#     level_cols = [col for col in level_cols if col in df.columns]\n",
    "    \n",
    "#     if level_cols:\n",
    "#         level_df = df.set_index('Model')[level_cols]\n",
    "#         level_df.plot(kind='bar', figsize=(14, 8))\n",
    "#         plt.ylabel('ROUGE-L F1 Score')\n",
    "#         plt.title('ROUGE-L Similarity by Level')\n",
    "#         plt.xticks(rotation=45, ha='right')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_rouge_level_similarity.png', dpi=300)\n",
    "#         plt.close()\n",
    "    \n",
    "#     # Create a bar chart for sum similarity\n",
    "#     plt.figure(figsize=(14, 8))\n",
    "#     if 'sum' in df.columns:\n",
    "#         sum_df = df.sort_values('sum', ascending=False)\n",
    "#         sns.barplot(x='Model', y='sum', data=sum_df)\n",
    "#         plt.ylabel('Sum of ROUGE-L F1 Scores')\n",
    "#         plt.title('Sum of ROUGE-L Similarity Scores by Model')\n",
    "#         plt.xticks(rotation=45, ha='right')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_rouge_sum_similarity.png', dpi=300)\n",
    "#         plt.close()\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Paths\n",
    "#     generated_file = \"super_mario_bros_generated_text.json\"\n",
    "#     ground_truth_file = \"super_mario_bros_ground_truth.json\"\n",
    "    \n",
    "#     # Install required packages if needed\n",
    "#     # !pip install rouge-score pandas matplotlib seaborn\n",
    "\n",
    "#     # Compute ROUGE similarity\n",
    "#     print(\"\\nCalculating ROUGE-L similarity...\")\n",
    "#     similarity_df = compute_rouge_similarity(generated_file, ground_truth_file)\n",
    "    \n",
    "#     # Display the similarity table\n",
    "#     print(\"\\nROUGE-L Similarity Scores:\")\n",
    "#     print(similarity_df)\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     similarity_df.to_csv(\"mario_rouge_similarity_scores.csv\")\n",
    "#     print(\"Results saved to mario_rouge_similarity_scores.csv\")\n",
    "    \n",
    "#     # Analyze key term overlap\n",
    "#     print(\"\\nAnalyzing key term overlap...\")\n",
    "#     term_df = analyze_term_overlap(generated_file, ground_truth_file)\n",
    "    \n",
    "#     # Display the term results\n",
    "#     print(\"\\nKey Term Overlap Analysis:\")\n",
    "#     print(term_df)\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     term_df.to_csv(\"mario_key_term_overlap.csv\")\n",
    "#     print(\"Results saved to mario_key_term_overlap.csv\")\n",
    "    \n",
    "#     # Visualize the ROUGE results\n",
    "#     visualize_results(similarity_df)\n",
    "#     print(\"Visualizations saved as mario_rouge_similarity_heatmap.png, mario_rouge_level_similarity.png, and mario_rouge_sum_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# def parse_level_details(text):\n",
    "#     \"\"\"\n",
    "#     Extract only the LevelDetails section from the text.\n",
    "    \n",
    "#     Args:\n",
    "#         text: Full text containing GameName, LevelNumber, Area, and LevelDetails\n",
    "        \n",
    "#     Returns:\n",
    "#         String containing only the LevelDetails part\n",
    "#     \"\"\"\n",
    "#     lines = text.strip().split('\\n')\n",
    "#     level_details = \"\"\n",
    "#     in_level_details = False\n",
    "    \n",
    "#     for i, line in enumerate(lines):\n",
    "#         if line.strip().startswith(\"LevelDetails:\"):\n",
    "#             in_level_details = True\n",
    "#             # Extract content after \"LevelDetails:\"\n",
    "#             level_details = line[line.find(\"LevelDetails:\") + len(\"LevelDetails:\"):].strip()\n",
    "#         elif in_level_details:\n",
    "#             # Continue collecting LevelDetails until we hit a new section header\n",
    "#             if (line.strip().startswith(\"GameName:\") or \n",
    "#                 line.strip().startswith(\"LevelNumber:\") or\n",
    "#                 line.strip().startswith(\"Area:\")):\n",
    "#                 in_level_details = False\n",
    "#             else:\n",
    "#                 # Append non-header lines to maintain original formatting\n",
    "#                 level_details += \" \" + line.strip()\n",
    "    \n",
    "#     # If no LevelDetails found explicitly, try to infer it from remaining lines\n",
    "#     if not level_details:\n",
    "#         # Skip GameName, LevelNumber, and Area lines\n",
    "#         remaining_lines = []\n",
    "#         for line in lines:\n",
    "#             if (not line.strip().startswith(\"GameName:\") and \n",
    "#                 not line.strip().startswith(\"LevelNumber:\") and\n",
    "#                 not line.strip().startswith(\"Area:\") and line.strip()):\n",
    "#                 remaining_lines.append(line.strip())\n",
    "        \n",
    "#         if remaining_lines:\n",
    "#             level_details = \" \".join(remaining_lines)\n",
    "    \n",
    "#     return level_details\n",
    "\n",
    "# def simple_tokenize(text):\n",
    "#     \"\"\"\n",
    "#     Simple tokenization without heavy dependencies.\n",
    "    \n",
    "#     Args:\n",
    "#         text: String to tokenize\n",
    "        \n",
    "#     Returns:\n",
    "#         List of tokens\n",
    "#     \"\"\"\n",
    "#     # Convert to lowercase and replace punctuation with spaces\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "#     # Split by whitespace and filter out empty strings\n",
    "#     tokens = [token.strip() for token in text.split()]\n",
    "#     tokens = [token for token in tokens if token]\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# def calculate_bleu_score(candidate_text, reference_text):\n",
    "#     \"\"\"\n",
    "#     Calculate BLEU score between candidate and reference text.\n",
    "    \n",
    "#     Args:\n",
    "#         candidate_text: Generated text\n",
    "#         reference_text: Ground truth text\n",
    "        \n",
    "#     Returns:\n",
    "#         BLEU score\n",
    "#     \"\"\"\n",
    "#     # Tokenize texts\n",
    "#     candidate_tokens = simple_tokenize(candidate_text)\n",
    "#     reference_tokens = [simple_tokenize(reference_text)]  # BLEU expects list of references\n",
    "    \n",
    "#     # Apply smoothing to handle cases with no n-gram overlaps\n",
    "#     smoothing = SmoothingFunction().method1\n",
    "    \n",
    "#     # Calculate BLEU with weights for 1-gram, 2-gram, 3-gram, and 4-gram\n",
    "#     # Default weights are (0.25, 0.25, 0.25, 0.25)\n",
    "#     weights = (0.4, 0.3, 0.2, 0.1)  # Giving more weight to unigrams and bigrams\n",
    "    \n",
    "#     try:\n",
    "#         score = sentence_bleu(reference_tokens, candidate_tokens, weights=weights, smoothing_function=smoothing)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error calculating BLEU score: {e}\")\n",
    "#         score = 0.0\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# def compute_bleu_similarity(generated_file, ground_truth_file):\n",
    "#     \"\"\"\n",
    "#     Compute similarity between generated texts and ground truth using BLEU score.\n",
    "    \n",
    "#     Args:\n",
    "#         generated_file: Path to the JSON file with generated texts\n",
    "#         ground_truth_file: Path to the JSON file with ground truth\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with similarity scores\n",
    "#     \"\"\"\n",
    "#     # Load generated texts\n",
    "#     with open(generated_file, 'r') as f:\n",
    "#         generated_texts = json.load(f)\n",
    "    \n",
    "#     # Load ground truth\n",
    "#     with open(ground_truth_file, 'r') as f:\n",
    "#         ground_truth = json.load(f)\n",
    "    \n",
    "#     # Extract LevelDetails from ground truth\n",
    "#     gt_level_details = {}\n",
    "#     for level_id, text in ground_truth.items():\n",
    "#         details = parse_level_details(text)\n",
    "#         gt_level_details[level_id] = details\n",
    "    \n",
    "#     # Calculate cross-similarity between ground truth 1-1 and 1-2\n",
    "#     if \"1-1\" in gt_level_details and \"1-2\" in gt_level_details:\n",
    "#         gt_cross_score = calculate_bleu_score(gt_level_details[\"1-1\"], gt_level_details[\"1-2\"])\n",
    "#         print(f\"Ground Truth cross-level similarity (1-1 to 1-2): {gt_cross_score:.4f}\")\n",
    "#     else:\n",
    "#         gt_cross_score = None\n",
    "#         print(\"Cannot calculate ground truth cross-similarity - missing details\")\n",
    "    \n",
    "#     # Create a dictionary to store similarity scores\n",
    "#     similarity_scores = {}\n",
    "    \n",
    "#     # For each model in the generated texts\n",
    "#     for model_name, level_data in generated_texts.items():\n",
    "#         similarity_scores[model_name] = {}\n",
    "        \n",
    "#         # Skip models with empty data\n",
    "#         if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "#             print(f\"Skipping {model_name} due to missing data\")\n",
    "#             continue\n",
    "            \n",
    "#         # Extract LevelDetails from model outputs\n",
    "#         model_level_details = {}\n",
    "#         for level_id, text in level_data.items():\n",
    "#             if text:  # Skip empty texts\n",
    "#                 details = parse_level_details(text)\n",
    "#                 model_level_details[level_id] = details\n",
    "        \n",
    "#         # Calculate similarity to corresponding ground truth\n",
    "#         for level_id, details in model_level_details.items():\n",
    "#             if level_id in gt_level_details:\n",
    "#                 bleu_score = calculate_bleu_score(details, gt_level_details[level_id])\n",
    "#                 similarity_scores[model_name][level_id] = bleu_score\n",
    "#                 print(f\"{model_name} {level_id} vs GT {level_id}: {bleu_score:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity between model outputs\n",
    "#         if \"1-1\" in model_level_details and \"1-2\" in model_level_details:\n",
    "#             model_cross_score = calculate_bleu_score(model_level_details[\"1-1\"], model_level_details[\"1-2\"])\n",
    "#             similarity_scores[model_name][\"cross_1-1_1-2\"] = model_cross_score\n",
    "#             print(f\"{model_name} cross-similarity (1-1 to 1-2): {model_cross_score:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity: model 1-1 to GT 1-2\n",
    "#         if \"1-1\" in model_level_details and \"1-2\" in gt_level_details:\n",
    "#             cross_score_1 = calculate_bleu_score(model_level_details[\"1-1\"], gt_level_details[\"1-2\"])\n",
    "#             similarity_scores[model_name][\"cross_model1-1_gt1-2\"] = cross_score_1\n",
    "#             print(f\"{model_name} 1-1 vs GT 1-2: {cross_score_1:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity: model 1-2 to GT 1-1\n",
    "#         if \"1-2\" in model_level_details and \"1-1\" in gt_level_details:\n",
    "#             cross_score_2 = calculate_bleu_score(model_level_details[\"1-2\"], gt_level_details[\"1-1\"])\n",
    "#             similarity_scores[model_name][\"cross_model1-2_gt1-1\"] = cross_score_2\n",
    "#             print(f\"{model_name} 1-2 vs GT 1-1: {cross_score_2:.4f}\")\n",
    "    \n",
    "#     # Add ground truth cross-similarity for reference\n",
    "#     if gt_cross_score is not None:\n",
    "#         similarity_scores[\"ground_truth\"] = {\n",
    "#             \"cross_gt1-1_gt1-2\": gt_cross_score\n",
    "#         }\n",
    "    \n",
    "#     # Create DataFrame for better visualization\n",
    "#     data = []\n",
    "#     for model, scores in similarity_scores.items():\n",
    "#         model_row = {\"Model\": model}\n",
    "#         for metric, score in scores.items():\n",
    "#             model_row[metric] = score\n",
    "#         data.append(model_row)\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Add row with sum of similarity scores per model\n",
    "#     if not df.empty and '1-1' in df.columns and '1-2' in df.columns:\n",
    "#         # Only sum the direct comparison scores for levels 1-1 and 1-2\n",
    "#         df['sum'] = df[['1-1', '1-2']].sum(axis=1, skipna=True)\n",
    "        \n",
    "#         # Sort by the sum of 1-1 and 1-2 similarity (descending)\n",
    "#         df = df.sort_values('sum', ascending=False)\n",
    "        \n",
    "#     return df\n",
    "\n",
    "# def visualize_results(df):\n",
    "#     \"\"\"\n",
    "#     Create visualizations for the similarity scores.\n",
    "    \n",
    "#     Args:\n",
    "#         df: DataFrame with similarity scores\n",
    "        \n",
    "#     Returns:\n",
    "#         None (displays plots)\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import seaborn as sns\n",
    "    \n",
    "#     # Set the style\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "    \n",
    "#     # Create a heatmap (exclude the sum column)\n",
    "#     plt.figure(figsize=(14, 10))\n",
    "#     # Exclude Model and sum columns for heatmap\n",
    "#     heatmap_cols = [col for col in df.columns if col != 'Model' and col != 'sum']\n",
    "#     if len(heatmap_cols) > 0:\n",
    "#         heatmap_df = df.set_index('Model')[heatmap_cols]\n",
    "#         sns.heatmap(heatmap_df, annot=True, cmap='viridis', fmt='.4f')\n",
    "#         plt.title('BLEU Similarity Scores for LevelDetails')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_bleu_similarity_heatmap.png', dpi=300)\n",
    "#         plt.close()\n",
    "    \n",
    "#     # Create a bar chart for level similarities\n",
    "#     plt.figure(figsize=(14, 8))\n",
    "#     # Filter for only direct level comparison columns\n",
    "#     level_cols = ['1-1', '1-2']\n",
    "#     level_cols = [col for col in level_cols if col in df.columns]\n",
    "    \n",
    "#     if level_cols:\n",
    "#         level_df = df.set_index('Model')[level_cols]\n",
    "#         level_df.plot(kind='bar', figsize=(14, 8))\n",
    "#         plt.ylabel('BLEU Score')\n",
    "#         plt.title('BLEU Similarity by Level')\n",
    "#         plt.xticks(rotation=45, ha='right')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_bleu_level_similarity.png', dpi=300)\n",
    "#         plt.close()\n",
    "    \n",
    "#     # Create a bar chart for sum similarity\n",
    "#     plt.figure(figsize=(14, 8))\n",
    "#     if 'sum' in df.columns:\n",
    "#         sum_df = df.sort_values('sum', ascending=False)\n",
    "#         sns.barplot(x='Model', y='sum', data=sum_df)\n",
    "#         plt.ylabel('Sum of BLEU Scores')\n",
    "#         plt.title('Sum of BLEU Similarity Scores by Model')\n",
    "#         plt.xticks(rotation=45, ha='right')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_bleu_sum_similarity.png', dpi=300)\n",
    "#         plt.close()\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Paths\n",
    "#     generated_file = \"super_mario_bros_generated_text.json\"\n",
    "#     ground_truth_file = \"super_mario_bros_ground_truth.json\"\n",
    "    \n",
    "#     # Install required packages if needed\n",
    "#     # !pip install nltk pandas matplotlib seaborn\n",
    "    \n",
    "#     # Download NLTK resources if needed\n",
    "#     import nltk\n",
    "#     try:\n",
    "#         nltk.data.find('tokenizers/punkt')\n",
    "#     except LookupError:\n",
    "#         nltk.download('punkt')\n",
    "\n",
    "#     # Compute BLEU similarity\n",
    "#     print(\"\\nCalculating BLEU similarity...\")\n",
    "#     similarity_df = compute_bleu_similarity(generated_file, ground_truth_file)\n",
    "    \n",
    "#     # Display the similarity table\n",
    "#     print(\"\\nBLEU Similarity Scores:\")\n",
    "#     print(similarity_df)\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     similarity_df.to_csv(\"mario_bleu_similarity_scores.csv\")\n",
    "#     print(\"Results saved to mario_bleu_similarity_scores.csv\")\n",
    "    \n",
    "#     # Visualize the BLEU results\n",
    "#     visualize_results(similarity_df)\n",
    "#     print(\"Visualizations saved as mario_bleu_similarity_heatmap.png, mario_bleu_level_similarity.png, and mario_bleu_sum_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# from transformers import CLIPProcessor, CLIPModel, CLIPConfig\n",
    "\n",
    "# def parse_level_details(text):\n",
    "#     \"\"\"\n",
    "#     Extract only the LevelDetails section from the text.\n",
    "    \n",
    "#     Args:\n",
    "#         text: Full text containing GameName, LevelNumber, Area, and LevelDetails\n",
    "        \n",
    "#     Returns:\n",
    "#         String containing only the LevelDetails part\n",
    "#     \"\"\"\n",
    "#     lines = text.strip().split('\\n')\n",
    "#     level_details = \"\"\n",
    "#     in_level_details = False\n",
    "    \n",
    "#     for i, line in enumerate(lines):\n",
    "#         if line.strip().startswith(\"LevelDetails:\"):\n",
    "#             in_level_details = True\n",
    "#             # Extract content after \"LevelDetails:\"\n",
    "#             level_details = line[line.find(\"LevelDetails:\") + len(\"LevelDetails:\"):].strip()\n",
    "#         elif in_level_details:\n",
    "#             # Continue collecting LevelDetails until we hit a new section header\n",
    "#             if (line.strip().startswith(\"GameName:\") or \n",
    "#                 line.strip().startswith(\"LevelNumber:\") or\n",
    "#                 line.strip().startswith(\"Area:\")):\n",
    "#                 in_level_details = False\n",
    "#             else:\n",
    "#                 # Append non-header lines to maintain original formatting\n",
    "#                 level_details += \" \" + line.strip()\n",
    "    \n",
    "#     # If no LevelDetails found explicitly, try to infer it from remaining lines\n",
    "#     if not level_details:\n",
    "#         # Skip GameName, LevelNumber, and Area lines\n",
    "#         remaining_lines = []\n",
    "#         for line in lines:\n",
    "#             if (not line.strip().startswith(\"GameName:\") and \n",
    "#                 not line.strip().startswith(\"LevelNumber:\") and\n",
    "#                 not line.strip().startswith(\"Area:\") and line.strip()):\n",
    "#                 remaining_lines.append(line.strip())\n",
    "        \n",
    "#         if remaining_lines:\n",
    "#             level_details = \" \".join(remaining_lines)\n",
    "    \n",
    "#     return level_details\n",
    "\n",
    "# class LongCLIPEncoder:\n",
    "#     \"\"\"\n",
    "#     A class that uses LongCLIP for both image and text encoding.\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         # Load LongCLIP\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         model_id = \"zer0int/LongCLIP-GmP-ViT-L-14\"\n",
    "        \n",
    "#         # Configure model with extended token length\n",
    "#         config = CLIPConfig.from_pretrained(model_id)\n",
    "#         config.text_config.max_position_embeddings = 248  # Extended context length\n",
    "        \n",
    "#         # Load model and processor\n",
    "#         print(\"Loading LongCLIP model...\")\n",
    "#         self.model = CLIPModel.from_pretrained(model_id, config=config)\n",
    "#         self.processor = CLIPProcessor.from_pretrained(model_id, padding=\"max_length\", max_length=248)\n",
    "        \n",
    "#         # Move model to device\n",
    "#         self.model = self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "    \n",
    "#     def encode_image(self, image_path):\n",
    "#         \"\"\"Encode an image using LongCLIP.\"\"\"\n",
    "#         try:\n",
    "#             # Load and preprocess image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "#             inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 image_features = self.model.get_image_features(**inputs)\n",
    "#                 # Normalize features\n",
    "#                 image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "            \n",
    "#             return image_features.cpu().numpy()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error encoding image {image_path}: {e}\")\n",
    "#             return None\n",
    "    \n",
    "#     def encode_text(self, text):\n",
    "#         \"\"\"Encode text using LongCLIP.\"\"\"\n",
    "#         try:\n",
    "#             # Process text\n",
    "#             inputs = self.processor(text=text, return_tensors=\"pt\", padding=\"max_length\", max_length=248).to(self.device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 text_features = self.model.get_text_features(**inputs)\n",
    "#                 # Normalize features\n",
    "#                 text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "            \n",
    "#             return text_features.cpu().numpy()\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error encoding text: {e}\")\n",
    "#             return None\n",
    "    \n",
    "#     def encode_combined(self, image_path, text):\n",
    "#         \"\"\"Encode both image and text, then average their embeddings.\"\"\"\n",
    "#         img_emb = self.encode_image(image_path)\n",
    "#         text_emb = self.encode_text(text)\n",
    "        \n",
    "#         if img_emb is not None and text_emb is not None:\n",
    "#             # Average the embeddings\n",
    "#             combined_emb = (img_emb + text_emb) / 2\n",
    "#             return combined_emb\n",
    "#         return None\n",
    "\n",
    "# def compute_similarity(generated_file, ground_truth_file, image_paths=None):\n",
    "#     \"\"\"\n",
    "#     Compute similarity between generated texts and ground truth using LongCLIP.\n",
    "    \n",
    "#     Args:\n",
    "#         generated_file: Path to the JSON file with generated texts\n",
    "#         ground_truth_file: Path to the JSON file with ground truth\n",
    "#         image_paths: Dictionary mapping level IDs to image file paths\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with similarity scores\n",
    "#     \"\"\"\n",
    "#     # Default image paths if not provided\n",
    "#     if image_paths is None:\n",
    "#         image_paths = {\n",
    "#             \"1-1\": \"super_mario_bros_1-1_screenshot.png\",\n",
    "#             \"1-2\": \"super_mario_bros_1-2_screenshot.png\"\n",
    "#         }\n",
    "    \n",
    "#     # Load generated texts\n",
    "#     with open(generated_file, 'r') as f:\n",
    "#         generated_texts = json.load(f)\n",
    "    \n",
    "#     # Load ground truth\n",
    "#     with open(ground_truth_file, 'r') as f:\n",
    "#         ground_truth = json.load(f)\n",
    "    \n",
    "#     # Initialize LongCLIP encoder\n",
    "#     encoder = LongCLIPEncoder()\n",
    "    \n",
    "#     # Extract LevelDetails from ground truth and create combined embeddings\n",
    "#     gt_level_details = {}\n",
    "#     gt_combined_embeddings = {}\n",
    "    \n",
    "#     for level_id, text in ground_truth.items():\n",
    "#         if level_id in image_paths:\n",
    "#             # Extract just the LevelDetails\n",
    "#             details = parse_level_details(text)\n",
    "#             gt_level_details[level_id] = details\n",
    "            \n",
    "#             # Create combined embeddings using LongCLIP\n",
    "#             combined_emb = encoder.encode_combined(image_paths[level_id], details)\n",
    "#             if combined_emb is not None:\n",
    "#                 gt_combined_embeddings[level_id] = combined_emb\n",
    "    \n",
    "#     # Calculate ground truth cross-level similarity\n",
    "#     if \"1-1\" in gt_combined_embeddings and \"1-2\" in gt_combined_embeddings:\n",
    "#         gt_cross_sim = cosine_similarity(gt_combined_embeddings[\"1-1\"], gt_combined_embeddings[\"1-2\"])[0][0]\n",
    "#         print(f\"Ground Truth combined (text+image) cross-level similarity (1-1 to 1-2): {gt_cross_sim:.4f}\")\n",
    "#     else:\n",
    "#         gt_cross_sim = None\n",
    "#         print(\"Cannot calculate ground truth cross-similarity - missing embeddings\")\n",
    "    \n",
    "#     # Create a dictionary to store similarity scores\n",
    "#     similarity_scores = {}\n",
    "    \n",
    "#     # For each model in the generated texts\n",
    "#     for model_name, level_data in generated_texts.items():\n",
    "#         similarity_scores[model_name] = {}\n",
    "        \n",
    "#         # Skip models with empty data\n",
    "#         if not level_data.get(\"1-1\") or not level_data.get(\"1-2\"):\n",
    "#             print(f\"Skipping {model_name} due to missing data\")\n",
    "#             continue\n",
    "            \n",
    "#         # Create combined model embeddings (text + image)\n",
    "#         model_combined_embeddings = {}\n",
    "#         for level_id, text in level_data.items():\n",
    "#             if text and level_id in image_paths:  # Skip empty texts\n",
    "#                 # Extract just the LevelDetails\n",
    "#                 details = parse_level_details(text)\n",
    "                \n",
    "#                 # Create combined embeddings using LongCLIP\n",
    "#                 combined_emb = encoder.encode_combined(image_paths[level_id], details)\n",
    "#                 if combined_emb is not None:\n",
    "#                     model_combined_embeddings[level_id] = combined_emb\n",
    "        \n",
    "#         # Calculate similarity to corresponding ground truth\n",
    "#         for level_id, embedding in model_combined_embeddings.items():\n",
    "#             if level_id in gt_combined_embeddings:\n",
    "#                 sim_score = cosine_similarity(embedding, gt_combined_embeddings[level_id])[0][0]\n",
    "#                 similarity_scores[model_name][level_id] = sim_score\n",
    "#                 print(f\"{model_name} {level_id} vs GT {level_id}: {sim_score:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity between model outputs\n",
    "#         if \"1-1\" in model_combined_embeddings and \"1-2\" in model_combined_embeddings:\n",
    "#             model_cross_sim = cosine_similarity(model_combined_embeddings[\"1-1\"], model_combined_embeddings[\"1-2\"])[0][0]\n",
    "#             similarity_scores[model_name][\"cross_1-1_1-2\"] = model_cross_sim\n",
    "#             print(f\"{model_name} cross-similarity (1-1 to 1-2): {model_cross_sim:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity: model 1-1 to GT 1-2\n",
    "#         if \"1-1\" in model_combined_embeddings and \"1-2\" in gt_combined_embeddings:\n",
    "#             cross_sim_1 = cosine_similarity(model_combined_embeddings[\"1-1\"], gt_combined_embeddings[\"1-2\"])[0][0]\n",
    "#             similarity_scores[model_name][\"cross_model1-1_gt1-2\"] = cross_sim_1\n",
    "#             print(f\"{model_name} 1-1 vs GT 1-2: {cross_sim_1:.4f}\")\n",
    "        \n",
    "#         # Calculate cross-similarity: model 1-2 to GT 1-1\n",
    "#         if \"1-2\" in model_combined_embeddings and \"1-1\" in gt_combined_embeddings:\n",
    "#             cross_sim_2 = cosine_similarity(model_combined_embeddings[\"1-2\"], gt_combined_embeddings[\"1-1\"])[0][0]\n",
    "#             similarity_scores[model_name][\"cross_model1-2_gt1-1\"] = cross_sim_2\n",
    "#             print(f\"{model_name} 1-2 vs GT 1-1: {cross_sim_2:.4f}\")\n",
    "    \n",
    "#     # Add ground truth cross-similarity for reference\n",
    "#     if gt_cross_sim is not None:\n",
    "#         similarity_scores[\"ground_truth\"] = {\n",
    "#             \"cross_gt1-1_gt1-2\": gt_cross_sim\n",
    "#         }\n",
    "    \n",
    "#     # Create DataFrame for better visualization\n",
    "#     data = []\n",
    "#     for model, scores in similarity_scores.items():\n",
    "#         for metric, score in scores.items():\n",
    "#             data.append({\n",
    "#                 'Model': model,\n",
    "#                 'Metric': metric,\n",
    "#                 'Score': score\n",
    "#             })\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Create a pivot table for better visualization\n",
    "#     pivot_df = df.pivot(index='Model', columns='Metric', values='Score')\n",
    "    \n",
    "#     # Add row with sum of similarity scores per model\n",
    "#     if not pivot_df.empty and '1-1' in pivot_df.columns and '1-2' in pivot_df.columns:\n",
    "#         # Only sum the direct comparison scores for levels 1-1 and 1-2\n",
    "#         pivot_df['Sum'] = pivot_df[['1-1', '1-2']].sum(axis=1, skipna=True)\n",
    "        \n",
    "#         # Sort by the sum of 1-1 and 1-2 similarity (descending)\n",
    "#         pivot_df = pivot_df.sort_values('Sum', ascending=False)\n",
    "        \n",
    "#     return pivot_df\n",
    "\n",
    "# def visualize_results(df):\n",
    "#     \"\"\"\n",
    "#     Create visualizations for the similarity scores.\n",
    "    \n",
    "#     Args:\n",
    "#         df: DataFrame with similarity scores\n",
    "        \n",
    "#     Returns:\n",
    "#         None (displays plots)\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import seaborn as sns\n",
    "    \n",
    "#     # Set the style\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "    \n",
    "#     # Create a heatmap (exclude the Sum column)\n",
    "#     plt.figure(figsize=(14, 10))\n",
    "#     heatmap_cols = [col for col in df.columns if col != 'Sum']\n",
    "#     if heatmap_cols:\n",
    "#         sns.heatmap(df[heatmap_cols], annot=True, cmap='viridis', fmt='.3f')\n",
    "#         plt.title('LongCLIP Combined (Text+Image) Similarity Scores')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_longclip_similarity_heatmap.png', dpi=300)\n",
    "#         plt.close()\n",
    "    \n",
    "#     # Create a bar chart for sum similarity\n",
    "#     plt.figure(figsize=(14, 8))\n",
    "#     if 'Sum' in df.columns:\n",
    "#         sum_similarity = df['Sum'].sort_values(ascending=False)\n",
    "#         sns.barplot(x=sum_similarity.index, y=sum_similarity.values)\n",
    "#         plt.xticks(rotation=45, ha='right')\n",
    "#         plt.xlabel('Model')\n",
    "#         plt.ylabel('Sum of Similarity Scores')\n",
    "#         plt.title('Sum of LongCLIP Combined Similarity Scores by Model')\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig('mario_longclip_sum_similarity.png', dpi=300)\n",
    "#         plt.close()\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Paths\n",
    "#     generated_file = \"super_mario_bros_generated_text.json\"\n",
    "#     ground_truth_file = \"super_mario_bros_ground_truth.json\"\n",
    "#     image_paths = {\n",
    "#         \"1-1\": \"super_mario_bros_1-1_screenshot.png\",\n",
    "#         \"1-2\": \"super_mario_bros_1-2_screenshot.png\"\n",
    "#     }\n",
    "    \n",
    "#     # Install required packages if needed\n",
    "#     # !pip install transformers torch pillow\n",
    "    \n",
    "#     # Compute similarity\n",
    "#     similarity_df = compute_similarity(generated_file, ground_truth_file, image_paths)\n",
    "    \n",
    "#     # Display the similarity table\n",
    "#     print(\"\\nLongCLIP Combined Similarity Scores:\")\n",
    "#     print(similarity_df)\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     similarity_df.to_csv(\"mario_longclip_similarity_scores.csv\")\n",
    "#     print(\"Results saved to mario_longclip_similarity_scores.csv\")\n",
    "    \n",
    "#     # Visualize the results\n",
    "#     visualize_results(similarity_df)\n",
    "#     print(\"Visualizations saved as mario_longclip_similarity_heatmap.png and mario_longclip_sum_similarity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 14 chunks to SMB_IMG/raw/1-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def cut_map_into_chunks(map_path, level):\n",
    "    \"\"\"\n",
    "    Cut a map image into chunks of 256x240 pixels and save them to a folder.\n",
    "    \n",
    "    Args:\n",
    "        map_path: Path to the map image\n",
    "        level: Level identifier (e.g., '1-1', '1-2')\n",
    "    \"\"\"\n",
    "    # Load the map image\n",
    "    try:\n",
    "        map_img = Image.open(map_path)\n",
    "        width, height = map_img.size\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image: {e}\")\n",
    "        return\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = f\"SMB_IMG/raw/{level}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define chunk size\n",
    "    chunk_width, chunk_height = 256, 240\n",
    "    \n",
    "    # Calculate number of chunks\n",
    "    num_chunks_x = width // chunk_width\n",
    "    num_chunks_y = height // chunk_height\n",
    "    \n",
    "    # If there's remaining pixels, add one more chunk\n",
    "    if width % chunk_width > 0:\n",
    "        num_chunks_x += 1\n",
    "    if height % chunk_height > 0:\n",
    "        num_chunks_y += 1\n",
    "    \n",
    "    # Extract and save chunks\n",
    "    chunk_count = 0\n",
    "    for y in range(num_chunks_y):\n",
    "        for x in range(num_chunks_x):\n",
    "            # Calculate coordinates for this chunk\n",
    "            left = x * chunk_width\n",
    "            top = y * chunk_height\n",
    "            right = min((x + 1) * chunk_width, width)\n",
    "            bottom = min((y + 1) * chunk_height, height)\n",
    "            \n",
    "            # Skip if the chunk would be smaller than the desired size\n",
    "            actual_width = right - left\n",
    "            actual_height = bottom - top\n",
    "            if actual_width < chunk_width or actual_height < chunk_height:\n",
    "                # Create a new blank image with the desired size\n",
    "                chunk = Image.new(map_img.mode, (chunk_width, chunk_height))\n",
    "                # Paste the partial image into the blank image\n",
    "                chunk.paste(map_img.crop((left, top, right, bottom)), (0, 0))\n",
    "            else:\n",
    "                # Crop the chunk\n",
    "                chunk = map_img.crop((left, top, right, bottom))\n",
    "            \n",
    "            # Save the chunk\n",
    "            chunk_path = os.path.join(output_dir, f\"chunk_{chunk_count:02d}.png\")\n",
    "            chunk.save(chunk_path)\n",
    "            chunk_count += 1\n",
    "    \n",
    "    print(f\"Successfully saved {chunk_count} chunks to {output_dir}\")\n",
    "\n",
    "# Example usage (replace these with your actual values when running)\n",
    "map_path = \"SMB_NES_World_1-1_Map.png\"\n",
    "level = \"1-1\"\n",
    "cut_map_into_chunks(map_path, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed: 42\n",
      "Successfully labeled 14 images with unique characters and saved to SMB_IMG/label/1-1\n",
      "Ground truth order saved to SMB_IMG/ground_truth_1-1.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['J', 'N', 'H', 'G', 'P', 'M', 'F', 'C', 'K', 'D', 'E', 'A', 'B', 'L']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42  # You can use any integer value\n",
    "random.seed(random_seed)\n",
    "print(f\"Using random seed: {random_seed}\")\n",
    "\n",
    "def label_images(level):\n",
    "    \"\"\"\n",
    "    Add a square with a unique character label in the middle of each image.\n",
    "    \n",
    "    Args:\n",
    "        level: Level identifier (e.g., '1-1', '1-2')\n",
    "    \"\"\"\n",
    "    input_dir = f\"SMB_IMG/raw/{level}\"\n",
    "    output_dir = f\"SMB_IMG/label/{level}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all chunk images\n",
    "    chunk_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "    \n",
    "    # Define the characters to use as labels - using a variety of distinct characters\n",
    "    characters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'R', 'S', 'T', 'W', 'X', 'Y']\n",
    "    \n",
    "    # Make sure we have enough unique characters for all chunks\n",
    "    if len(chunk_files) > len(characters):\n",
    "        raise ValueError(f\"Not enough unique characters ({len(characters)}) for all chunks ({len(chunk_files)})\")\n",
    "    \n",
    "    # Take just the number of characters we need, then shuffle them\n",
    "    needed_chars = characters[:len(chunk_files)]\n",
    "    random.shuffle(needed_chars)\n",
    "    \n",
    "    # Map from chunk file to assigned character\n",
    "    char_assignments = {}\n",
    "    \n",
    "    # Process each image\n",
    "    for i, chunk_file in enumerate(chunk_files):\n",
    "        # Open the image\n",
    "        img_path = os.path.join(input_dir, chunk_file)\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        \n",
    "        # Create a drawing context\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Define square size and position (in the middle)\n",
    "        square_size = min(width, height) // 6\n",
    "        square_left = (width - square_size) // 2\n",
    "        square_top = (height - square_size) // 2\n",
    "        \n",
    "        # Draw a square\n",
    "        draw.rectangle(\n",
    "            [(square_left, square_top), \n",
    "             (square_left + square_size, square_top + square_size)],\n",
    "            outline=\"black\",\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        # Add text (character) in the middle of the square\n",
    "        try:\n",
    "            # Try to load a font\n",
    "            font = ImageFont.truetype(\"arial.ttf\", square_size // 2)\n",
    "        except IOError:\n",
    "            # Fallback to default font\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        char = needed_chars[i]\n",
    "        char_assignments[chunk_file] = char\n",
    "        \n",
    "        # Calculate text position to center it in the square\n",
    "        text_width = draw.textlength(char, font=font)\n",
    "        text_height = square_size // 2  # Approximate height\n",
    "        \n",
    "        text_x = square_left + (square_size - text_width) // 2\n",
    "        text_y = square_top + (square_size - text_height) // 2\n",
    "        \n",
    "        # Draw the character\n",
    "        draw.text((text_x, text_y), char, fill=\"black\", font=font)\n",
    "        \n",
    "        # Save the labeled image\n",
    "        output_path = os.path.join(output_dir, chunk_file)\n",
    "        img.save(output_path)\n",
    "    \n",
    "    # Create a ground truth sequence based on the original order of chunk files\n",
    "    sorted_chunks = sorted(chunk_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    ground_truth_sequence = [char_assignments[chunk] for chunk in sorted_chunks]\n",
    "    \n",
    "    print(f\"Successfully labeled {len(chunk_files)} images with unique characters and saved to {output_dir}\")\n",
    "    return ground_truth_sequence\n",
    "\n",
    "def save_ground_truth_order(level, ground_truth_order):\n",
    "    \"\"\"\n",
    "    Save the ground truth order of images to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        level: Level identifier (e.g., '1-1', '1-2')\n",
    "        ground_truth_order: List of characters representing the ground truth order\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(\"SMB_IMG\", exist_ok=True)\n",
    "    \n",
    "    # Save ground truth to a JSON file\n",
    "    output_path = f\"SMB_IMG/ground_truth_{level}.json\"\n",
    "    \n",
    "    ground_truth_data = {\n",
    "        \"level\": level,\n",
    "        \"ground_truth_sequence\": ground_truth_order,\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(ground_truth_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Ground truth order saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Example usage after labeling images\n",
    "\n",
    "\n",
    "# Example usage\n",
    "level = \"1-1\"  # Replace with your level\n",
    "ground_truth_order = label_images(level)\n",
    "save_ground_truth_order(level, ground_truth_order)\n",
    "ground_truth_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed: 42\n",
      "Error reading SMB_IMG/model_predictions_1-1.json, starting fresh\n",
      "Processing model: o4-mini-2025-04-16\n",
      "  Run 1/3\n",
      "Images have been randomly shuffled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuxua\\miniconda3\\envs\\local_cua\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from o4-mini-2025-04-16 (openai):\n",
      "[\"J\",\"C\",\"G\",\"H\",\"K\",\"N\",\"M\",\"P\",\"F\",\"D\"]\n",
      "  Saved results for o4-mini-2025-04-16 run 1\n",
      "  Run 2/3\n",
      "Images have been randomly shuffled\n",
      "Response from o4-mini-2025-04-16 (openai):\n",
      "[\"J\",\"G\",\"F\",\"M\",\"N\",\"H\",\"C\",\"K\",\"D\",\"P\"]\n",
      "  Saved results for o4-mini-2025-04-16 run 2\n",
      "  Run 3/3\n",
      "Images have been randomly shuffled\n",
      "Response from o4-mini-2025-04-16 (openai):\n",
      "[\"J\",\"C\",\"M\",\"N\",\"G\",\"F\",\"P\",\"H\",\"K\",\"D\"]\n",
      "  Saved results for o4-mini-2025-04-16 run 3\n",
      "Processing model: o3-2025-04-16\n",
      "  Run 1/3\n",
      "Images have been randomly shuffled\n",
      "Response from o3-2025-04-16 (openai):\n",
      "[\"J\",\"C\",\"N\",\"G\",\"H\",\"F\",\"P\",\"M\",\"K\",\"D\"]\n",
      "  Saved results for o3-2025-04-16 run 1\n",
      "  Run 2/3\n",
      "Images have been randomly shuffled\n",
      "Response from o3-2025-04-16 (openai):\n",
      "[\"J\",\"N\",\"F\",\"G\",\"H\",\"P\",\"C\",\"K\",\"M\",\"D\"]\n",
      "  Saved results for o3-2025-04-16 run 2\n",
      "  Run 3/3\n",
      "Images have been randomly shuffled\n",
      "Response from o3-2025-04-16 (openai):\n",
      "[\"J\",\"G\",\"H\",\"C\",\"K\",\"N\",\"P\",\"F\",\"M\",\"D\"]\n",
      "  Saved results for o3-2025-04-16 run 3\n",
      "Processing model: gemini-2.5-pro-preview-03-25\n",
      "  Run 1/3\n",
      "Images have been randomly shuffled\n",
      "Response from gemini-2.5-pro-preview-03-25 (gemini):\n",
      "```json\n",
      "[\"J\", \"C\", \"N\", \"P\", \"H\", \"G\", \"M\", \"F\", \"K\", \"D\", \"L\"]\n",
      "```\n",
      "  Saved results for gemini-2.5-pro-preview-03-25 run 1\n",
      "  Run 2/3\n",
      "Images have been randomly shuffled\n",
      "Response from gemini-2.5-pro-preview-03-25 (gemini):\n",
      "```json\n",
      "[\"J\", \"C\", \"F\", \"P\", \"N\", \"G\", \"H\", \"K\", \"D\", \"M\"]\n",
      "```\n",
      "  Saved results for gemini-2.5-pro-preview-03-25 run 2\n",
      "  Run 3/3\n",
      "Images have been randomly shuffled\n",
      "Response from gemini-2.5-pro-preview-03-25 (gemini):\n",
      "```json\n",
      "[\"J\", \"F\", \"C\", \"M\", \"P\", \"N\", \"H\", \"G\", \"K\", \"D\"]\n",
      "```\n",
      "  Saved results for gemini-2.5-pro-preview-03-25 run 3\n",
      "Processing model: claude-3-7-sonnet-20250219\n",
      "  Run 1/3\n",
      "Images have been randomly shuffled\n",
      "Response from claude-3-7-sonnet-20250219 (anthropic):\n",
      "[\"J\",\"D\",\"G\",\"F\",\"H\",\"K\",\"C\",\"P\",\"N\",\"M\"]\n",
      "  Saved results for claude-3-7-sonnet-20250219 run 1\n",
      "  Run 2/3\n",
      "Images have been randomly shuffled\n",
      "Response from claude-3-7-sonnet-20250219 (anthropic):\n",
      "[\"J\",\"G\",\"H\",\"F\",\"D\",\"K\",\"C\",\"N\",\"P\",\"M\"]\n",
      "  Saved results for claude-3-7-sonnet-20250219 run 2\n",
      "  Run 3/3\n",
      "Images have been randomly shuffled\n",
      "Response from claude-3-7-sonnet-20250219 (anthropic):\n",
      "[\"J\",\"D\",\"K\",\"F\",\"N\",\"G\",\"H\",\"C\",\"M\",\"P\"]\n",
      "  Saved results for claude-3-7-sonnet-20250219 run 3\n",
      "Processing model: Llama-4-Maverick-17B-128E-Instruct-FP8\n",
      "  Run 1/3\n",
      "Images have been randomly shuffled\n",
      "Response from meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 (together):\n",
      "[\"G\",\"H\",\"J\",\"C\",\"P\",\"F\",\"M\",\"N\",\"D\",\"K\"]\n",
      "  Saved results for Llama-4-Maverick-17B-128E-Instruct-FP8 run 1\n",
      "  Run 2/3\n",
      "Images have been randomly shuffled\n",
      "Response from meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 (together):\n",
      "[\"H\",\"C\",\"K\",\"D\",\"M\",\"J\",\"F\",\"N\",\"G\",\"P\"]\n",
      "  Saved results for Llama-4-Maverick-17B-128E-Instruct-FP8 run 2\n",
      "  Run 3/3\n",
      "Images have been randomly shuffled\n",
      "Response from meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 (together):\n",
      "[\"G\",\"H\",\"J\",\"D\",\"K\",\"C\",\"N\",\"F\",\"P\",\"M\"]\n",
      "  Saved results for Llama-4-Maverick-17B-128E-Instruct-FP8 run 3\n",
      "Processing model: claude-3-5-sonnet-20241022\n",
      "  Run 1/3\n",
      "Images have been randomly shuffled\n",
      "Response from claude-3-5-sonnet-20241022 (anthropic):\n",
      "[\"J\",\"G\",\"H\",\"K\",\"D\",\"F\",\"P\",\"N\",\"M\",\"C\"]\n",
      "  Saved results for claude-3-5-sonnet-20241022 run 1\n",
      "  Run 2/3\n",
      "Images have been randomly shuffled\n",
      "Response from claude-3-5-sonnet-20241022 (anthropic):\n",
      "[\"J\",\"D\",\"K\",\"C\",\"M\",\"N\",\"H\",\"G\",\"F\",\"P\"]\n",
      "  Saved results for claude-3-5-sonnet-20241022 run 2\n",
      "  Run 3/3\n",
      "Images have been randomly shuffled\n",
      "Response from claude-3-5-sonnet-20241022 (anthropic):\n",
      "[\"J\",\"G\",\"H\",\"F\",\"D\",\"K\",\"M\",\"N\",\"C\",\"P\"]\n",
      "  Saved results for claude-3-5-sonnet-20241022 run 3\n",
      "All results saved to SMB_IMG/model_predictions_1-1.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42  # You can use any integer value\n",
    "random.seed(random_seed)\n",
    "print(f\"Using random seed: {random_seed}\")\n",
    "\n",
    "def load_images_from_directory(directory, max_images=10, shuffle=True):\n",
    "    \"\"\"Load images from a directory and convert them to base64.\"\"\"\n",
    "    image_files = sorted(glob.glob(f\"{directory}/*.png\"))[:max_images]\n",
    "    base64_images = []\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        with open(image_file, \"rb\") as f:\n",
    "            image_bytes = f.read()\n",
    "            base64_image = base64.b64encode(image_bytes).decode('utf-8')\n",
    "            base64_images.append(base64_image)\n",
    "    \n",
    "    if shuffle:\n",
    "        random.shuffle(base64_images)\n",
    "        print(f\"Images have been randomly shuffled\")\n",
    "    \n",
    "    return base64_images\n",
    "\n",
    "def analyze_labeled_images_with_models(level, models, runs=3):\n",
    "    \"\"\"\n",
    "    Loop through models to analyze labeled images multiple times.\n",
    "    \n",
    "    Args:\n",
    "        level: Level identifier (e.g., '1-1', '1-2')\n",
    "        models: List of model names to use\n",
    "        runs: Number of times to run each model (default: 3)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    output_path = f\"SMB_IMG/model_predictions_{level}.json\"\n",
    "    \n",
    "    # Check if the output file exists and load previous results\n",
    "    if os.path.exists(output_path):\n",
    "        try:\n",
    "            with open(output_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            print(f\"Loaded existing results from {output_path}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error reading {output_path}, starting fresh\")\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"Processing model: {model}\")\n",
    "        \n",
    "        # Initialize model results if not already present\n",
    "        if model not in results:\n",
    "            results[model] = []\n",
    "        \n",
    "        # Check how many runs already exist for this model\n",
    "        completed_runs = len(results[model])\n",
    "        remaining_runs = runs - completed_runs\n",
    "        \n",
    "        if remaining_runs <= 0:\n",
    "            print(f\"Skipping model: {model} (already has {completed_runs} runs)\")\n",
    "            continue\n",
    "        \n",
    "        # Determine provider based on model name\n",
    "        if \"claude\" in model.lower():\n",
    "            provider = \"anthropic\"\n",
    "        elif \"o3\" in model.lower() or \"o4\" in model.lower():\n",
    "            provider = \"openai\"\n",
    "        elif \"gemini\" in model.lower():\n",
    "            provider = \"gemini\"\n",
    "        elif \"llama\" in model.lower() or \"maverick\" in model.lower():\n",
    "            provider = \"together\"\n",
    "            if model == \"Llama-4-Maverick-17B-128E-Instruct-FP8\":\n",
    "                actual_model = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "            else:\n",
    "                actual_model = model\n",
    "        else:\n",
    "            print(f\"Unknown model: {model}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Perform the remaining runs\n",
    "        for run in range(completed_runs + 1, runs + 1):\n",
    "            print(f\"  Run {run}/{runs}\")\n",
    "            try:\n",
    "                if provider == \"together\":\n",
    "                    result = analyze_labeled_images(level, provider, actual_model)\n",
    "                else:\n",
    "                    result = analyze_labeled_images(level, provider, model)\n",
    "                \n",
    "                # Add the result to the model's results list\n",
    "                results[model].append(result)\n",
    "                \n",
    "                # Save after each run to preserve progress\n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "                print(f\"  Saved results for {model} run {run}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {model} run {run}: {str(e)}\")\n",
    "                results[model].append(f\"Error run {run}: {str(e)}\")\n",
    "                \n",
    "                # Save after each error too\n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"All results saved to {output_path}\")\n",
    "    return results\n",
    "\n",
    "def analyze_labeled_images(level, model_provider, model_name):\n",
    "    \"\"\"\n",
    "    Submit the first 10 labeled images to an AI model to determine their order.\n",
    "    \n",
    "    Args:\n",
    "        level: Level identifier (e.g., '1-1', '1-2')\n",
    "        model_provider: Which API provider to use ('anthropic', 'openai', 'gemini', 'together')\n",
    "        model_name: Name of the specific model to use\n",
    "    \"\"\"\n",
    "    # System prompt\n",
    "    system_prompt = \"You are an AI assistant tasked with analyzing labeled images from Super Mario Bros (1985).\"\n",
    "    \n",
    "    # Directory containing the labeled images\n",
    "    directory = f\"SMB_IMG/label/{level}\"\n",
    "    \n",
    "    # Load images\n",
    "    base64_images = load_images_from_directory(directory)\n",
    "    \n",
    "    # User prompt\n",
    "    prompt = f\"\"\"This is Super Mario Bros (1985) level {level}. \n",
    "    Each image shows a section of the level, labeled with a character (A, B, D, E, F, G, H, J, K, L) in a square.\n",
    "\n",
    "    Your task: Determine the correct left-to-right sequence of these level sections as they would appear in the original game.\n",
    "\n",
    "    Based on game elements, obstacles, terrain features, and progression logic, order the sections from left to right.\n",
    "\n",
    "    RETURN FORMAT: Return ONLY a JSON-compatible list of characters in square brackets, like this: [\"A\",\"B\",\"D\",\"J\",\"K\"]\n",
    "\n",
    "    Important:\n",
    "    - Start with the leftmost part of the level\n",
    "    - Consider game progression logic (e.g., platforms, pipes, enemies, power-ups)\n",
    "    - Each image is part of a continuous level\n",
    "    - Do not include any explanation, only the character list in [\"A\",\"B\",...] format\n",
    "    \"\"\"\n",
    "    # Choose API provider\n",
    "    if model_provider == \"anthropic\":\n",
    "        from tools.serving.api_providers import anthropic_multiimage_completion\n",
    "        if \"claude-3-5\" in model_name:\n",
    "            response = anthropic_multiimage_completion(system_prompt, model_name, prompt, base64_images, max_tokens=8192)\n",
    "        else:\n",
    "            response = anthropic_multiimage_completion(system_prompt, model_name, prompt, base64_images)\n",
    "    \n",
    "    elif model_provider == \"openai\":\n",
    "        from tools.serving.api_providers import openai_multiimage_completion\n",
    "        response = openai_multiimage_completion(system_prompt, model_name, prompt, base64_images)\n",
    "    \n",
    "    elif model_provider == \"gemini\":\n",
    "        from tools.serving.api_providers import gemini_multiimage_completion\n",
    "        response = gemini_multiimage_completion(system_prompt, model_name, prompt, base64_images)\n",
    "    \n",
    "    elif model_provider == \"together\":\n",
    "        from tools.serving.api_providers import together_ai_multiimage_completion\n",
    "        response = together_ai_multiimage_completion(system_prompt, model_name, prompt, base64_images)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model provider: {model_provider}\")\n",
    "    \n",
    "    print(f\"Response from {model_name} ({model_provider}):\\n{response}\")\n",
    "    return response\n",
    "\n",
    "# Define models to use\n",
    "MODEL_NAMES = [\n",
    "    \"o4-mini-2025-04-16\",\n",
    "    \"o3-2025-04-16\",\n",
    "    \"gemini-2.5-pro-preview-03-25\",\n",
    "    \"claude-3-7-sonnet-20250219\",\n",
    "    \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "    \"claude-3-5-sonnet-20241022\"\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "level = \"1-1\"  # Replace with your level\n",
    "results = analyze_labeled_images_with_models(level, MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: ['J', 'N', 'H', 'G', 'P', 'M', 'F', 'C', 'K', 'D']\n",
      "\n",
      "Comparison Results:\n",
      "--------------------------------------------------\n",
      "o4-mini-2025-04-16:\n",
      "  Run 1: Accuracy 20.00% (2/10 correct)\n",
      "  Run 2: Accuracy 10.00% (1/10 correct)\n",
      "  Run 3: Accuracy 30.00% (3/10 correct)\n",
      "  Average Accuracy: 20.00% (3/3 successful runs)\n",
      "o3-2025-04-16:\n",
      "  Run 1: Accuracy 40.00% (4/10 correct)\n",
      "  Run 2: Accuracy 40.00% (4/10 correct)\n",
      "  Run 3: Accuracy 30.00% (3/10 correct)\n",
      "  Average Accuracy: 36.67% (3/3 successful runs)\n",
      "gemini-2.5-pro-preview-03-25:\n",
      "  Run 1: Accuracy 30.00% (3/10 correct)\n",
      "  Run 2: Accuracy 10.00% (1/10 correct)\n",
      "  Run 3: Accuracy 40.00% (4/10 correct)\n",
      "  Average Accuracy: 26.67% (3/3 successful runs)\n",
      "claude-3-7-sonnet-20250219:\n",
      "  Run 1: Accuracy 10.00% (1/10 correct)\n",
      "  Run 2: Accuracy 20.00% (2/10 correct)\n",
      "  Run 3: Accuracy 20.00% (2/10 correct)\n",
      "  Average Accuracy: 16.67% (3/3 successful runs)\n",
      "Llama-4-Maverick-17B-128E-Instruct-FP8:\n",
      "  Run 1: Accuracy 10.00% (1/10 correct)\n",
      "  Run 2: Accuracy 10.00% (1/10 correct)\n",
      "  Run 3: Accuracy 0.00% (0/10 correct)\n",
      "  Average Accuracy: 6.67% (3/3 successful runs)\n",
      "claude-3-5-sonnet-20241022:\n",
      "  Run 1: Accuracy 20.00% (2/10 correct)\n",
      "  Run 2: Accuracy 10.00% (1/10 correct)\n",
      "  Run 3: Accuracy 20.00% (2/10 correct)\n",
      "  Average Accuracy: 16.67% (3/3 successful runs)\n",
      "\n",
      "Overall Average Accuracy Across All Models: 20.56%\n",
      "\n",
      "Results saved to SMB_IMG/accuracy_results_1-1.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'o4-mini-2025-04-16': {'runs': [{'run': 1,\n",
       "    'accuracy': 0.2,\n",
       "    'correct': 2,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'C', 'G', 'H', 'K', 'N', 'M', 'P', 'F', 'D']},\n",
       "   {'run': 2,\n",
       "    'accuracy': 0.1,\n",
       "    'correct': 1,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'G', 'F', 'M', 'N', 'H', 'C', 'K', 'D', 'P']},\n",
       "   {'run': 3,\n",
       "    'accuracy': 0.3,\n",
       "    'correct': 3,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'C', 'M', 'N', 'G', 'F', 'P', 'H', 'K', 'D']}],\n",
       "  'average_accuracy': 0.20000000000000004,\n",
       "  'total_runs': 3,\n",
       "  'successful_runs': 3},\n",
       " 'o3-2025-04-16': {'runs': [{'run': 1,\n",
       "    'accuracy': 0.4,\n",
       "    'correct': 4,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'C', 'N', 'G', 'H', 'F', 'P', 'M', 'K', 'D']},\n",
       "   {'run': 2,\n",
       "    'accuracy': 0.4,\n",
       "    'correct': 4,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'N', 'F', 'G', 'H', 'P', 'C', 'K', 'M', 'D']},\n",
       "   {'run': 3,\n",
       "    'accuracy': 0.3,\n",
       "    'correct': 3,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'G', 'H', 'C', 'K', 'N', 'P', 'F', 'M', 'D']}],\n",
       "  'average_accuracy': 0.3666666666666667,\n",
       "  'total_runs': 3,\n",
       "  'successful_runs': 3},\n",
       " 'gemini-2.5-pro-preview-03-25': {'runs': [{'run': 1,\n",
       "    'accuracy': 0.3,\n",
       "    'correct': 3,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'C', 'N', 'P', 'H', 'G', 'M', 'F', 'K', 'D', 'L']},\n",
       "   {'run': 2,\n",
       "    'accuracy': 0.1,\n",
       "    'correct': 1,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'C', 'F', 'P', 'N', 'G', 'H', 'K', 'D', 'M']},\n",
       "   {'run': 3,\n",
       "    'accuracy': 0.4,\n",
       "    'correct': 4,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'F', 'C', 'M', 'P', 'N', 'H', 'G', 'K', 'D']}],\n",
       "  'average_accuracy': 0.26666666666666666,\n",
       "  'total_runs': 3,\n",
       "  'successful_runs': 3},\n",
       " 'claude-3-7-sonnet-20250219': {'runs': [{'run': 1,\n",
       "    'accuracy': 0.1,\n",
       "    'correct': 1,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'D', 'G', 'F', 'H', 'K', 'C', 'P', 'N', 'M']},\n",
       "   {'run': 2,\n",
       "    'accuracy': 0.2,\n",
       "    'correct': 2,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'G', 'H', 'F', 'D', 'K', 'C', 'N', 'P', 'M']},\n",
       "   {'run': 3,\n",
       "    'accuracy': 0.2,\n",
       "    'correct': 2,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'D', 'K', 'F', 'N', 'G', 'H', 'C', 'M', 'P']}],\n",
       "  'average_accuracy': 0.16666666666666666,\n",
       "  'total_runs': 3,\n",
       "  'successful_runs': 3},\n",
       " 'Llama-4-Maverick-17B-128E-Instruct-FP8': {'runs': [{'run': 1,\n",
       "    'accuracy': 0.1,\n",
       "    'correct': 1,\n",
       "    'total': 10,\n",
       "    'prediction': ['G', 'H', 'J', 'C', 'P', 'F', 'M', 'N', 'D', 'K']},\n",
       "   {'run': 2,\n",
       "    'accuracy': 0.1,\n",
       "    'correct': 1,\n",
       "    'total': 10,\n",
       "    'prediction': ['H', 'C', 'K', 'D', 'M', 'J', 'F', 'N', 'G', 'P']},\n",
       "   {'run': 3,\n",
       "    'accuracy': 0.0,\n",
       "    'correct': 0,\n",
       "    'total': 10,\n",
       "    'prediction': ['G', 'H', 'J', 'D', 'K', 'C', 'N', 'F', 'P', 'M']}],\n",
       "  'average_accuracy': 0.06666666666666667,\n",
       "  'total_runs': 3,\n",
       "  'successful_runs': 3},\n",
       " 'claude-3-5-sonnet-20241022': {'runs': [{'run': 1,\n",
       "    'accuracy': 0.2,\n",
       "    'correct': 2,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'G', 'H', 'K', 'D', 'F', 'P', 'N', 'M', 'C']},\n",
       "   {'run': 2,\n",
       "    'accuracy': 0.1,\n",
       "    'correct': 1,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'D', 'K', 'C', 'M', 'N', 'H', 'G', 'F', 'P']},\n",
       "   {'run': 3,\n",
       "    'accuracy': 0.2,\n",
       "    'correct': 2,\n",
       "    'total': 10,\n",
       "    'prediction': ['J', 'G', 'H', 'F', 'D', 'K', 'M', 'N', 'C', 'P']}],\n",
       "  'average_accuracy': 0.16666666666666666,\n",
       "  'total_runs': 3,\n",
       "  'successful_runs': 3}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def compare_predictions(level, max_len=10):\n",
    "    \"\"\"\n",
    "    Compare model predictions with ground truth and calculate accuracy.\n",
    "    Handles multiple runs for each model.\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    try:\n",
    "        with open(f\"SMB_IMG/ground_truth_{level}.json\", 'r') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "            ground_truth = ground_truth_data.get(\"ground_truth_sequence\", [])\n",
    "            ground_truth = ground_truth[:max_len]\n",
    "            print(f\"Ground Truth: {ground_truth}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Load model predictions\n",
    "    try:\n",
    "        with open(f\"SMB_IMG/model_predictions_{level}.json\", 'r') as f:\n",
    "            predictions = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading predictions: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Compare each model's prediction with ground truth\n",
    "    results = {}\n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model, model_runs in predictions.items():\n",
    "        results[model] = {\n",
    "            \"runs\": [],\n",
    "            \"average_accuracy\": 0,\n",
    "            \"total_runs\": 0,\n",
    "            \"successful_runs\": 0\n",
    "        }\n",
    "        \n",
    "        # Check if model_runs is a list (multiple runs) or single result\n",
    "        if not isinstance(model_runs, list):\n",
    "            model_runs = [model_runs]  # Convert to list for consistency\n",
    "        \n",
    "        # Process each run\n",
    "        for run_idx, pred_str in enumerate(model_runs):\n",
    "            run_result = {\n",
    "                \"run\": run_idx + 1,\n",
    "                \"accuracy\": 0,\n",
    "                \"correct\": 0,\n",
    "                \"total\": len(ground_truth)\n",
    "            }\n",
    "            \n",
    "            # Try to parse the prediction\n",
    "            try:\n",
    "                # Handle different formats\n",
    "                if isinstance(pred_str, str):\n",
    "                    if pred_str.startswith(\"Error\"):\n",
    "                        run_result[\"error\"] = pred_str\n",
    "                        results[model][\"runs\"].append(run_result)\n",
    "                        continue\n",
    "                    \n",
    "                    # Clean up common formats\n",
    "                    pred_str = pred_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                    \n",
    "                    # Try JSON parsing\n",
    "                    if pred_str.startswith(\"[\"):\n",
    "                        prediction = json.loads(pred_str)\n",
    "                    else:\n",
    "                        # Handle comma or space separated\n",
    "                        if \",\" in pred_str:\n",
    "                            prediction = [p.strip().strip('\"\\'') for p in pred_str.split(\",\")]\n",
    "                        else:\n",
    "                            prediction = [p.strip().strip('\"\\'') for p in pred_str.split()]\n",
    "                else:\n",
    "                    prediction = pred_str\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                min_len = min(len(prediction), len(ground_truth))\n",
    "                correct = 0\n",
    "                \n",
    "                # Compare character by character\n",
    "                for i in range(min_len):\n",
    "                    if prediction[i] == ground_truth[i]:\n",
    "                        correct += 1\n",
    "                \n",
    "                # Calculate accuracy metrics\n",
    "                accuracy = correct / len(ground_truth) if ground_truth else 0\n",
    "                \n",
    "                # Store run results\n",
    "                run_result[\"prediction\"] = prediction\n",
    "                run_result[\"accuracy\"] = accuracy\n",
    "                run_result[\"correct\"] = correct\n",
    "                results[model][\"runs\"].append(run_result)\n",
    "                \n",
    "                # Update model stats\n",
    "                results[model][\"successful_runs\"] += 1\n",
    "                results[model][\"total_runs\"] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                run_result[\"error\"] = f\"Could not parse prediction: {str(e)}\"\n",
    "                results[model][\"runs\"].append(run_result)\n",
    "                results[model][\"total_runs\"] += 1\n",
    "        \n",
    "        # Calculate average accuracy for successful runs\n",
    "        successful_runs = [run for run in results[model][\"runs\"] if \"error\" not in run]\n",
    "        if successful_runs:\n",
    "            avg_accuracy = np.mean([run[\"accuracy\"] for run in successful_runs])\n",
    "            results[model][\"average_accuracy\"] = avg_accuracy\n",
    "            \n",
    "            print(f\"{model}:\")\n",
    "            for run in results[model][\"runs\"]:\n",
    "                if \"error\" in run:\n",
    "                    print(f\"  Run {run['run']}: Error - {run['error']}\")\n",
    "                else:\n",
    "                    print(f\"  Run {run['run']}: Accuracy {run['accuracy']:.2%} ({run['correct']}/{run['total']} correct)\")\n",
    "            print(f\"  Average Accuracy: {avg_accuracy:.2%} ({len(successful_runs)}/{results[model]['total_runs']} successful runs)\")\n",
    "        else:\n",
    "            print(f\"{model}: No successful runs\")\n",
    "    \n",
    "    # Calculate overall average accuracy across all models\n",
    "    model_avg_accuracies = [r[\"average_accuracy\"] for r in results.values() if r[\"successful_runs\"] > 0]\n",
    "    if model_avg_accuracies:\n",
    "        overall_avg_accuracy = np.mean(model_avg_accuracies)\n",
    "        print(f\"\\nOverall Average Accuracy Across All Models: {overall_avg_accuracy:.2%}\")\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"SMB_IMG/accuracy_results_{level}.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to SMB_IMG/accuracy_results_{level}.json\")\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "level = \"1-1\"\n",
    "compare_predictions(level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_cua",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
